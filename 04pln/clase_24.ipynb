{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfCWq4kOx87qzUWah8G4JM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidlealo/sic_ai_2025_jun/blob/main/04pln/clase_24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Procesamiento del Lenguaje Natural (PLN) - Slides 47 a 60\n",
        "\n",
        "Este documento explica en profundidad los contenidos presentados en las diapositivas 47 a 60 del capítulo 7 sobre PLN, junto con ejercicios prácticos en Python.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Flujo de trabajo en PLN\n",
        "\n",
        "El pipeline estándar del PLN incluye:\n",
        "\n",
        "1. **Adquisición de datos**: recopilación de corpus (textos).\n",
        "2. **Preprocesamiento**: limpieza del texto (tokenización, stopwords, lematización).\n",
        "3. **Modelado**: representación del texto (TF-IDF, embeddings).\n",
        "4. **Aprendizaje automático**: entrenamiento de modelos de clasificación, etiquetado, etc.\n",
        "5. **Despliegue**: puesta en producción del modelo.\n",
        "6. **EDA (Exploración de datos)**: análisis transversal del texto antes y después del preprocesamiento.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "Udh2Z0UShV7v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "id": "-U26Vo2RhT7v",
        "outputId": "6a068f3e-9239-4691-b001-f2072f95d564"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (0.21)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"414pt\" height=\"439pt\"\n viewBox=\"0.00 0.00 413.75 438.70\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 434.7)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-434.7 409.75,-434.7 409.75,4 -4,4\"/>\n<!-- A -->\n<g id=\"node1\" class=\"node\">\n<title>A</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"98.99\" cy=\"-403.83\" rx=\"98.99\" ry=\"26.74\"/>\n<text text-anchor=\"middle\" x=\"98.99\" y=\"-407.63\" font-family=\"Times,serif\" font-size=\"14.00\">Adquisición de datos</text>\n<text text-anchor=\"middle\" x=\"98.99\" y=\"-392.63\" font-family=\"Times,serif\" font-size=\"14.00\">Recolección de corpus</text>\n</g>\n<!-- B -->\n<g id=\"node2\" class=\"node\">\n<title>B</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"171.99\" cy=\"-299.09\" rx=\"103.48\" ry=\"26.74\"/>\n<text text-anchor=\"middle\" x=\"171.99\" y=\"-302.89\" font-family=\"Times,serif\" font-size=\"14.00\">Preprocesamiento</text>\n<text text-anchor=\"middle\" x=\"171.99\" y=\"-287.89\" font-family=\"Times,serif\" font-size=\"14.00\">Limpieza y preparación</text>\n</g>\n<!-- A&#45;&gt;B -->\n<g id=\"edge1\" class=\"edge\">\n<title>A&#45;&gt;B</title>\n<path fill=\"none\" stroke=\"black\" d=\"M117.23,-377.17C126.46,-364.18 137.79,-348.22 147.75,-334.21\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"150.8,-335.96 153.74,-325.78 145.1,-331.9 150.8,-335.96\"/>\n</g>\n<!-- C -->\n<g id=\"node3\" class=\"node\">\n<title>C</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"240.99\" cy=\"-208.35\" rx=\"106.55\" ry=\"26.74\"/>\n<text text-anchor=\"middle\" x=\"240.99\" y=\"-212.15\" font-family=\"Times,serif\" font-size=\"14.00\">Modelado</text>\n<text text-anchor=\"middle\" x=\"240.99\" y=\"-197.15\" font-family=\"Times,serif\" font-size=\"14.00\">Representación del texto</text>\n</g>\n<!-- B&#45;&gt;C -->\n<g id=\"edge2\" class=\"edge\">\n<title>B&#45;&gt;C</title>\n<path fill=\"none\" stroke=\"black\" d=\"M191.95,-272.42C199.1,-263.23 207.28,-252.71 214.87,-242.95\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"217.71,-245 221.08,-234.96 212.18,-240.71 217.71,-245\"/>\n</g>\n<!-- D -->\n<g id=\"node4\" class=\"node\">\n<title>D</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"240.99\" cy=\"-117.61\" rx=\"103.89\" ry=\"26.74\"/>\n<text text-anchor=\"middle\" x=\"240.99\" y=\"-121.41\" font-family=\"Times,serif\" font-size=\"14.00\">Aprendizaje automático</text>\n<text text-anchor=\"middle\" x=\"240.99\" y=\"-106.41\" font-family=\"Times,serif\" font-size=\"14.00\">Entrenamiento</text>\n</g>\n<!-- C&#45;&gt;D -->\n<g id=\"edge3\" class=\"edge\">\n<title>C&#45;&gt;D</title>\n<path fill=\"none\" stroke=\"black\" d=\"M240.99,-181.21C240.99,-172.9 240.99,-163.56 240.99,-154.69\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"244.5,-154.57 240.99,-144.57 237.5,-154.57 244.5,-154.57\"/>\n</g>\n<!-- E -->\n<g id=\"node5\" class=\"node\">\n<title>E</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"240.99\" cy=\"-26.87\" rx=\"94.09\" ry=\"26.74\"/>\n<text text-anchor=\"middle\" x=\"240.99\" y=\"-30.67\" font-family=\"Times,serif\" font-size=\"14.00\">Despliegue</text>\n<text text-anchor=\"middle\" x=\"240.99\" y=\"-15.67\" font-family=\"Times,serif\" font-size=\"14.00\">Puesta en producción</text>\n</g>\n<!-- D&#45;&gt;E -->\n<g id=\"edge4\" class=\"edge\">\n<title>D&#45;&gt;E</title>\n<path fill=\"none\" stroke=\"black\" d=\"M240.99,-90.47C240.99,-82.16 240.99,-72.82 240.99,-63.95\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"244.5,-63.83 240.99,-53.83 237.5,-63.83 244.5,-63.83\"/>\n</g>\n<!-- F -->\n<g id=\"node6\" class=\"node\">\n<title>F</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"310.99\" cy=\"-403.83\" rx=\"94.51\" ry=\"26.74\"/>\n<text text-anchor=\"middle\" x=\"310.99\" y=\"-407.63\" font-family=\"Times,serif\" font-size=\"14.00\">EDA</text>\n<text text-anchor=\"middle\" x=\"310.99\" y=\"-392.63\" font-family=\"Times,serif\" font-size=\"14.00\">Exploración de Datos</text>\n</g>\n<!-- F&#45;&gt;B -->\n<g id=\"edge5\" class=\"edge\">\n<title>F&#45;&gt;B</title>\n<path fill=\"none\" stroke=\"black\" d=\"M278.05,-378.48C258.72,-364.19 234.11,-346 213.46,-330.74\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"215.3,-327.75 205.18,-324.62 211.14,-333.38 215.3,-327.75\"/>\n<text text-anchor=\"middle\" x=\"268.49\" y=\"-347.76\" font-family=\"Times,serif\" font-size=\"14.00\">Apoya</text>\n</g>\n<!-- F&#45;&gt;C -->\n<g id=\"edge6\" class=\"edge\">\n<title>F&#45;&gt;C</title>\n<path fill=\"none\" stroke=\"black\" d=\"M308.68,-376.9C305.58,-349.88 298.67,-306.74 283.99,-272.22 279.76,-262.26 273.84,-252.24 267.74,-243.22\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"270.51,-241.07 261.89,-234.91 264.79,-245.11 270.51,-241.07\"/>\n<text text-anchor=\"middle\" x=\"318.49\" y=\"-295.39\" font-family=\"Times,serif\" font-size=\"14.00\">Apoya</text>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x7cedd3d4f450>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!pip install graphviz\n",
        "\n",
        "from graphviz import Digraph\n",
        "\n",
        "dot = Digraph()\n",
        "\n",
        "dot.node('A', 'Adquisición de datos\\nRecolección de corpus')\n",
        "dot.node('B', 'Preprocesamiento\\nLimpieza y preparación')\n",
        "dot.node('C', 'Modelado\\nRepresentación del texto')\n",
        "dot.node('D', 'Aprendizaje automático\\nEntrenamiento')\n",
        "dot.node('E', 'Despliegue\\nPuesta en producción')\n",
        "dot.node('F', 'EDA\\nExploración de Datos')\n",
        "\n",
        "dot.edges(['AB', 'BC', 'CD', 'DE'])\n",
        "dot.edge('F', 'B', label='Apoya')\n",
        "dot.edge('F', 'C', label='Apoya')\n",
        "\n",
        "dot.render('pln_flowchart', format='png', cleanup=False)\n",
        "dot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 2. Comprensión vs. Generación\n",
        "\n",
        "- **NLU (Natural Language Understanding)**: entender el lenguaje.\n",
        "  - Ej: análisis de sentimientos, NER, clasificación.\n",
        "- **NLG (Natural Language Generation)**: generar texto.\n",
        "  - Ej: respuestas automáticas, generación de resúmenes.\n",
        "\n"
      ],
      "metadata": {
        "id": "oMrXW_EgiEPI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Comprensión vs. Generación en Procesamiento del Lenguaje Natural (PLN)\n",
        "\n",
        "Este documento explica en detalle las diferencias entre NLU y NLG, dos grandes ramas del PLN, con ejemplos prácticos en Python.\n",
        "\n",
        "---\n",
        "\n",
        "## Natural Language Understanding (NLU)\n",
        "\n",
        "### ¿Qué es?\n",
        "\n",
        "NLU se enfoca en **entender** el texto natural. Busca interpretar el significado, contexto, intención y estructura gramatical.\n",
        "\n",
        "---\n",
        "\n",
        "### Componentes clave\n",
        "\n",
        "- **Tokenización y análisis morfológico**\n",
        "- **Etiquetado gramatical (POS tagging)**\n",
        "- **Reconocimiento de entidades nombradas (NER)**\n",
        "- **Análisis de sentimientos**\n",
        "- **Desambiguación semántica**\n",
        "- **Detección de intención**\n",
        "\n",
        "---\n",
        "\n",
        "### Ejemplo en Python (con spaCy)\n",
        "\n",
        "```python\n",
        "!pip install spacy\n",
        "!python -m spacy download es_core_news_md\n",
        "\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"es_core_news_md\")\n",
        "texto = \"Michael viajará a Santiago el próximo lunes.\"\n",
        "\n",
        "doc = nlp(texto)\n",
        "\n",
        "# Tokenización y etiquetas gramaticales\n",
        "for token in doc:\n",
        "    print(token.text, token.pos_)\n",
        "\n",
        "# Entidades nombradas\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Natural Language Generation (NLG)\n",
        "\n",
        "### ¿Qué es?\n",
        "\n",
        "NLG genera texto comprensible por humanos a partir de estructuras de datos o comandos.\n",
        "\n",
        "---\n",
        "\n",
        "### Etapas\n",
        "\n",
        "1. **Content Planning**: qué decir\n",
        "2. **Text Structuring**: cómo organizar\n",
        "3. **Sentence Planning**: qué palabras usar\n",
        "4. **Surface Realization**: producir texto final\n",
        "\n",
        "---\n",
        "\n",
        "### Aplicaciones\n",
        "\n",
        "- Respuestas automáticas\n",
        "- Traducción automática\n",
        "- Resúmenes automáticos\n",
        "- Reportes y narrativas de datos\n",
        "- Autocompletado\n",
        "\n",
        "---\n",
        "\n",
        "### Ejemplo en Python (con Hugging Face GPT-2)\n",
        "\n",
        "```python\n",
        "!pip install transformers\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "texto_inicial = \"El futuro de la inteligencia artificial es\"\n",
        "resultado = generator(texto_inicial, max_length=40, num_return_sequences=1)\n",
        "print(resultado[0]['generated_text'])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Comparación entre NLU y NLG\n",
        "\n",
        "| Aspecto       | NLU                                  | NLG                              |\n",
        "|---------------|---------------------------------------|----------------------------------|\n",
        "| Objetivo      | Entender e interpretar                | Generar y redactar               |\n",
        "| Entrada       | Texto en lenguaje natural             | Datos estructurados o prompts    |\n",
        "| Salida        | Estructuras, etiquetas, categorías    | Texto legible para humanos       |\n",
        "| Ejemplos      | Clasificación, análisis de sentimientos | Resúmenes, respuestas automáticas |\n",
        "\n",
        "---\n",
        "\n",
        "## Ejercicio Propuesto\n",
        "\n",
        "### Parte 1: NLU\n",
        "\n",
        "Analiza la siguiente oración:\n",
        "\n",
        "```python\n",
        "texto = \"El Galaxy S25 de Samsung es increíble.\"\n",
        "```\n",
        "\n",
        "- Extrae las entidades (producto, empresa).\n",
        "- Determina el sentimiento (usando un modelo Hugging Face si se desea).\n",
        "\n",
        "---\n",
        "\n",
        "### Parte 2: NLG\n",
        "\n",
        "Genera una oración que diga:\n",
        "\n",
        "> El producto Galaxy S25 de Samsung ha sido calificado como increíble.\n",
        "\n",
        "Puedes practicar con plantillas o con modelos preentrenados.\n"
      ],
      "metadata": {
        "id": "cW-cint9in69"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "axRXSefFiBZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## 3. Tareas de PLN\n",
        "\n",
        "- **Tokenización**: separar texto en unidades (palabras, frases).\n",
        "- **Etiquetado gramatical (POS tagging)**: asignar categoría gramatical a cada palabra.\n",
        "- **Reconocimiento de entidades nombradas (NER)**: detectar nombres de personas, lugares, etc.\n",
        "- **Parsing**: análisis sintáctico.\n",
        "- **Resolución de correferencias**: identificar a qué refiere un pronombre.\n",
        "\n",
        "### Ejercicio en Python\n",
        "```python\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "texto = \"Envíale un mensaje a Michael diciéndole que llegaré tarde a la reunión.\"\n",
        "tokens = word_tokenize(texto)\n",
        "print(tokens)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Etiquetado gramatical y ambigüedad\n",
        "\n",
        "El POS tagging puede requerir contexto para decidir el rol gramatical.\n",
        "\n",
        "### Ejercicio en Python\n",
        "```python\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "tags = nltk.pos_tag(tokens)\n",
        "print(tags)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Probabilidad léxica y contextual\n",
        "\n",
        "- **P(POS | palabra)**: probabilidad de que una palabra tenga una categoría.\n",
        "- **P(POS₂ | POS₁)**: probabilidad de una categoría dada la anterior.\n",
        "\n",
        "La categoría más probable se selecciona como aquella que maximiza el producto de ambas probabilidades.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Aprendizaje profundo en PLN\n",
        "\n",
        "Modelos basados en redes neuronales (como RNN o Transformers) capturan contexto de toda la oración y no solo de palabras individuales.\n",
        "\n",
        "Esto mejora tareas como:\n",
        "\n",
        "- Traducción automática\n",
        "- Resumen de texto\n",
        "- Respuesta a preguntas\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Datos secuenciales\n",
        "\n",
        "Los textos son datos secuenciales. El orden importa.\n",
        "\n",
        "Otros ejemplos:\n",
        "\n",
        "- Audio\n",
        "- ECG\n",
        "- ADN\n",
        "- Series de tiempo\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Corpus\n",
        "\n",
        "Un corpus es una colección de textos usada para análisis o entrenamiento.\n",
        "\n",
        "### Ejercicio en Python\n",
        "```python\n",
        "from nltk.corpus import gutenberg\n",
        "nltk.download('gutenberg')\n",
        "\n",
        "corpus_text = gutenberg.raw('austen-emma.txt')\n",
        "print(corpus_text[:300])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Tokenización avanzada\n",
        "\n",
        "### NLTK\n",
        "```python\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "texto = \"Hola. Esto es un ejemplo. Contiene varias oraciones.\"\n",
        "print(sent_tokenize(texto))\n",
        "```\n",
        "\n",
        "### Keras\n",
        "```python\n",
        "from keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "secuencia = text_to_word_sequence(\"La casa es bonita, pero está vacía.\")\n",
        "print(secuencia)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Actividades sugeridas\n",
        "\n",
        "1. Tokenizar el texto completo de un corpus (como `austen-emma.txt`) y contar las palabras.\n",
        "2. Realizar POS tagging y contar cuántos verbos hay.\n",
        "3. Crear un modelo simple de clasificación de textos con TF-IDF + Naive Bayes (usando `sklearn`).\n",
        "4. Implementar una visualización básica de frecuencia de palabras usando `matplotlib`.\n"
      ],
      "metadata": {
        "id": "jGtlbFZIifG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.nltk.org/book/"
      ],
      "metadata": {
        "id": "isCu6NSFigBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.researchgate.net/publication/220691633_Natural_Language_Processing_with_Python"
      ],
      "metadata": {
        "id": "dmmRO8_fkIEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://jalammar.github.io/illustrated-transformer/"
      ],
      "metadata": {
        "id": "UPrqoZ3NjVkS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xxZR_nrzjUbQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}