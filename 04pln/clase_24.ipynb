{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/davidlealo/sic_ai_2025_jun/blob/main/04pln/clase_24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Udh2Z0UShV7v"
   },
   "source": [
    "\n",
    "# Procesamiento del Lenguaje Natural (PLN) - Slides 47 a 60\n",
    "\n",
    "Este documento explica en profundidad los contenidos presentados en las diapositivas 47 a 60 del capítulo 7 sobre PLN, junto con ejercicios prácticos en Python.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Flujo de trabajo en PLN\n",
    "\n",
    "El pipeline estándar del PLN incluye:\n",
    "\n",
    "1. **Adquisición de datos**: recopilación de corpus (textos).\n",
    "2. **Preprocesamiento**: limpieza del texto (tokenización, stopwords, lematización).\n",
    "3. **Modelado**: representación del texto (TF-IDF, embeddings).\n",
    "4. **Aprendizaje automático**: entrenamiento de modelos de clasificación, etiquetado, etc.\n",
    "5. **Despliegue**: puesta en producción del modelo.\n",
    "6. **EDA (Exploración de datos)**: análisis transversal del texto antes y después del preprocesamiento.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 644
    },
    "id": "-U26Vo2RhT7v",
    "outputId": "6a068f3e-9239-4691-b001-f2072f95d564"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (0.21)\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"414pt\" height=\"439pt\"\n",
       " viewBox=\"0.00 0.00 413.75 438.70\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 434.7)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-434.7 409.75,-434.7 409.75,4 -4,4\"/>\n",
       "<!-- A -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>A</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"98.99\" cy=\"-403.83\" rx=\"98.99\" ry=\"26.74\"/>\n",
       "<text text-anchor=\"middle\" x=\"98.99\" y=\"-407.63\" font-family=\"Times,serif\" font-size=\"14.00\">Adquisición de datos</text>\n",
       "<text text-anchor=\"middle\" x=\"98.99\" y=\"-392.63\" font-family=\"Times,serif\" font-size=\"14.00\">Recolección de corpus</text>\n",
       "</g>\n",
       "<!-- B -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>B</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"171.99\" cy=\"-299.09\" rx=\"103.48\" ry=\"26.74\"/>\n",
       "<text text-anchor=\"middle\" x=\"171.99\" y=\"-302.89\" font-family=\"Times,serif\" font-size=\"14.00\">Preprocesamiento</text>\n",
       "<text text-anchor=\"middle\" x=\"171.99\" y=\"-287.89\" font-family=\"Times,serif\" font-size=\"14.00\">Limpieza y preparación</text>\n",
       "</g>\n",
       "<!-- A&#45;&gt;B -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>A&#45;&gt;B</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M117.23,-377.17C126.46,-364.18 137.79,-348.22 147.75,-334.21\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"150.8,-335.96 153.74,-325.78 145.1,-331.9 150.8,-335.96\"/>\n",
       "</g>\n",
       "<!-- C -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>C</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"240.99\" cy=\"-208.35\" rx=\"106.55\" ry=\"26.74\"/>\n",
       "<text text-anchor=\"middle\" x=\"240.99\" y=\"-212.15\" font-family=\"Times,serif\" font-size=\"14.00\">Modelado</text>\n",
       "<text text-anchor=\"middle\" x=\"240.99\" y=\"-197.15\" font-family=\"Times,serif\" font-size=\"14.00\">Representación del texto</text>\n",
       "</g>\n",
       "<!-- B&#45;&gt;C -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>B&#45;&gt;C</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M191.95,-272.42C199.1,-263.23 207.28,-252.71 214.87,-242.95\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"217.71,-245 221.08,-234.96 212.18,-240.71 217.71,-245\"/>\n",
       "</g>\n",
       "<!-- D -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>D</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"240.99\" cy=\"-117.61\" rx=\"103.89\" ry=\"26.74\"/>\n",
       "<text text-anchor=\"middle\" x=\"240.99\" y=\"-121.41\" font-family=\"Times,serif\" font-size=\"14.00\">Aprendizaje automático</text>\n",
       "<text text-anchor=\"middle\" x=\"240.99\" y=\"-106.41\" font-family=\"Times,serif\" font-size=\"14.00\">Entrenamiento</text>\n",
       "</g>\n",
       "<!-- C&#45;&gt;D -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>C&#45;&gt;D</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M240.99,-181.21C240.99,-172.9 240.99,-163.56 240.99,-154.69\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"244.5,-154.57 240.99,-144.57 237.5,-154.57 244.5,-154.57\"/>\n",
       "</g>\n",
       "<!-- E -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>E</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"240.99\" cy=\"-26.87\" rx=\"94.09\" ry=\"26.74\"/>\n",
       "<text text-anchor=\"middle\" x=\"240.99\" y=\"-30.67\" font-family=\"Times,serif\" font-size=\"14.00\">Despliegue</text>\n",
       "<text text-anchor=\"middle\" x=\"240.99\" y=\"-15.67\" font-family=\"Times,serif\" font-size=\"14.00\">Puesta en producción</text>\n",
       "</g>\n",
       "<!-- D&#45;&gt;E -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>D&#45;&gt;E</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M240.99,-90.47C240.99,-82.16 240.99,-72.82 240.99,-63.95\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"244.5,-63.83 240.99,-53.83 237.5,-63.83 244.5,-63.83\"/>\n",
       "</g>\n",
       "<!-- F -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>F</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"310.99\" cy=\"-403.83\" rx=\"94.51\" ry=\"26.74\"/>\n",
       "<text text-anchor=\"middle\" x=\"310.99\" y=\"-407.63\" font-family=\"Times,serif\" font-size=\"14.00\">EDA</text>\n",
       "<text text-anchor=\"middle\" x=\"310.99\" y=\"-392.63\" font-family=\"Times,serif\" font-size=\"14.00\">Exploración de Datos</text>\n",
       "</g>\n",
       "<!-- F&#45;&gt;B -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>F&#45;&gt;B</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M278.05,-378.48C258.72,-364.19 234.11,-346 213.46,-330.74\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"215.3,-327.75 205.18,-324.62 211.14,-333.38 215.3,-327.75\"/>\n",
       "<text text-anchor=\"middle\" x=\"268.49\" y=\"-347.76\" font-family=\"Times,serif\" font-size=\"14.00\">Apoya</text>\n",
       "</g>\n",
       "<!-- F&#45;&gt;C -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>F&#45;&gt;C</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M308.68,-376.9C305.58,-349.88 298.67,-306.74 283.99,-272.22 279.76,-262.26 273.84,-252.24 267.74,-243.22\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"270.51,-241.07 261.89,-234.91 264.79,-245.11 270.51,-241.07\"/>\n",
       "<text text-anchor=\"middle\" x=\"318.49\" y=\"-295.39\" font-family=\"Times,serif\" font-size=\"14.00\">Apoya</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7cedd3d4f450>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install graphviz\n",
    "\n",
    "from graphviz import Digraph\n",
    "\n",
    "dot = Digraph()\n",
    "\n",
    "dot.node('A', 'Adquisición de datos\\nRecolección de corpus')\n",
    "dot.node('B', 'Preprocesamiento\\nLimpieza y preparación')\n",
    "dot.node('C', 'Modelado\\nRepresentación del texto')\n",
    "dot.node('D', 'Aprendizaje automático\\nEntrenamiento')\n",
    "dot.node('E', 'Despliegue\\nPuesta en producción')\n",
    "dot.node('F', 'EDA\\nExploración de Datos')\n",
    "\n",
    "dot.edges(['AB', 'BC', 'CD', 'DE'])\n",
    "dot.edge('F', 'B', label='Apoya')\n",
    "dot.edge('F', 'C', label='Apoya')\n",
    "\n",
    "dot.render('pln_flowchart', format='png', cleanup=False)\n",
    "dot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZ-RxY7womEc"
   },
   "source": [
    "![Comparación modelos GPT - Bert](https://miro.medium.com/v2/resize:fit:1400/1*U_GQqBNuEXvsJWV7kAJeDQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMrXW_EgiEPI"
   },
   "source": [
    "\n",
    "## 2. Comprensión vs. Generación\n",
    "\n",
    "- **NLU (Natural Language Understanding)**: entender el lenguaje.\n",
    "  - Ej: análisis de sentimientos, NER, clasificación.\n",
    "- **NLG (Natural Language Generation)**: generar texto.\n",
    "  - Ej: respuestas automáticas, generación de resúmenes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cW-cint9in69"
   },
   "source": [
    "\n",
    "# Comprensión vs. Generación en Procesamiento del Lenguaje Natural (PLN)\n",
    "\n",
    "Este documento explica en detalle las diferencias entre NLU y NLG, dos grandes ramas del PLN, con ejemplos prácticos en Python.\n",
    "\n",
    "---\n",
    "\n",
    "## Natural Language Understanding (NLU)\n",
    "\n",
    "### ¿Qué es?\n",
    "\n",
    "NLU se enfoca en **entender** el texto natural. Busca interpretar el significado, contexto, intención y estructura gramatical.\n",
    "\n",
    "---\n",
    "\n",
    "### Componentes clave\n",
    "\n",
    "- **Tokenización y análisis morfológico**\n",
    "- **Etiquetado gramatical (POS tagging)**\n",
    "- **Reconocimiento de entidades nombradas (NER)**\n",
    "- **Análisis de sentimientos**\n",
    "- **Desambiguación semántica**\n",
    "- **Detección de intención**\n",
    "\n",
    "---\n",
    "\n",
    "### Ejemplo en Python (con spaCy)\n",
    "\n",
    "```python\n",
    "!pip install spacy\n",
    "!python -m spacy download es_core_news_md\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "texto = \"Michael viajará a Santiago el próximo lunes.\"\n",
    "\n",
    "doc = nlp(texto)\n",
    "\n",
    "# Tokenización y etiquetas gramaticales\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)\n",
    "\n",
    "# Entidades nombradas\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Natural Language Generation (NLG)\n",
    "\n",
    "### ¿Qué es?\n",
    "\n",
    "NLG genera texto comprensible por humanos a partir de estructuras de datos o comandos.\n",
    "\n",
    "---\n",
    "\n",
    "### Etapas\n",
    "\n",
    "1. **Content Planning**: qué decir\n",
    "2. **Text Structuring**: cómo organizar\n",
    "3. **Sentence Planning**: qué palabras usar\n",
    "4. **Surface Realization**: producir texto final\n",
    "\n",
    "---\n",
    "\n",
    "### Aplicaciones\n",
    "\n",
    "- Respuestas automáticas\n",
    "- Traducción automática\n",
    "- Resúmenes automáticos\n",
    "- Reportes y narrativas de datos\n",
    "- Autocompletado\n",
    "\n",
    "---\n",
    "\n",
    "### Ejemplo en Python (con Hugging Face GPT-2)\n",
    "\n",
    "```python\n",
    "!pip install transformers\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "texto_inicial = \"El futuro de la inteligencia artificial es\"\n",
    "resultado = generator(texto_inicial, max_length=40, num_return_sequences=1)\n",
    "print(resultado[0]['generated_text'])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Comparación entre NLU y NLG\n",
    "\n",
    "| Aspecto       | NLU                                  | NLG                              |\n",
    "|---------------|---------------------------------------|----------------------------------|\n",
    "| Objetivo      | Entender e interpretar                | Generar y redactar               |\n",
    "| Entrada       | Texto en lenguaje natural             | Datos estructurados o prompts    |\n",
    "| Salida        | Estructuras, etiquetas, categorías    | Texto legible para humanos       |\n",
    "| Ejemplos      | Clasificación, análisis de sentimientos | Resúmenes, respuestas automáticas |\n",
    "\n",
    "---\n",
    "\n",
    "## Ejercicio Propuesto\n",
    "\n",
    "### Parte 1: NLU\n",
    "\n",
    "Analiza la siguiente oración:\n",
    "\n",
    "```python\n",
    "texto = \"El Galaxy S25 de Samsung es increíble.\"\n",
    "```\n",
    "\n",
    "- Extrae las entidades (producto, empresa).\n",
    "- Determina el sentimiento (usando un modelo Hugging Face si se desea).\n",
    "\n",
    "---\n",
    "\n",
    "### Parte 2: NLG\n",
    "\n",
    "Genera una oración que diga:\n",
    "\n",
    "> El producto Galaxy S25 de Samsung ha sido calificado como increíble.\n",
    "\n",
    "Puedes practicar con plantillas o con modelos preentrenados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NwtQPpSMFAfK"
   },
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download es_core_news_md\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "axRXSefFiBZO",
    "outputId": "52ef5370-5b76-4411-cb7e-1b31c3338f72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Michael PROPN\n",
      "viajará VERB\n",
      "a ADP\n",
      "Santiago PROPN\n",
      "el DET\n",
      "próximo ADJ\n",
      "lunes NOUN\n",
      ". PUNCT\n",
      "Michael PER\n",
      "Santiago LOC\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "texto = \"Michael viajará a Santiago el próximo lunes.\"\n",
    "\n",
    "doc = nlp(texto)\n",
    "\n",
    "# Tokenización y etiquetas gramaticales\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_)\n",
    "\n",
    "# Entidades nombradas\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZYIN3emFFUGB",
    "outputId": "5daf117d-1af4-49e9-ca6d-c930e68c3fa6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452,
     "referenced_widgets": [
      "a69eb89ffc434a29a35026daf54c359a",
      "757fbd22dda540b5bd4d3df019fb2434",
      "adb0253220404d1486db0988733622e2",
      "507302b17dad4d6cb0730ede3aa65c9f",
      "399083c67c744b1991129bf475e0912d",
      "5a900f3b3ce94e26ac0ba33a4fc313ad",
      "e031708d1e38484c9c31e216a881474c",
      "cd2c01ba4983434d97cf99a1ddff5d37",
      "d2ccf19030694fbcac9c6868e0e16775",
      "4ffca22e8eb2415c9a494292b8e54f2a",
      "2e1e4659e23e4ff386e95c752bf009bd",
      "9e08e75fa4d14d328c6b4192a814cfee",
      "4a0bae2d14f1444d9315c829b04b48d5",
      "cec3caae7fc244fda2b988f1694aa313",
      "c7cea82cfdd949678ae04e28350f076c",
      "7a95a3c0b99f41dd8792567a40790882",
      "72dc9a450281417d8fcfe6321f57de56",
      "9bd6a70f2dcf43a2925d0941092e13d3",
      "4f36c68f8653494c8d2d76ca1ad26df0",
      "975866721a2a45cc8c1c7e3610612b30",
      "3c512e8fe23b4db1a87991c9580f91fd",
      "0ea428c68e224b57a0de95adfd713a1c",
      "aeb051352bcb4f19a90f97d0969acd11",
      "100579605a6b41c7be13be39e88d29b5",
      "95bba7a7d78c4fb3a119a1cfac8be72d",
      "1b08337a957d4aa2b5b6eea19095ae36",
      "2e4f1c15fede4d91917283ce94d44f76",
      "1d82aad01c6c4da09fbd4215cb4b32d2",
      "5dce5bd25f184335a79964f5ac0b5f0a",
      "5007a4fa26554f7f8983351629935e48",
      "e59905e40035424d89e5405497bfdd01",
      "624ab72f140441b786513757c84b1ca6",
      "5c4e2a5b26ce4f10a50523c4af832259",
      "781edee97c3244718ff17f7bb404cbe5",
      "e4c1f6e45ef44e9fa9b4711c9d151f12",
      "a2b13508f801402c88d4711ad45ee8c5",
      "636f528d1fa74b4fb97abe7d2a395be6",
      "22b2c83b87b8465088aafb21a9ce435d",
      "2faa9b632d574aacb028e945b99fba41",
      "d9dc7752907544158a55c94f2a417027",
      "4c7c5014eb354f089364e6e02753a4d4",
      "2b8fef6b71c84b9c813ac6a2affa85d2",
      "424514ca45d64a78b2f661644a235912",
      "56abaad7dd424086b701ce88c91a8aee",
      "613555349cf443b5ab6afc6e10456f99",
      "45cf1c0eac014f9d8d87b40fb42f1f99",
      "6e3aa6c3a36649a3906b8bf0e193d748",
      "16dfab952c7849c7a6a51f365ea70a2b",
      "eae4b261008140eba91915b5ce098033",
      "7834dbc49131426ea731c29f8aa9aff3",
      "1ece772081a74af0986a45d4e241f0ac",
      "e22fc6a510624b33a53a6337a923b9f2",
      "c4cdbf7e6a01436eb9c5d192f567e95c",
      "772b36dd8fab4bdd81c82bea794f803b",
      "736f2d91a8fd48968a4f77824a957eba",
      "25f6284e709644cd90333b789401b5eb",
      "fc173daeea0448d0b9dbce1bce6cbf23",
      "e66e8403a2a44406b4bf6a81b0f25ff5",
      "d37a91f18f20445ba31f23cf29a90e35",
      "811d1a7306b741198a4aa590360180d0",
      "1b987ea513b942ac96e2e2d75a870b98",
      "fd060ca8426b45e6b154f043ca03e707",
      "55ee7161e6704013a7c7311b0939c644",
      "8065fe32785347e6b09a4f544d37cdf8",
      "457786d379af4be98a514e1ef7a73b4e",
      "2a06723b50414e09af73f97dd240c7cb",
      "f270996b674b47f886954d64136cbdcf",
      "7f96873284c14183888f0c9ac2220936",
      "273389cb20334675a9aacb0486d7a9f2",
      "b998fca51078414f8891de6b0bb03eac",
      "e3ea6885b7114de8a30ad07560fa81fc",
      "91db498fd98243ed8dfd1c6cf047858b",
      "9815345fda3049529b94c771b7cefe8f",
      "0af16acb31dd4ca488f6f92978989a4a",
      "1ccf83e6d6bb43619f62f4e3faa90f48",
      "ee6089b14085448694addbaa3dbdac9e",
      "f7456a8dd87f4bba83c32f99a5406352"
     ]
    },
    "id": "0sM8FFbtFWtM",
    "outputId": "7545d46a-ce0a-47a2-93d8-46dbdd7e0548"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a69eb89ffc434a29a35026daf54c359a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e08e75fa4d14d328c6b4192a814cfee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeb051352bcb4f19a90f97d0969acd11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "781edee97c3244718ff17f7bb404cbe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "613555349cf443b5ab6afc6e10456f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f6284e709644cd90333b789401b5eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f270996b674b47f886954d64136cbdcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=40) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El futuro de la inteligencia artificial español (SIP) / Fermi de la fermion en la libro de la fermion en la fermion de la fermion en la fermion en la fermion de la fermion de la fermion de la fermion de la fermion de la fermion de la fermion de la fermion de la fermion de la fermion de la fermion de la fermion de la fermion de la fermion de la fermion de la fermion de la fermion de la fermion de la fermion de la fermion de la fermion de la fermion de la fermion de la fermion de la fermion de la fermion de la fermion de la fermion de la fermion de la fermion de la fermion de la fermion [en] [en] [en] [en] [en] [en] [en] [en] [en] [en] [en] [en] [en] [en] [en] [en] [en] [en] [en\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "texto_inicial = \"El futuro de la inteligencia artificial es\"\n",
    "resultado = generator(texto_inicial, max_length=40, num_return_sequences=1)\n",
    "print(resultado[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "3124249e43534c86bda4fed55fcdcea5",
      "d18728541de44fc59d1206e03a6d88f8",
      "5fd2b0894d094c93bff252eaad926765",
      "1872eede8afc4affac0e20609b64a1d2",
      "e13c7aa1f55347c4a998295c9eace15f",
      "18ae02ecc4d04872aec97c0ca53c4ad9",
      "5f92d51b2b7444babef0380e9c8aeecd",
      "3e577c720d074cada99ab27a98d46d03",
      "06de7da1debe4cf2a36fb0b05e6855e4",
      "12fe5f1edb3946d8bedc5ef3880d9719",
      "0e5220db51894630a0c6e0ad0b00596d",
      "1f0d2504c7c94ab58517aae192c5a5cb",
      "e56fb30b373e4aef85a6666e3a65ad71",
      "a671df07e34649eda88847f1a32162be",
      "01fd0f238dec4d0c94a112883bd947ef",
      "7f81fd47d9064a65857ec49c082429ad",
      "1e6885b0fd9f4134b08d685d9fb50763",
      "ad991f0072af41e3b51546a4dae91408",
      "775dbc72128b4de5a17a66953db39b0a",
      "7487ed7368904905b822c04d30dc297c",
      "ab810528d2af47c79bfc29824136e3ce",
      "59f54153b2124e768c72c1cdc68bcb69"
     ]
    },
    "id": "Pzw3YupbF_Sh",
    "outputId": "7b399106-6f9e-47c1-b8c3-ee2ba2162f96"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3124249e43534c86bda4fed55fcdcea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/930 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f0d2504c7c94ab58517aae192c5a5cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/24.2G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"EleutherAI/gpt-j-6B\")\n",
    "texto_inicial = \"El futuro de la inteligencia artificial es\"\n",
    "resultado = generator(texto_inicial, max_length=60, num_return_sequences=1)\n",
    "print(resultado[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9mR8A-uIfKO"
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 371,
     "referenced_widgets": [
      "a394e6fe986f4c13a82172e34ce557ce",
      "bea98c72595345e1928de1b14cd6e8fd",
      "aa13d8ceddef41819373fdbe84be520d",
      "40ff04ce4d9a46bb8df8f7f7e4a884e9",
      "8c78f608e9ef4b9e877e122b4b7d7644",
      "539445208c464689ac95374096ddab1f",
      "7976a96495b64ea0b84f85f69815b6d1",
      "ebc85a7429164927a35418d8607bb4cb",
      "83a2916f05ec4e65ace900767c524f9b",
      "82ec32b22ad64a439014cee6f4f231a0",
      "25886a5237b44799b058f6ffa4245097",
      "6e161b2b076a40f087e681e939857651",
      "653adda153b84093a2c903001f2a83e8",
      "03796caa54f34a4399810965b03094d4",
      "91ebabbea1ef4ba2b6c9a0f1d019fed4",
      "697bf1233b2f4c4bbae56ccc6a24e17c",
      "a40a410e5b7148b7a76b85ef4fcb7764",
      "757698c9872c4d9bb84f0cb76457661b",
      "a21490ee20b84ced8c767de95e19465a",
      "36386a7aabab4743887a9a3af9061010",
      "e4b645ba502c4848b110d3e77bb01acb",
      "26eacc5bf5914f97a31fb815cd7dbd18",
      "e04a0248930f429d9102895d0ca9120f",
      "c492f0650a564cd4b809e313d604b3ec",
      "f7a02b7eb2fd44b8a94cbf9ef4da7911",
      "86e806740a8c4badb2c251839554a374",
      "391a457a272844aead2b5740aeba1df8",
      "2704873784d74e978b443e865343e78e",
      "11d6d9c226c04bea904a06917a161911",
      "da6b5821ec89475b8657dd9d0e20c50b",
      "4a2c8081709c426489c4ac0de4cf943e",
      "741951316b1142d6b8c842da81dc5f01",
      "9b695122b0ec478cb853956f6798e92d",
      "bb768cb02c014f4cb6604bfa3bce7c30",
      "843ea562a09b40a685c40cce3efe417a",
      "eb1a7fa8761547d886d543966a50f5fe",
      "7134a0b32221404f87b0b3187c966ad0",
      "4935fba451be4da9bdcf42598299b697",
      "782b8538239c4220849dfe8276e8880a",
      "8d7bf7518bcd4448895cd369e4714cdc",
      "46444b09b4f448d8b7befa65ec506a6e",
      "cdc7378a14ba44c49d05e272d8c8c1b6",
      "854146a30b024ce78db268b32d2be5b1",
      "8c880584fd604207a8a9e9f3925a8cea",
      "becdc0ee8b0144e793252588611d68ca",
      "3ecb0826401a4255ac325b5dc18dd405",
      "27ac74fa6da14f71abe4cb82bb27b769",
      "05729c55746c4939813457eaf70b4ee6",
      "c54141efc43148cea0b581d4ddd13a8c",
      "1b432df8261f4d6187979d00efbe1dde",
      "6d4642955a934f2eb03374f8cdfd3e55",
      "642e583a542c4476a68ba822ef9402b3",
      "e08f779074a746a8ad86961b2a845929",
      "596fee91413c4c788bfb5d92f3d240c0",
      "f53fa91f7f284077afaa5cda88108559"
     ]
    },
    "id": "TPAQEupqGifY",
    "outputId": "f25069fb-2029-4d45-f904-8394b12003dd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a394e6fe986f4c13a82172e34ce557ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/715 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e161b2b076a40f087e681e939857651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e04a0248930f429d9102895d0ca9120f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/222 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb768cb02c014f4cb6604bfa3bce7c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "becdc0ee8b0144e793252588611d68ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=60) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El futuro de la inteligencia artificial es un campo en el que existe un gran abanico de posibilidades y desafíos\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"bigscience/bloomz-560m\")\n",
    "texto_inicial = \"El futuro de la inteligencia artificial es\"\n",
    "resultado = generator(texto_inicial, max_length=60, num_return_sequences=1)\n",
    "print(resultado[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bLO5BWcEJQji"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"ESCRIBE TU TOKEN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ynnoVZ1aKm0c",
    "outputId": "2fbed7b8-fb72-4d4d-ae22-bf3e1693e77f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Debería dar True\n",
    "print(torch.cuda.get_device_name(0))  # Muestra el nombre de la GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293,
     "referenced_widgets": [
      "34b59b85fd25487492dee9744279e2f7",
      "bd491d7ba6db47dab645ca6b809e2206",
      "41b548f1858d4761bb6ec6e27c89be0b",
      "85caf97b9167476faeea7d3ebb054e93",
      "59884bd9a6ee4d2196032d303c216e83",
      "76da83ce11684261ac7cb0a2b4300d90",
      "2968e2f4dc5a40d2a0dc496a931c1f1a",
      "849255c45728483eb27683ae427c6c84",
      "e21e44804f7d4eeebd30a96b62d86870",
      "f30f57c1a2d8426cabecab95d77ac990",
      "46a237006a0a4af9aff4eb1cd37fa12e",
      "729f00f9851146edbfdb7e2ee5afe4eb",
      "587c36281e504ecca5c895110f237ad4",
      "1055c7c883eb45b2b5abb51e1389f747",
      "f02f424b6f6445378b43f911bedabb95",
      "695a6d61f13f4cc582ba99a2c59da77b",
      "41f68bbef9be4ebdb855769287ee5673",
      "3bdefe4ba9b0430299bdef01ab3ac7a4",
      "b119f7b74902492cadc27d02e1d85237",
      "d3f27f7150e0467b85c2fd594fb7bf26",
      "06b2d6a8ddf24db6bf3c9ed5451e310c",
      "0bafea92aee2440cb004d60bbc650f5f",
      "43ef02b12a414a5c98da9437a2c7d596",
      "71f43d38e20947f88baa0a10df0477e2",
      "8845eea901a248b88758f9a71723e263",
      "2cd5b79cfa8c4f179889ff34cfbc8145",
      "ceb89b4dd03d466d96ae1e153b81bdc3",
      "4341c759cbe1411c96592ea0f2690a65",
      "789efbba9ec140cb94428485eb90d09f",
      "3b11d375f5074a1ab42c8956174feab9",
      "d96482c22ac2478ea8722c69ef242617",
      "a98d412275d2444cbd54c7c8dcc8dad0",
      "e1c0f987f5bd463cb007c768024e840f"
     ]
    },
    "id": "0OuuDYUNGZwD",
    "outputId": "789a36a1-8fc0-40e5-efd0-d4ae017046aa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34b59b85fd25487492dee9744279e2f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "729f00f9851146edbfdb7e2ee5afe4eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ef02b12a414a5c98da9437a2c7d596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.1\", device=0)\n",
    "texto_inicial = \"El futuro de la inteligencia artificial es\"\n",
    "resultado = generator(texto_inicial, max_length=60, num_return_sequences=1)\n",
    "print(resultado[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339,
     "referenced_widgets": [
      "3e470aa75fad4405a8e2cbf15487e8d8",
      "0e31d281afcd47a69aa698a64bf9451b",
      "db857917af1e4503a18a85ee87ab9343",
      "26e5b9dc29ab462e8a8478e7423d2d11",
      "9f66b15869324cbc817ccc0ed7db7464",
      "521d2e2d5ec14488bfd9e9c2bd1a52c4",
      "479283f905bd44ee9d09ca61e4de47a5",
      "2a54ac41bcf042aa8e6aa024d1b8f6fd",
      "0308792ad3c448cc8e3003b54f958edf",
      "4aa1c62db10249edb4ca63ea6401317a",
      "952d743ee4b6496c97e5dc5b91c1f739",
      "6e2428156cf14f2988b556bdafdb0f03",
      "71a06655d94343d4bc2e4ff2875bdcfc",
      "20c1a2fafabc4f7ea3924fc49ee9e4c5",
      "ddd426f067df43b0b932546904bb64c7",
      "68dd9dc0b51948378bb5feb43c2f5f10",
      "d7c104a4119d4758b78e2253e813c03a",
      "c4087d1f0f9444c6a1993aeb5453a843",
      "aa2d8fdf78204ccbaef0aa7174ef9fe7",
      "226f721f5ffa40fa94d665ccf7d04153",
      "eb14b338b70f4c9fa93403ce5008ff18",
      "b0bd18effc3b48e98561f345bd7d0832",
      "6e4807ec2cda4a24b73cf8780623f69c",
      "91e7b863f49e43afbff241317272f96c",
      "cff60298fb0c451bbc5cb350bbc9d8ca",
      "a31205c7e5e64413addee620fbda901b",
      "87a7c0e78e334b8896575f34d2d8e532",
      "943c91ba9e6a4e79a3c1221fab57dfa1",
      "2f589bada2244b35af4fe76e1cb19eda",
      "2650c4fea9374a50af388ab579c506da",
      "991b285bf99f4300af2c467fd6f588b1",
      "443128198dff4bbb92af4d159a866ea0",
      "f005d638723e4fc2ab78b57ef433159c",
      "c7978e9fb05c4484a9133a3697a854ff",
      "ec895fb363fe4b9e969eef70a8b4228d",
      "29a175f6c338401ca24c4c69d168799b",
      "f48bf40b603b4cf186122418732badbc",
      "a000cd3d0c3e4260a4c9e2872660f667",
      "53b346b395b3479793fea6d381989e58",
      "59d9a9edbb7c45928f6c470d9804fc65",
      "02033bf4d26e4f84918c6e28fcf5534d",
      "f5ca80f42a644f9494c60b9a3ec9e461",
      "b0ba250d50f74b7a9162dbd6ca162677",
      "ce1bac463b824cd39c4933ea058a2f86"
     ]
    },
    "id": "bH7YP3TzGYBD",
    "outputId": "82d540e3-b232-40af-df2b-a121bc7ab53e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e470aa75fad4405a8e2cbf15487e8d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/681 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e2428156cf14f2988b556bdafdb0f03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4807ec2cda4a24b73cf8780623f69c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7978e9fb05c4484a9133a3697a854ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=61) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El futuro de la inteligencia artificial es la de la inteligencia humana, y es más una ciencia de la comunicación, es más una inteligencia de la comunicación. \n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"PlanTL-GOB-ES/gpt2-base-bne\")\n",
    "texto_inicial = \"El futuro de la inteligencia artificial es\"\n",
    "resultado = generator(texto_inicial, max_length=60, num_return_sequences=1)\n",
    "print(resultado[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 533,
     "referenced_widgets": [
      "0ab3ab35f5e842d2a2a80cf50a70c0c8",
      "9183f4679b5a4eb68a6fc3c204517ae1",
      "3b08cb81030b4fe58beecc9299230e5d",
      "a8bd18afe5564f0894a502c24c7e17e4",
      "43890989cc4949ec87666990cf8c0921",
      "2fafdda0d3294d46bd6bdfab19ffce8c",
      "e55efdcdece94f12961670e12d2db4b4",
      "dac5a956803042a2a50d87cc0048030d",
      "2d2af44f0c3e4b41b48e60f80e68fabf",
      "f7913a42a6f54d4e91fb3e5496d98cc1",
      "bbaf78539b2f4c658745cd1b762d692e",
      "d5bb68086ffe47f3b0f87cf8d0f97520",
      "567c6f9cf08c42b8b385b91ef8704701",
      "bd7d6513dbcf4b888c804743dc7ffa14",
      "24973044c15f440da789e88d7a908fa9",
      "87d17483c0114cd7851fa66857f04939",
      "1ced00288431417db4ae9a09f8750866",
      "151e0b475e7d476da6e695774e89e965",
      "fd046a599b424d03a542d9671bf8545e",
      "b5194b4f806d416cb7df321663c087b9",
      "f7f4de5fa4d141de90bae4f4a7abed04",
      "de6665546a134605b7081cdebada1787",
      "6092e2271e334c3db1133b906e19b2c9",
      "d0f2b6b8d5cc4598b04fd94731f5fd8e",
      "7b6d4d5881c749989b31fa66e6198e40",
      "165280de3ab04d619c777e80d3f637f6",
      "16582b7ee8cc457696d8b0e629aff0d9",
      "63292061dd1f435f8c5355214e2eb76f",
      "9e52a74ae4104aa69782a5f7dd4d7754",
      "1a5e3c0c2e47441baab8f2d4467b39df",
      "babade6abe8e48259ab81fa90f49abc7",
      "ae891ef8795143cf813020070ec27777",
      "1b6c7064cf934d14acb90f6c0a76b61e",
      "3e76d72ae5324969aa402a93f1d176b8",
      "8374cf845825460ea6029dc4ad92b0eb",
      "834ed352898646209f1eccb380ee2309",
      "eb7f1c9fc88b41338346c821439e53be",
      "e9c65550cb944e159eb1c7f675cd438a",
      "f08e39b336b04b8ab6a7099490cc4d4b",
      "49246f48bab049709ecb942b39db545f",
      "6aa7c01ac3d947d9b4003c8ee533befa",
      "f9c8d80f3bf647cdadf734a83e137e2b",
      "cdd08389b50a4112bce1526986c5c8ce",
      "6a6d1aae4e9f44838fefff15a126063d",
      "0ed0ea0f3ebb450da83cd6b81753c440",
      "16c9b45994834ba2aafef6742cc27399",
      "516971be1ebf4f1a974cc55ccaa53811",
      "4fb16fdbf2bc4854b079dfa0fe625cf0",
      "3358874da38e47bb9e90abbb44a8a456",
      "1ca3527e7aee4913997deb3f8d27587a",
      "91ad60a720564c4b94c6268c04be96ea",
      "cb454ffce34d4866a854239a9aaa5c16",
      "03fc00c7e2914b44a7d6881ef2ba280c",
      "fde4582b8d5a40e9a698ee72d8b8e899",
      "eb0f97d279a44b09bc31f7fc3e6109ba",
      "d2d4f9fc0aa1484fbf54b502c8251594",
      "c32616524a65469b9bff9ffada5d99ba",
      "5798d23a9699441aadcf0dad44d210e8",
      "831eb6302c644044a574c07394eac9aa",
      "f118911cecc44192b9ecf552e98cb773",
      "bd2ce0a3e7574ffc9e27d9d91cb01e95",
      "580e12108952439ca48cd9ce5370e45d",
      "80901aec14204c2ca66ebcd877372640",
      "02de368425cc46e688d56c984b3305b3",
      "4d168e3702f84ca5a324860d3a094ceb",
      "4ce92e57229c4709a7235abcf46720d6",
      "916546967a0d47e6be7e8037bfcea4e9",
      "b7809292618b4870ad16b53fec5f8652",
      "c318bf54353945ca9bafaf9922c690b9",
      "beb9fb3d952f418fb966924a6fb4a0d4",
      "765b86a200f646daadc3eeccf495cce4",
      "3890cca377a34468aed7e283d3d18fed",
      "16e99785049f4ca5b59459c0c6590c49",
      "dc1e534ebc2e47558bd469bb677c59a0",
      "f7a4241720b64b038870e42da37aae9e",
      "24a8f75b97de4a9582554624b803f8cc",
      "2a61218d4374414883e9efae7b64ca30",
      "022924c3283b4c02a82d6456dde0247c",
      "a2ba08f91bd0477ab082e58eb69071e0",
      "aa37c7c1b83a4e45bdad5fc6422669b7",
      "d9d0d8a1f86b42c098a9e689121e0a52",
      "520b346216184ce981a58ebd662da010",
      "99adc5fb18964f488adcc8bb3e06cf44",
      "138ec2ef7f7048eda7716dae135184bd",
      "15b79cf441bf40aaa5cdfe15cc3f80b1",
      "e3fb01e9b92f4da580aae5b4b159d938",
      "78dd90177edd4cd79c8bd2d9eff8047a",
      "5827a063e078400d9019cc5dd506085d",
      "3b2b2d3adb92453b8aa1bb5e6add4fc2",
      "74577793fbe04cd3a6289b88a4fe2992",
      "d06e55c12268494185f181658df31b2e",
      "5ab6490f8ae848d1b78bee143fea376b",
      "a6048a8d8a6e4eb59d3f6f8b10b91609",
      "3a6f724637f34609a6f774093ea70635",
      "81285ef1b276442091009a130ba62deb",
      "48b233ef0a8c421b9ba8d2b64eb64e67",
      "ab708f6cc9b74939ad5ff74064cedc5c",
      "4ba809fcdbad49c399c1ca51565147ee",
      "fb42f53fe3f444b5b7981a7192018499",
      "07316e2f51d8454bb245551d49e0230a",
      "f479e1059c8d42e1a4345ae5cc230427",
      "6b82b9c8739747ff9a2a0c198425e6a8",
      "47154196194647fdbdfacd17f99043e6",
      "cfff2fea9e13495c91857457c756e4f8",
      "04a7032a92674d7ca0bb042d60e00817",
      "d13afd374ebb43ea9c693672261bb90f",
      "2d591bbb3ad84173a9d20aefda037615",
      "eb221c4e22704f9ca3eaafecad00bd29",
      "60c2094e0dc046e39880bb745c2108c3",
      "888f2d51ac024c908936e7ac6690da84"
     ]
    },
    "id": "rMTAp8lnG843",
    "outputId": "9b56f9e0-072b-477e-9766-bde7a050d998"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ab3ab35f5e842d2a2a80cf50a70c0c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/742 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5bb68086ffe47f3b0f87cf8d0f97520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6092e2271e334c3db1133b906e19b2c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e76d72ae5324969aa402a93f1d176b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00007.safetensors:   0%|          | 0.00/9.75G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed0ea0f3ebb450da83cd6b81753c440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00007.safetensors:   0%|          | 0.00/9.77G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2d4f9fc0aa1484fbf54b502c8251594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00007.safetensors:   0%|          | 0.00/9.75G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "916546967a0d47e6be7e8037bfcea4e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00007.safetensors:   0%|          | 0.00/9.75G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022924c3283b4c02a82d6456dde0247c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00007.safetensors:   0%|          | 0.00/9.75G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b2b2d3adb92453b8aa1bb5e6add4fc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00007.safetensors:   0%|          | 0.00/9.75G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07316e2f51d8454bb245551d49e0230a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00007.safetensors:   0%|          | 0.00/7.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"nvidia/OpenReasoning-Nemotron-32B\")\n",
    "texto_inicial = \"El futuro de la inteligencia artificial es\"\n",
    "resultado = generator(texto_inicial, max_length=60, num_return_sequences=1)\n",
    "print(resultado[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nirg9MrVG7i-",
    "outputId": "c1589f8c-5e3b-4adf-c502-6f5f2369408d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Envíale', 'un', 'mensaje', 'a', 'Michael', 'diciéndole', 'que', 'llegaré', 'tarde', 'a', 'la', 'reunión', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "texto = \"Envíale un mensaje a Michael diciéndole que llegaré tarde a la reunión.\"\n",
    "tokens = word_tokenize(texto)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vc7aeFruMYhY",
    "outputId": "c64cae59-dc0f-43fa-d686-58d10d755e7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n",
      "[nltk_data] Downloading package tagsets_json to /root/nltk_data...\n",
      "[nltk_data]   Unzipping help/tagsets_json.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('tagsets')\n",
    "nltk.download('tagsets_json')\n",
    "\n",
    "nltk.help.upenn_tagset()  # Muestra todas las etiquetas POS disponibles y sus significados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5qD7EKdyMMgv",
    "outputId": "2c3f2518-2887-4063-8e9b-1f942363ea78"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Envíale', 'NNP'), ('un', 'JJ'), ('mensaje', 'VBD'), ('a', 'DT'), ('Michael', 'NNP'), ('diciéndole', 'NN'), ('que', 'NN'), ('llegaré', 'NN'), ('tarde', 'VBD'), ('a', 'DT'), ('la', 'NN'), ('reunión', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "tags = nltk.pos_tag(tokens)\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rc5FFKPVMuOR",
    "outputId": "9e34a828-cba8-480a-afb3-2682fc0192b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.7.14)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Collecting es-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: es-core-news-sm\n",
      "Successfully installed es-core-news-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download es_core_news_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CZyMEr9PMw6e",
    "outputId": "0806936b-f89a-4728-accc-cbc95bacd05d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Envíale: VERB (VERB)\n",
      "un: DET (DET)\n",
      "mensaje: NOUN (NOUN)\n",
      "a: ADP (ADP)\n",
      "Michael: PROPN (PROPN)\n",
      "diciéndole: VERB (VERB)\n",
      "que: SCONJ (SCONJ)\n",
      "llegaré: VERB (VERB)\n",
      "tarde: ADV (ADV)\n",
      "a: ADP (ADP)\n",
      "la: DET (DET)\n",
      "reunión: NOUN (NOUN)\n",
      ".: PUNCT (PUNCT)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Cargar modelo para español\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "texto = \"Envíale un mensaje a Michael diciéndole que llegaré tarde a la reunión.\"\n",
    "doc = nlp(texto)\n",
    "\n",
    "for token in doc:\n",
    "    print(f\"{token.text}: {token.pos_} ({token.tag_})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NwQmgp12NEwX",
    "outputId": "ccd89573-1858-4e60-f73d-c5cd74e3aeb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stanza\n",
      "  Downloading stanza-1.10.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting emoji (from stanza)\n",
      "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from stanza) (2.0.2)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.11/dist-packages (from stanza) (5.29.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from stanza) (2.32.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from stanza) (3.5)\n",
      "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from stanza) (2.6.0+cu124)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from stanza) (4.67.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (4.14.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (2025.3.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.3.0->stanza)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.3.0->stanza)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.3.0->stanza)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.3.0->stanza)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.3.0->stanza)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.3.0->stanza)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.3.0->stanza)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.3.0->stanza)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.3.0->stanza)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.3.0->stanza)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.3.0->stanza) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (2025.7.14)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.3.0->stanza) (3.0.2)\n",
      "Downloading stanza-1.10.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m124.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, emoji, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stanza\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed emoji-2.14.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 stanza-1.10.1\n"
     ]
    }
   ],
   "source": [
    "!pip install stanza\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 660,
     "referenced_widgets": [
      "2413994fd66a4bb1aebaf0e489129deb",
      "d324a468a31d4a37b077367725989d42",
      "3523bd9397ac4802946092d5320a4a3c",
      "28d03dac10fe4f4aa0291136d6d95cc9",
      "c35533a443de4098abb854153f455c7c",
      "4bcce55bc4c24f1e81759e7af341cfef",
      "58732f5c260845498ba86e012c5a814f",
      "42b6123f35424ec6aa57c6ffa6e8ac26",
      "77f7181cc1864adaa9396603981f0ea2",
      "dcf692c382674ace835148fc8374b0f2",
      "7c9bb4127da44bbbb5e958848b93ce44",
      "14f76dd8499d406da1083061f07fdf8f",
      "8801f604de184db3ba5c063e896aea99",
      "aff8f75b16ad438294b1c640521c5428",
      "88be887e3aef40c9b7079f72e01da0c9",
      "50180ec97955412a80740cdccc8ac685",
      "a801ccbf04dc46e08dfbd451655d966d",
      "1d797d9894544b44b39d0bf974b50d7b",
      "2bcd9970bedd407bbaf2a8c9fd735f8a",
      "a6e3c160a01644dab52fcd305ac274fd",
      "66b73d55001348b39b5425ea3f212106",
      "df09d80f9c574431a9b3050890cc1c09"
     ]
    },
    "id": "4nmlRKIgNIF8",
    "outputId": "d48eeda8-7874-44b3-b945-dcbbfb5cc7f4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2413994fd66a4bb1aebaf0e489129deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
      "INFO:stanza:Downloading default packages for language: es (Spanish) ...\n",
      "INFO:stanza:File exists: /root/stanza_resources/es/default.zip\n",
      "INFO:stanza:Finished downloading models and saved to /root/stanza_resources\n",
      "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f76dd8499d406da1083061f07fdf8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
      "INFO:stanza:Loading these models for language: es (Spanish):\n",
      "===============================\n",
      "| Processor | Package         |\n",
      "-------------------------------\n",
      "| tokenize  | combined        |\n",
      "| mwt       | combined        |\n",
      "| pos       | combined_charlm |\n",
      "===============================\n",
      "\n",
      "INFO:stanza:Using device: cuda\n",
      "INFO:stanza:Loading: tokenize\n",
      "INFO:stanza:Loading: mwt\n",
      "INFO:stanza:Loading: pos\n",
      "INFO:stanza:Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El: DET (da0ms0)\n",
      "futuro: NOUN (ncms000)\n",
      "de: ADP (sps00)\n",
      "la: DET (da0fs0)\n",
      "inteligencia: NOUN (ncfs000)\n",
      "artificial: ADJ (aq0cs0)\n",
      "es: AUX (vsip3s0)\n",
      "prometedor: ADJ (aq0ms0)\n",
      ".: PUNCT (fp)\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download(\"es\")\n",
    "nlp = stanza.Pipeline(lang=\"es\", processors=\"tokenize,mwt,pos\")\n",
    "\n",
    "doc = nlp(\"El futuro de la inteligencia artificial es prometedor.\")\n",
    "for sent in doc.sentences:\n",
    "    for word in sent.words:\n",
    "        print(f\"{word.text}: {word.upos} ({word.xpos})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGtlbFZIifG-"
   },
   "source": [
    "---\n",
    "\n",
    "## 3. Tareas de PLN\n",
    "\n",
    "- **Tokenización**: separar texto en unidades (palabras, frases).\n",
    "- **Etiquetado gramatical (POS tagging)**: asignar categoría gramatical a cada palabra.\n",
    "- **Reconocimiento de entidades nombradas (NER)**: detectar nombres de personas, lugares, etc.\n",
    "- **Parsing**: análisis sintáctico.\n",
    "- **Resolución de correferencias**: identificar a qué refiere un pronombre.\n",
    "\n",
    "### Ejercicio en Python\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "texto = \"Envíale un mensaje a Michael diciéndole que llegaré tarde a la reunión.\"\n",
    "tokens = word_tokenize(texto)\n",
    "print(tokens)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Etiquetado gramatical y ambigüedad\n",
    "\n",
    "El POS tagging puede requerir contexto para decidir el rol gramatical.\n",
    "\n",
    "### Ejercicio en Python\n",
    "```python\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "tags = nltk.pos_tag(tokens)\n",
    "print(tags)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Probabilidad léxica y contextual\n",
    "\n",
    "- **P(POS | palabra)**: probabilidad de que una palabra tenga una categoría.\n",
    "- **P(POS₂ | POS₁)**: probabilidad de una categoría dada la anterior.\n",
    "\n",
    "La categoría más probable se selecciona como aquella que maximiza el producto de ambas probabilidades.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Aprendizaje profundo en PLN\n",
    "\n",
    "Modelos basados en redes neuronales (como RNN o Transformers) capturan contexto de toda la oración y no solo de palabras individuales.\n",
    "\n",
    "Esto mejora tareas como:\n",
    "\n",
    "- Traducción automática\n",
    "- Resumen de texto\n",
    "- Respuesta a preguntas\n",
    "\n",
    "---\n",
    "\n",
    "## 7. Datos secuenciales\n",
    "\n",
    "Los textos son datos secuenciales. El orden importa.\n",
    "\n",
    "Otros ejemplos:\n",
    "\n",
    "- Audio\n",
    "- ECG\n",
    "- ADN\n",
    "- Series de tiempo\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Corpus\n",
    "\n",
    "Un corpus es una colección de textos usada para análisis o entrenamiento.\n",
    "\n",
    "### Ejercicio en Python\n",
    "```python\n",
    "from nltk.corpus import gutenberg\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "corpus_text = gutenberg.raw('austen-emma.txt')\n",
    "print(corpus_text[:300])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Tokenización avanzada\n",
    "\n",
    "### NLTK\n",
    "```python\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "texto = \"Hola. Esto es un ejemplo. Contiene varias oraciones.\"\n",
    "print(sent_tokenize(texto))\n",
    "```\n",
    "\n",
    "### Keras\n",
    "```python\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "secuencia = text_to_word_sequence(\"La casa es bonita, pero está vacía.\")\n",
    "print(secuencia)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Actividades sugeridas\n",
    "\n",
    "1. Tokenizar el texto completo de un corpus (como `austen-emma.txt`) y contar las palabras.\n",
    "2. Realizar POS tagging y contar cuántos verbos hay.\n",
    "3. Crear un modelo simple de clasificación de textos con TF-IDF + Naive Bayes (usando `sklearn`).\n",
    "4. Implementar una visualización básica de frecuencia de palabras usando `matplotlib`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "isCu6NSFigBz"
   },
   "outputs": [],
   "source": [
    "# https://www.nltk.org/book/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmmRO8_fkIEi"
   },
   "outputs": [],
   "source": [
    "# https://www.researchgate.net/publication/220691633_Natural_Language_Processing_with_Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UPrqoZ3NjVkS"
   },
   "outputs": [],
   "source": [
    "# https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xxZR_nrzjUbQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMc8hR6/MnZJtB7gGkjK/gj",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
