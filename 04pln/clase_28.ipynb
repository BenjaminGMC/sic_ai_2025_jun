{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQxHG8n+Bqb/ALrgLKz8Tq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidlealo/sic_ai_2025_jun/blob/main/04pln/clase_28.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clase 28\n",
        "\n",
        "### Unidad‚ÄØ3 ‚Äì Cap√≠tulo‚ÄØ7 ¬∑ Samsung‚ÄØInnovation‚ÄØCampus‚ÄØ2025\n",
        "\n",
        "---\n",
        "\n",
        "## Objetivo\n",
        "\n",
        "Esta gu√≠a paso‚ÄØa‚ÄØpaso introduce los conceptos y t√©cnicas esenciales que viste en la Unidad‚ÄØ3 (¬´Modelo de representaci√≥n¬ª, ¬´An√°lisis de clasificaci√≥n¬ª y ¬´Modelado de temas¬ª). Incluye explicaciones detalladas, ejemplos en **Python** y ejercicios pr√°cticos. Est√° pensada para personas que ya programan pero son nuevas en PLN.\n",
        "\n",
        "---\n",
        "\n",
        "## Prerrequisitos r√°pidos\n",
        "\n",
        "| Herramienta      | Comando de instalaci√≥n     |\n",
        "| ---------------- | -------------------------- |\n",
        "| **NLTK**         | `pip install nltk`         |\n",
        "| **scikit‚Äëlearn** | `pip install scikit-learn` |\n",
        "| **gensim**       | `pip install gensim`       |\n",
        "\n",
        "> **Paso‚ÄØ0.** Crea un entorno virtual (opcional pero recomendado) y activa tu entorno antes de instalar los paquetes.\n",
        "\n",
        "---\n",
        "\n",
        "## Tabla de contenidos\n",
        "\n",
        "1. [Flujo de un proyecto de PLN](#flujo)\n",
        "2. [Pre‚Äëprocesamiento del texto](#prepro)\n",
        "3. [Modelos de representaci√≥n](#repr)\n",
        "4. [Clasificaci√≥n supervisada](#clasif)\n",
        "5. [Modelado de temas](#temas)\n",
        "6. [Proyecto integrador & ejercicios](#proyecto)\n",
        "7. [Recursos adicionales](#recursos)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## 1. Flujo de un proyecto de PLN\n",
        "\n",
        "1. **Definir el problema** ‚Üí ¬øClasificar sentimiento? ¬øDetectar spam? ¬øExtraer temas?\n",
        "2. **Recolecci√≥n de datos** ‚Üí APIs, scraping, Kaggle, archivos internos.\n",
        "3. **Pre‚Äëprocesamiento** ‚Üí Limpieza, tokenizaci√≥n, lematizaci√≥n, stopwords.\n",
        "4. **Representaci√≥n** ‚Üí BoW, TF‚ÄëIDF o embeddings.\n",
        "5. **Modelado** ‚Üí Clasificadores supervisados o modelos no supervisados (LDA, LSA).\n",
        "6. **Evaluaci√≥n** ‚Üí M√©tricas (accuracy, precisi√≥n, F1, coherencia de temas‚Ä¶).\n",
        "7. **Despliegue** ‚Üí API REST, batch, dashboard, etc.\n",
        "\n",
        "Cada paso se explica con mayor detalle a continuaci√≥n.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## 2. Pre‚Äëprocesamiento del texto\n",
        "\n",
        "### ¬øPor qu√© limpiar el texto?\n",
        "\n",
        "Los algoritmos trabajan con n√∫meros. El pre‚Äëprocesamiento normaliza la entrada y reduce ruido, mejorando la calidad de la representaci√≥n.\n",
        "\n",
        "### Paso a paso üõ†Ô∏è\n",
        "\n",
        "1. **Convertir a min√∫sculas** ‚Äì Homogeneiza el texto.\n",
        "2. **Eliminar URLs** ‚Äì No aportan significado en la mayor√≠a de tareas.\n",
        "3. **Quitar puntuaci√≥n** ‚Äì Facilita la tokenizaci√≥n.\n",
        "4. **Tokenizar** ‚Äì Dividir el texto en palabras o sub‚Äëpalabras.\n",
        "5. **Eliminar stopwords** ‚Äì Palabras muy frecuentes sin carga sem√°ntica (¬´the¬ª, ¬´y¬ª, ¬´de¬ª‚Ä¶).\n",
        "6. **Lematizar** ‚Äì Reducir palabras a su forma can√≥nica ( *running* ‚Üí *run*).\n",
        "\n",
        "```python\n",
        "import nltk, re, string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"punkt\"); nltk.download(\"wordnet\"); nltk.download(\"stopwords\")\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def clean_text(text):\n",
        "    # 1‚Äë2. normalizar y quitar URLs\n",
        "    text = re.sub(r\"https?://\\S+\", \"\", text.lower())\n",
        "\n",
        "    # 3. quitar puntuaci√≥n\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "    # 4. tokenizar\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # 5‚Äë6. stopwords + lematizar\n",
        "    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "```\n",
        "\n",
        "> **Ejercicio 2.1 ‚Äì¬†Explora el impacto**\n",
        ">\n",
        "> 1. Copia 5‚Äì10 tweets (o rese√±as IMDB).\n",
        "> 2. Aplica `clean_text` y compara texto original vs. limpiado.\n",
        "> 3. ¬øQu√© tokens se eliminaron? ¬øCu√°les se transformaron?\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## 3. Modelos de representaci√≥n\n",
        "\n",
        "### 3.0 Exploraci√≥n de frecuencia de palabras\n",
        "\n",
        "El **conteo de frecuencias** es la forma m√°s b√°sica de cuantificar texto y suele ser el *primer paso* del an√°lisis exploratorio de datos (EDA) en PLN.\n",
        "\n",
        "#### ¬øPor qu√© contar palabras?\n",
        "\n",
        "- **Panorama r√°pido del corpus**¬†‚Üí Identifica temas dominantes y vocabulario caracter√≠stico.\n",
        "- **Ley de¬†Zipf**¬†‚Üí¬†La distribuci√≥n sigue una cola larga: pocas palabras muy frecuentes y muchas raras. Conocerla ayuda a decidir filtros (`min_df`, `max_df`).\n",
        "- **Detecci√≥n de *****stopwords***** y ruido**¬†‚Üí Palabras ultra‚Äëfrecuentes pueden eliminarse para mejorar modelos.\n",
        "- **Selecci√≥n de caracter√≠sticas**¬†‚Üí Elegir el *top‚Äëk* de t√©rminos o fijar umbrales de frecuencia reduce dimensionalidad.\n",
        "- **Validaci√≥n de limpieza**¬†‚Üí N√∫meros, URLs o artefactos extra√±os destacan en los rankings.\n",
        "\n",
        "#### Paso a paso üõ†Ô∏è\n",
        "\n",
        "```python\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# 1. Corpus ya limpiado con clean_text()\n",
        "texts = [clean_text(doc) for doc in data.data]\n",
        "\n",
        "# 2. Tokenizar y aplanar\n",
        "all_tokens = []\n",
        "for t in texts:\n",
        "    all_tokens.extend(word_tokenize(t))\n",
        "\n",
        "# 3. Contar\n",
        "freq = Counter(all_tokens)\n",
        "print(freq.most_common(10))  # Top‚Äë10 palabras\n",
        "\n",
        "# 4. Visualizar\n",
        "terms, counts = zip(*freq.most_common(20))\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.bar(terms, counts)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title('Top‚Äë20 palabras m√°s frecuentes')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "> **Ejercicio¬†3.0 ‚Äì¬†Word cloud**\n",
        ">\n",
        "> 1. Instala `pip install wordcloud`.\n",
        "> 2. Genera una nube de palabras con `WordCloud().generate_from_frequencies(freq)`.\n",
        "> 3. Cambia el resultado filtrando stopwords distintas y observa el efecto.\n",
        "\n",
        "---\n",
        "\n",
        "### 3.1 Bolsa de Palabras (BoW)\n",
        "\n",
        "**Idea:** Cada documento se convierte en un vector donde la dimensi√≥n *j* es la frecuencia de la palabra *j*.\n",
        "\n",
        "```python\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = [\n",
        "    \"Cats eat fish\",\n",
        "    \"Dogs eat meat\",\n",
        "    \"Cats and dogs are pets\"\n",
        "]\n",
        "vectorizer = CountVectorizer()\n",
        "X_bow = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names_out())      # columnas\n",
        "print(X_bow.toarray())                         # matriz doc√óvocab\n",
        "```\n",
        "\n",
        "| Paso | Acci√≥n                                                  |\n",
        "| ---- | ------------------------------------------------------- |\n",
        "| 1    | Construir vocabulario ‚Üí `fit`                           |\n",
        "| 2    | Contar apariciones    ‚Üí `transform`                     |\n",
        "| 3    | Obtener matriz esparsa de tama√±o *(n\\_docs √ó n\\_terms)* |\n",
        "\n",
        "### 3.2 TF‚ÄëIDF\n",
        "\n",
        "**F√≥rmula:**\\\n",
        "\\(\\text{tf‚Äëidf}(d, t) = tf(d,t) \\times \\log \\frac{N}{df(t)}\\)\n",
        "\n",
        "- Penaliza t√©rminos muy frecuentes globalmente.\n",
        "\n",
        "```python\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vect = TfidfVectorizer()\n",
        "X_tfidf = vect.fit_transform(corpus)\n",
        "print(X_tfidf.toarray())\n",
        "```\n",
        "\n",
        "> **Ejercicio 3.1 ‚Äì¬†N‚Äëgramas y filtros**\n",
        ">\n",
        "> - Ajusta `ngram_range=(1,2)` (unigramas + bigramas).\n",
        "> - Ignora t√©rminos raros con `min_df=2`.\n",
        "> - Observa c√≥mo cambia la matriz.\n",
        "\n",
        "### 3.3 Word Embeddings (vistazo r√°pido)\n",
        "\n",
        "Representaciones densas que capturan similitud sem√°ntica.\n",
        "\n",
        "```python\n",
        "from gensim.models import Word2Vec\n",
        "sentences = [doc.split() for doc in corpus]\n",
        "model = Word2Vec(sentences, vector_size=50, window=5, min_count=1, workers=4)\n",
        "print(model.wv[\"cats\"][:10])  # 10 primeras dimensiones\n",
        "```\n",
        "\n",
        "> Para ir m√°s lejos: `sentence-transformers`, `spaCy`.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## 4. Clasificaci√≥n supervisada\n",
        "\n",
        "### Caso de uso: Clasificar art√≠culos de noticias ¬´espacio¬ª vs. ¬´medicina¬ª\n",
        "\n",
        "**Paso‚ÄØ1.** Cargar datos ‚Üí `fetch_20newsgroups`.\n",
        "\n",
        "**Paso‚ÄØ2.** Dividir en entrenamiento / prueba.\n",
        "\n",
        "**Paso‚ÄØ3.** Crear un **pipeline**:\n",
        "\n",
        "1. Vectorizador (`CountVectorizer` o `TfidfVectorizer`).\n",
        "2. Clasificador (Naive¬†Bayes, SVM, etc.).\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "cats = [\"sci.med\", \"sci.space\"]\n",
        "data = fetch_20newsgroups(subset=\"all\", categories=cats, remove=(\"headers\",\"footers\",\"quotes\"))\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "pipe = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
        "pipe.fit(X_train, y_train)\n",
        "print(classification_report(y_test, pipe.predict(X_test), target_names=cats))\n",
        "```\n",
        "\n",
        "| M√©trica       | Significado                              |\n",
        "| ------------- | ---------------------------------------- |\n",
        "| **Accuracy**  | Proporci√≥n total de aciertos.            |\n",
        "| **Precision** | Exactitud de las predicciones positivas. |\n",
        "| **Recall**    | Cobertura de los positivos reales.       |\n",
        "| **F1**        | Media arm√≥nica de Precision y Recall.    |\n",
        "\n",
        "> **Ejercicio 4.1 ‚Äì¬†Comparar modelos**\n",
        ">\n",
        "> 1. Sustituye `MultinomialNB` por `LinearSVC()`.\n",
        "> 2. ¬øQui√©n gana en F1? ¬øPor qu√© podr√≠a ocurrir?\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## 5. Modelado de temas\n",
        "\n",
        "### 5.1 LSA (Latent Semantic Analysis) con SVD truncado\n",
        "\n",
        "**Paso‚ÄØa‚ÄØpaso**\n",
        "\n",
        "1. Vectorizar con **TF‚ÄëIDF**.\n",
        "2. Aplicar `TruncatedSVD(n_components=k)` para descomponer la matriz.\n",
        "3. Cada componente ‚âà un **tema** (vector de t√©rminos con pesos altos).\n",
        "\n",
        "```python\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words=\"english\")\n",
        "X = vectorizer.fit_transform(data.data)\n",
        "svd = TruncatedSVD(n_components=5, random_state=42)\n",
        "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
        "X_lsa = lsa.fit_transform(X)\n",
        "\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "for idx, comp in enumerate(svd.components_):\n",
        "    terms_in_comp = [terms[i] for i in comp.argsort()[-8:]]\n",
        "    print(f\"Tema {idx}:\", terms_in_comp)\n",
        "```\n",
        "\n",
        "### 5.2 LDA (Latent Dirichlet Allocation) con gensim\n",
        "\n",
        "1. **Tokenizar & limpiar**.\n",
        "2. **Diccionario** `id2word`.\n",
        "3. **Corpus** (lista de pares `(id, freq)`).\n",
        "4. Entrenar `LdaModel` especificando `num_topics`.\n",
        "\n",
        "```python\n",
        "import gensim.corpora as corpora\n",
        "from gensim.models import LdaModel\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "docs = [word_tokenize(clean_text(doc)) for doc in data.data]\n",
        "id2word = corpora.Dictionary(docs)\n",
        "corpus_g = [id2word.doc2bow(d) for d in docs]\n",
        "\n",
        "lda = LdaModel(corpus=corpus_g, id2word=id2word, num_topics=5, passes=10, random_state=42)\n",
        "lda.print_topics(num_words=6)\n",
        "```\n",
        "\n",
        "| S√≠mbolo | Significado en el gr√°fico de placas                                              |\n",
        "| ------- | -------------------------------------------------------------------------------- |\n",
        "| **Œ±**   | Par√°metro Dirichlet para las distribuciones de temas por documento (\\(\\theta\\)). |\n",
        "| **Œ≤**   | Par√°metro Dirichlet para las distribuciones de palabras por tema (\\(\\phi\\)).     |\n",
        "| **z**   | Tema asignado a una palabra concreta.                                            |\n",
        "| **w**   | Palabra observada.                                                               |\n",
        "\n",
        "> **Ejercicio 5.1 ‚Äì¬†Coherencia de temas** Usa `gensim.models.CoherenceModel` para medir la coherencia y elige el `num_topics` √≥ptimo.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## 6. Proyecto integrador & ejercicios finales\n",
        "\n",
        "1. **Dataset** ‚Äì Elige uno de Kaggle (*IMDB Reviews*, *News Category*‚Ä¶).\n",
        "2. **Objetivo** ‚Äì Sentiment Analysis **o** Topic Modeling.\n",
        "3. **Gu√≠a m√≠nima**:\n",
        "   1. Limpieza y pre‚Äëprocesamiento detallado.\n",
        "   2. Dos representaciones (BoW vs. TF‚ÄëIDF) y compara resultados.\n",
        "   3. Clasificador (NB / SVM) **o** LDA con curva de coherencia.\n",
        "   4. Informe (`markdown` o `notebook`) con m√©tricas, gr√°ficos y reflexi√≥n.\n",
        "4. **Bonus** ‚Äì Implementa un peque√±o *streamlit* para probar tu modelo.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## 7. Recursos adicionales\n",
        "\n",
        "- **NLTK Book** ‚Äì [https://www.nltk.org/book/](https://www.nltk.org/book/) (Cap.‚ÄØ1‚Äë6 ‚Üí tokenizaci√≥n, clasificaci√≥n)\n",
        "- **Machine Learning with Text (J.‚ÄØZumstein)** ‚Äì cap√≠tulos gratuitos online.\n",
        "- **Gensim Tutorials** ‚Äì [https://radimrehurek.com/gensim/auto\\_examples/index.html](https://radimrehurek.com/gensim/auto_examples/index.html)\n",
        "- **Coursera NLP Specialization (Deeplearning.ai)** ‚Äì curso 1 (vector spaces & BoW).\n",
        "\n",
        "---\n",
        "\n",
        "### ¬°Ahora s√≠, manos al teclado!\n",
        "\n",
        "Esta versi√≥n incluye explicaciones paso‚ÄØa‚ÄØpaso para cada t√©cnica. Ajusta par√°metros, prueba datasets nuevos y profundiza.\n",
        "\n"
      ],
      "metadata": {
        "id": "vyYue2UbOcAv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLE448TZMPCU"
      },
      "outputs": [],
      "source": [
        "# Revisar http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Revisar https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html"
      ],
      "metadata": {
        "id": "il65bi6ws5-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Revisar https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html"
      ],
      "metadata": {
        "id": "LubYC2OTDcER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicio\n",
        "\n",
        "Entrena un modelo de reconocimiento de correo spam\n",
        "\n",
        "https://www.kaggle.com/datasets/venky73/spam-mails-dataset/data\n",
        "\n"
      ],
      "metadata": {
        "id": "SKOTRHW2Fdhi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quedamos slide 152"
      ],
      "metadata": {
        "id": "P7WqzRfcEu4_"
      }
    }
  ]
}