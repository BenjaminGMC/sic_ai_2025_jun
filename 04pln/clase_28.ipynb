{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQxHG8n+Bqb/ALrgLKz8Tq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidlealo/sic_ai_2025_jun/blob/main/04pln/clase_28.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clase 28\n",
        "\n",
        "### Unidadâ€¯3 â€“ CapÃ­tuloâ€¯7 Â· Samsungâ€¯Innovationâ€¯Campusâ€¯2025\n",
        "\n",
        "---\n",
        "\n",
        "## Objetivo\n",
        "\n",
        "Esta guÃ­a pasoâ€¯aâ€¯paso introduce los conceptos y tÃ©cnicas esenciales que viste en la Unidadâ€¯3 (Â«Modelo de representaciÃ³nÂ», Â«AnÃ¡lisis de clasificaciÃ³nÂ» y Â«Modelado de temasÂ»). Incluye explicaciones detalladas, ejemplos en **Python** y ejercicios prÃ¡cticos. EstÃ¡ pensada para personas que ya programan pero son nuevas en PLN.\n",
        "\n",
        "---\n",
        "\n",
        "## Prerrequisitos rÃ¡pidos\n",
        "\n",
        "| Herramienta      | Comando de instalaciÃ³n     |\n",
        "| ---------------- | -------------------------- |\n",
        "| **NLTK**         | `pip install nltk`         |\n",
        "| **scikitâ€‘learn** | `pip install scikit-learn` |\n",
        "| **gensim**       | `pip install gensim`       |\n",
        "\n",
        "> **Pasoâ€¯0.** Crea un entorno virtual (opcional pero recomendado) y activa tu entorno antes de instalar los paquetes.\n",
        "\n",
        "---\n",
        "\n",
        "## Tabla de contenidos\n",
        "\n",
        "1. [Flujo de un proyecto de PLN](#flujo)\n",
        "2. [Preâ€‘procesamiento del texto](#prepro)\n",
        "3. [Modelos de representaciÃ³n](#repr)\n",
        "4. [ClasificaciÃ³n supervisada](#clasif)\n",
        "5. [Modelado de temas](#temas)\n",
        "6. [Proyecto integrador & ejercicios](#proyecto)\n",
        "7. [Recursos adicionales](#recursos)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## 1. Flujo de un proyecto de PLN\n",
        "\n",
        "1. **Definir el problema** â†’ Â¿Clasificar sentimiento? Â¿Detectar spam? Â¿Extraer temas?\n",
        "2. **RecolecciÃ³n de datos** â†’ APIs, scraping, Kaggle, archivos internos.\n",
        "3. **Preâ€‘procesamiento** â†’ Limpieza, tokenizaciÃ³n, lematizaciÃ³n, stopwords.\n",
        "4. **RepresentaciÃ³n** â†’ BoW, TFâ€‘IDF o embeddings.\n",
        "5. **Modelado** â†’ Clasificadores supervisados o modelos no supervisados (LDA, LSA).\n",
        "6. **EvaluaciÃ³n** â†’ MÃ©tricas (accuracy, precisiÃ³n, F1, coherencia de temasâ€¦).\n",
        "7. **Despliegue** â†’ API REST, batch, dashboard, etc.\n",
        "\n",
        "Cada paso se explica con mayor detalle a continuaciÃ³n.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## 2. Preâ€‘procesamiento del texto\n",
        "\n",
        "### Â¿Por quÃ© limpiar el texto?\n",
        "\n",
        "Los algoritmos trabajan con nÃºmeros. El preâ€‘procesamiento normaliza la entrada y reduce ruido, mejorando la calidad de la representaciÃ³n.\n",
        "\n",
        "### Paso a paso ðŸ› ï¸\n",
        "\n",
        "1. **Convertir a minÃºsculas** â€“ Homogeneiza el texto.\n",
        "2. **Eliminar URLs** â€“ No aportan significado en la mayorÃ­a de tareas.\n",
        "3. **Quitar puntuaciÃ³n** â€“ Facilita la tokenizaciÃ³n.\n",
        "4. **Tokenizar** â€“ Dividir el texto en palabras o subâ€‘palabras.\n",
        "5. **Eliminar stopwords** â€“ Palabras muy frecuentes sin carga semÃ¡ntica (Â«theÂ», Â«yÂ», Â«deÂ»â€¦).\n",
        "6. **Lematizar** â€“ Reducir palabras a su forma canÃ³nica ( *running* â†’ *run*).\n",
        "\n",
        "```python\n",
        "import nltk, re, string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download(\"punkt\"); nltk.download(\"wordnet\"); nltk.download(\"stopwords\")\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "def clean_text(text):\n",
        "    # 1â€‘2. normalizar y quitar URLs\n",
        "    text = re.sub(r\"https?://\\S+\", \"\", text.lower())\n",
        "\n",
        "    # 3. quitar puntuaciÃ³n\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "    # 4. tokenizar\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # 5â€‘6. stopwords + lematizar\n",
        "    tokens = [lemmatizer.lemmatize(t) for t in tokens if t not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "```\n",
        "\n",
        "> **Ejercicio 2.1 â€“Â Explora el impacto**\n",
        ">\n",
        "> 1. Copia 5â€“10 tweets (o reseÃ±as IMDB).\n",
        "> 2. Aplica `clean_text` y compara texto original vs. limpiado.\n",
        "> 3. Â¿QuÃ© tokens se eliminaron? Â¿CuÃ¡les se transformaron?\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## 3. Modelos de representaciÃ³n\n",
        "\n",
        "### 3.0 ExploraciÃ³n de frecuencia de palabras\n",
        "\n",
        "El **conteo de frecuencias** es la forma mÃ¡s bÃ¡sica de cuantificar texto y suele ser el *primer paso* del anÃ¡lisis exploratorio de datos (EDA) en PLN.\n",
        "\n",
        "#### Â¿Por quÃ© contar palabras?\n",
        "\n",
        "- **Panorama rÃ¡pido del corpus**Â â†’ Identifica temas dominantes y vocabulario caracterÃ­stico.\n",
        "- **Ley deÂ Zipf**Â â†’Â La distribuciÃ³n sigue una cola larga: pocas palabras muy frecuentes y muchas raras. Conocerla ayuda a decidir filtros (`min_df`, `max_df`).\n",
        "- **DetecciÃ³n de *****stopwords***** y ruido**Â â†’ Palabras ultraâ€‘frecuentes pueden eliminarse para mejorar modelos.\n",
        "- **SelecciÃ³n de caracterÃ­sticas**Â â†’ Elegir el *topâ€‘k* de tÃ©rminos o fijar umbrales de frecuencia reduce dimensionalidad.\n",
        "- **ValidaciÃ³n de limpieza**Â â†’ NÃºmeros, URLs o artefactos extraÃ±os destacan en los rankings.\n",
        "\n",
        "#### Paso a paso ðŸ› ï¸\n",
        "\n",
        "```python\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# 1. Corpus ya limpiado con clean_text()\n",
        "texts = [clean_text(doc) for doc in data.data]\n",
        "\n",
        "# 2. Tokenizar y aplanar\n",
        "all_tokens = []\n",
        "for t in texts:\n",
        "    all_tokens.extend(word_tokenize(t))\n",
        "\n",
        "# 3. Contar\n",
        "freq = Counter(all_tokens)\n",
        "print(freq.most_common(10))  # Topâ€‘10 palabras\n",
        "\n",
        "# 4. Visualizar\n",
        "terms, counts = zip(*freq.most_common(20))\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.bar(terms, counts)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title('Topâ€‘20 palabras mÃ¡s frecuentes')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "> **EjercicioÂ 3.0 â€“Â Word cloud**\n",
        ">\n",
        "> 1. Instala `pip install wordcloud`.\n",
        "> 2. Genera una nube de palabras con `WordCloud().generate_from_frequencies(freq)`.\n",
        "> 3. Cambia el resultado filtrando stopwords distintas y observa el efecto.\n",
        "\n",
        "---\n",
        "\n",
        "### 3.1 Bolsa de Palabras (BoW)\n",
        "\n",
        "**Idea:** Cada documento se convierte en un vector donde la dimensiÃ³n *j* es la frecuencia de la palabra *j*.\n",
        "\n",
        "```python\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = [\n",
        "    \"Cats eat fish\",\n",
        "    \"Dogs eat meat\",\n",
        "    \"Cats and dogs are pets\"\n",
        "]\n",
        "vectorizer = CountVectorizer()\n",
        "X_bow = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names_out())      # columnas\n",
        "print(X_bow.toarray())                         # matriz docÃ—vocab\n",
        "```\n",
        "\n",
        "| Paso | AcciÃ³n                                                  |\n",
        "| ---- | ------------------------------------------------------- |\n",
        "| 1    | Construir vocabulario â†’ `fit`                           |\n",
        "| 2    | Contar apariciones    â†’ `transform`                     |\n",
        "| 3    | Obtener matriz esparsa de tamaÃ±o *(n\\_docs Ã— n\\_terms)* |\n",
        "\n",
        "### 3.2 TFâ€‘IDF\n",
        "\n",
        "**FÃ³rmula:**\\\n",
        "\\(\\text{tfâ€‘idf}(d, t) = tf(d,t) \\times \\log \\frac{N}{df(t)}\\)\n",
        "\n",
        "- Penaliza tÃ©rminos muy frecuentes globalmente.\n",
        "\n",
        "```python\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vect = TfidfVectorizer()\n",
        "X_tfidf = vect.fit_transform(corpus)\n",
        "print(X_tfidf.toarray())\n",
        "```\n",
        "\n",
        "> **Ejercicio 3.1 â€“Â Nâ€‘gramas y filtros**\n",
        ">\n",
        "> - Ajusta `ngram_range=(1,2)` (unigramas + bigramas).\n",
        "> - Ignora tÃ©rminos raros con `min_df=2`.\n",
        "> - Observa cÃ³mo cambia la matriz.\n",
        "\n",
        "### 3.3 Word Embeddings (vistazo rÃ¡pido)\n",
        "\n",
        "Representaciones densas que capturan similitud semÃ¡ntica.\n",
        "\n",
        "```python\n",
        "from gensim.models import Word2Vec\n",
        "sentences = [doc.split() for doc in corpus]\n",
        "model = Word2Vec(sentences, vector_size=50, window=5, min_count=1, workers=4)\n",
        "print(model.wv[\"cats\"][:10])  # 10 primeras dimensiones\n",
        "```\n",
        "\n",
        "> Para ir mÃ¡s lejos: `sentence-transformers`, `spaCy`.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## 4. ClasificaciÃ³n supervisada\n",
        "\n",
        "### Caso de uso: Clasificar artÃ­culos de noticias Â«espacioÂ» vs. Â«medicinaÂ»\n",
        "\n",
        "**Pasoâ€¯1.** Cargar datos â†’ `fetch_20newsgroups`.\n",
        "\n",
        "**Pasoâ€¯2.** Dividir en entrenamiento / prueba.\n",
        "\n",
        "**Pasoâ€¯3.** Crear un **pipeline**:\n",
        "\n",
        "1. Vectorizador (`CountVectorizer` o `TfidfVectorizer`).\n",
        "2. Clasificador (NaiveÂ Bayes, SVM, etc.).\n",
        "\n",
        "```python\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "cats = [\"sci.med\", \"sci.space\"]\n",
        "data = fetch_20newsgroups(subset=\"all\", categories=cats, remove=(\"headers\",\"footers\",\"quotes\"))\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "pipe = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
        "pipe.fit(X_train, y_train)\n",
        "print(classification_report(y_test, pipe.predict(X_test), target_names=cats))\n",
        "```\n",
        "\n",
        "| MÃ©trica       | Significado                              |\n",
        "| ------------- | ---------------------------------------- |\n",
        "| **Accuracy**  | ProporciÃ³n total de aciertos.            |\n",
        "| **Precision** | Exactitud de las predicciones positivas. |\n",
        "| **Recall**    | Cobertura de los positivos reales.       |\n",
        "| **F1**        | Media armÃ³nica de Precision y Recall.    |\n",
        "\n",
        "> **Ejercicio 4.1 â€“Â Comparar modelos**\n",
        ">\n",
        "> 1. Sustituye `MultinomialNB` por `LinearSVC()`.\n",
        "> 2. Â¿QuiÃ©n gana en F1? Â¿Por quÃ© podrÃ­a ocurrir?\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## 5. Modelado de temas\n",
        "\n",
        "### 5.1 LSA (Latent Semantic Analysis) con SVD truncado\n",
        "\n",
        "**Pasoâ€¯aâ€¯paso**\n",
        "\n",
        "1. Vectorizar con **TFâ€‘IDF**.\n",
        "2. Aplicar `TruncatedSVD(n_components=k)` para descomponer la matriz.\n",
        "3. Cada componente â‰ˆ un **tema** (vector de tÃ©rminos con pesos altos).\n",
        "\n",
        "```python\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_df=0.5, min_df=2, stop_words=\"english\")\n",
        "X = vectorizer.fit_transform(data.data)\n",
        "svd = TruncatedSVD(n_components=5, random_state=42)\n",
        "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
        "X_lsa = lsa.fit_transform(X)\n",
        "\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "for idx, comp in enumerate(svd.components_):\n",
        "    terms_in_comp = [terms[i] for i in comp.argsort()[-8:]]\n",
        "    print(f\"Tema {idx}:\", terms_in_comp)\n",
        "```\n",
        "\n",
        "### 5.2 LDA (Latent Dirichlet Allocation) con gensim\n",
        "\n",
        "1. **Tokenizar & limpiar**.\n",
        "2. **Diccionario** `id2word`.\n",
        "3. **Corpus** (lista de pares `(id, freq)`).\n",
        "4. Entrenar `LdaModel` especificando `num_topics`.\n",
        "\n",
        "```python\n",
        "import gensim.corpora as corpora\n",
        "from gensim.models import LdaModel\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "docs = [word_tokenize(clean_text(doc)) for doc in data.data]\n",
        "id2word = corpora.Dictionary(docs)\n",
        "corpus_g = [id2word.doc2bow(d) for d in docs]\n",
        "\n",
        "lda = LdaModel(corpus=corpus_g, id2word=id2word, num_topics=5, passes=10, random_state=42)\n",
        "lda.print_topics(num_words=6)\n",
        "```\n",
        "\n",
        "| SÃ­mbolo | Significado en el grÃ¡fico de placas                                              |\n",
        "| ------- | -------------------------------------------------------------------------------- |\n",
        "| **Î±**   | ParÃ¡metro Dirichlet para las distribuciones de temas por documento (\\(\\theta\\)). |\n",
        "| **Î²**   | ParÃ¡metro Dirichlet para las distribuciones de palabras por tema (\\(\\phi\\)).     |\n",
        "| **z**   | Tema asignado a una palabra concreta.                                            |\n",
        "| **w**   | Palabra observada.                                                               |\n",
        "\n",
        "> **Ejercicio 5.1 â€“Â Coherencia de temas** Usa `gensim.models.CoherenceModel` para medir la coherencia y elige el `num_topics` Ã³ptimo.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## 6. Proyecto integrador & ejercicios finales\n",
        "\n",
        "1. **Dataset** â€“ Elige uno de Kaggle (*IMDB Reviews*, *News Category*â€¦).\n",
        "2. **Objetivo** â€“ Sentiment Analysis **o** Topic Modeling.\n",
        "3. **GuÃ­a mÃ­nima**:\n",
        "   1. Limpieza y preâ€‘procesamiento detallado.\n",
        "   2. Dos representaciones (BoW vs. TFâ€‘IDF) y compara resultados.\n",
        "   3. Clasificador (NB / SVM) **o** LDA con curva de coherencia.\n",
        "   4. Informe (`markdown` o `notebook`) con mÃ©tricas, grÃ¡ficos y reflexiÃ³n.\n",
        "4. **Bonus** â€“ Implementa un pequeÃ±o *streamlit* para probar tu modelo.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## 7. Recursos adicionales\n",
        "\n",
        "- **NLTK Book** â€“ [https://www.nltk.org/book/](https://www.nltk.org/book/) (Cap.â€¯1â€‘6 â†’ tokenizaciÃ³n, clasificaciÃ³n)\n",
        "- **Machine Learning with Text (J.â€¯Zumstein)** â€“ capÃ­tulos gratuitos online.\n",
        "- **Gensim Tutorials** â€“ [https://radimrehurek.com/gensim/auto\\_examples/index.html](https://radimrehurek.com/gensim/auto_examples/index.html)\n",
        "- **Coursera NLP Specialization (Deeplearning.ai)** â€“ curso 1 (vector spaces & BoW).\n",
        "\n",
        "---\n",
        "\n",
        "### Â¡Ahora sÃ­, manos al teclado!\n",
        "\n",
        "Esta versiÃ³n incluye explicaciones pasoâ€¯aâ€¯paso para cada tÃ©cnica. Ajusta parÃ¡metros, prueba datasets nuevos y profundiza.\n",
        "\n"
      ],
      "metadata": {
        "id": "vyYue2UbOcAv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLE448TZMPCU"
      },
      "outputs": [],
      "source": [
        "# Revisar http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Revisar https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html"
      ],
      "metadata": {
        "id": "il65bi6ws5-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Revisar https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html"
      ],
      "metadata": {
        "id": "LubYC2OTDcER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejercicio\n",
        "\n",
        "Entrena un modelo de reconocimiento de correo spam\n",
        "\n",
        "https://www.kaggle.com/datasets/venky73/spam-mails-dataset/data\n",
        "\n"
      ],
      "metadata": {
        "id": "SKOTRHW2Fdhi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quedamos slide 152"
      ],
      "metadata": {
        "id": "P7WqzRfcEu4_"
      }
    }
  ]
}