{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPaRi+UHCBFxp5aqtP2H3j9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidlealo/sic_ai_2025_jun/blob/main/04pln/clase_25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clase 25"
      ],
      "metadata": {
        "id": "Ka7Q3nZILHmp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# WordNetLemmatizer - Documentaci√≥n detallada\n",
        "\n",
        "üìå **Fuente oficial:**  \n",
        "https://www.nltk.org/api/nltk.stem.WordNetLemmatizer.html\n",
        "\n",
        "## ¬øQu√© es lematizaci√≥n?\n",
        "\n",
        "La **lematizaci√≥n** es el proceso de reducir una palabra a su forma base o \"lema\" utilizando un diccionario ling√º√≠stico. A diferencia del \"stemming\", que simplemente corta sufijos sin tener en cuenta el contexto gramatical, la lematizaci√≥n produce formas reales de palabras.\n",
        "\n",
        "Ejemplo:\n",
        "- Stemming de *\"better\"* ‚Üí *\"bett\"*\n",
        "- Lematizaci√≥n de *\"better\"* ‚Üí *\"good\"* (basado en el contexto gramatical)\n",
        "\n",
        "---\n",
        "\n",
        "## Introducci√≥n a `WordNetLemmatizer`\n",
        "\n",
        "`WordNetLemmatizer` es una clase en `nltk.stem` que utiliza el corpus **WordNet**, una base de datos l√©xica del ingl√©s, para hacer lematizaci√≥n basada en reglas ling√º√≠sticas.\n",
        "\n",
        "```python\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## M√©todo principal: `lemmatize(word, pos='n')`\n",
        "\n",
        "Este m√©todo devuelve el **lema (forma base)** de una palabra dada, considerando opcionalmente su categor√≠a gramatical.\n",
        "\n",
        "### üîß Par√°metros\n",
        "\n",
        "- `word` (`str`): la palabra a lematizar.\n",
        "- `pos` (`str`, opcional): parte del discurso (part-of-speech). Los valores v√°lidos son:\n",
        "  - `'n'` ‚Üí sustantivo *(noun)* (por defecto)\n",
        "  - `'v'` ‚Üí verbo *(verb)*\n",
        "  - `'a'` ‚Üí adjetivo *(adjective)*\n",
        "  - `'r'` ‚Üí adverbio *(adverb)*\n",
        "  - `'s'` ‚Üí adjetivo sat√©lite *(adjective satellite)*\n",
        "\n",
        "### üîÅ Retorno\n",
        "\n",
        "Devuelve el **lema** de la palabra, es decir, su forma base seg√∫n WordNet.\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Ejemplos de uso\n",
        "\n",
        "```python\n",
        "lemmatizer.lemmatize(\"cats\")          # 'cat'\n",
        "lemmatizer.lemmatize(\"cacti\")         # 'cactus'\n",
        "lemmatizer.lemmatize(\"geese\")         # 'goose'\n",
        "lemmatizer.lemmatize(\"rocks\")         # 'rock'\n",
        "lemmatizer.lemmatize(\"python\")        # 'python'\n",
        "lemmatizer.lemmatize(\"better\", pos=\"a\")  # 'good'\n",
        "lemmatizer.lemmatize(\"running\", pos=\"v\") # 'run'\n",
        "```\n",
        "\n",
        "‚ö†Ô∏è Por defecto, el m√©todo trata las palabras como **sustantivos**, por lo que es importante pasar la etiqueta `pos` adecuada para obtener resultados precisos.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† ¬øC√≥mo mejorar la precisi√≥n?\n",
        "\n",
        "Para lematizar correctamente, especialmente verbos o adjetivos, es recomendable hacer un **etiquetado gramatical (POS tagging)** antes:\n",
        "\n",
        "```python\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Funci√≥n para convertir etiquetas POS de nltk a WordNet\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN  # default\n",
        "\n",
        "# Texto de entrada\n",
        "text = \"The striped bats are hanging on their feet for best\"\n",
        "tokens = word_tokenize(text)\n",
        "tagged = pos_tag(tokens)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tagged]\n",
        "print(lemmatized)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üì¶ Requisitos previos\n",
        "\n",
        "Para usar WordNet, necesitas descargar los siguientes recursos:\n",
        "\n",
        "```python\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # Para traducciones y sin√≥nimos\n",
        "nltk.download('punkt')    # Para tokenizar textos\n",
        "nltk.download('averaged_perceptron_tagger')  # Para POS tagging\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üß≠ Diferencias entre lematizaci√≥n y stemming\n",
        "\n",
        "| Caracter√≠stica       | Lemmatizaci√≥n                   | Stemming                        |\n",
        "|----------------------|----------------------------------|----------------------------------|\n",
        "| Usa diccionario real | ‚úÖ S√≠                           | ‚ùå No                            |\n",
        "| Precisi√≥n contextual | ‚úÖ Alta (con POS)               | ‚ùå Baja                          |\n",
        "| Velocidad            | ‚ö†Ô∏è M√°s lenta                   | ‚úÖ M√°s r√°pida                    |\n",
        "| Ejemplo              | \"better\" ‚Üí \"good\"               | \"better\" ‚Üí \"bett\"               |\n",
        "\n",
        "---\n",
        "\n",
        "## üîö Conclusi√≥n\n",
        "\n",
        "`WordNetLemmatizer` es ideal para tareas de procesamiento de lenguaje natural (NLP) donde se requiere una forma limpia y gramaticalmente v√°lida de las palabras, como:\n",
        "\n",
        "- An√°lisis de sentimientos\n",
        "- Clasificaci√≥n de texto\n",
        "- Extracci√≥n de entidades\n",
        "- Traducci√≥n autom√°tica\n",
        "\n",
        "---\n",
        "\n",
        "## üîó Referencias √∫tiles\n",
        "\n",
        "- [Documentaci√≥n oficial de NLTK](https://www.nltk.org)\n",
        "- [WordNet en NLTK](https://www.nltk.org/howto/wordnet.html)\n",
        "- [WordNet API](https://wordnet.princeton.edu/)\n"
      ],
      "metadata": {
        "id": "YKK-VbnKMhSr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbq1ZN-6KunW",
        "outputId": "27bddd6a-43a6-4bea-cf4c-29768164452c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dog\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "print(wnl.lemmatize('dogs'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wnl.lemmatize('churches'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3fMY5fgMwcd",
        "outputId": "6a489c64-be1b-4c6e-b6de-fa883fe4e187"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "church\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wnl.lemmatize('aardwolves'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HF95U0JpM0Bt",
        "outputId": "87823ca9-6d73-4b16-c105-aab4b9a8bbeb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aardwolf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wnl.lemmatize('abaci'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mz4nOeMXNA4_",
        "outputId": "b7c0c6d0-78b9-4dfd-b0d4-d10abc72042d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abacus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wnl.lemmatize('hardrock'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppIeUSqyNDyA",
        "outputId": "001e0058-e5f3-4c98-8bdf-b3d9dd86be95"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hardrock\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print([wnl.lemmatize(w) for w in words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dlDBZLcMVXT",
        "outputId": "b3360f63-d9a3-4d72-85e6-c76929b79a2e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wnl.lemmatize('dies', 'v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPHUV1N1NjoW",
        "outputId": "ea62b8e5-5220-4e39-e15b-21884781582e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "die\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wnl.lemmatize('watched', 'v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EP1kEOo6NsB7",
        "outputId": "1a1841e0-beb0-4161-ca49-4f74c89ef9c4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "watch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wnl.lemmatize('has', 'v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8aVEw66NyQm",
        "outputId": "72ff13f8-af86-4591-f655-4b914301484a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "have\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "animals = [\n",
        "    \"aardwolves\",\n",
        "    \"mice\",\n",
        "    \"geese\",\n",
        "    \"deer\",\n",
        "    \"sheep\",\n",
        "    \"moose\",\n",
        "    \"oxen\",\n",
        "    \"manatees\",\n",
        "    \"fish\",\n",
        "    \"fishes\",\n",
        "    \"wolves\",\n",
        "    \"leafcutter ants\",\n",
        "    \"calves\",\n",
        "    \"hoofed animals\",\n",
        "    \"octopuses\",\n",
        "    \"octopi\",\n",
        "    \"cacti\",\n",
        "    \"fungi\",\n",
        "    \"platypuses\",\n",
        "    \"platypi\",\n",
        "    \"larvae\",\n",
        "    \"bacteria\"\n",
        "]\n",
        "\n",
        "print([wnl.lemmatize(a) for a in animals])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ym2vGQ-OKZ5",
        "outputId": "55135333-7e88-454a-edd1-fe7be0a55aa4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['aardwolf', 'mouse', 'goose', 'deer', 'sheep', 'moose', 'ox', 'manatee', 'fish', 'fish', 'wolf', 'leafcutter ants', 'calf', 'hoofed animals', 'octopus', 'octopus', 'cactus', 'fungi', 'platypus', 'platypi', 'larva', 'bacteria']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wnl.lemmatize('leafcutter ants', 'n'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiTxyg8IOn1f",
        "outputId": "92754017-08d8-4df6-89a4-0668347c9de9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leafcutter ants\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# PorterStemmer en NLTK - Gu√≠a detallada\n",
        "\n",
        "üìå Basado en la documentaci√≥n de NLTK: https://www.nltk.org/howto/stem.html\n",
        "\n",
        "## üå± ¬øQu√© es PorterStemmer?\n",
        "\n",
        "`PorterStemmer` es una implementaci√≥n del algoritmo de stemming creado por **Martin Porter** en 1980.  \n",
        "Su objetivo es reducir una palabra a su **ra√≠z morfol√≥gica** (stem) de forma eficiente, aunque sin preocuparse por la correcci√≥n gramatical o si el resultado es una palabra real del idioma.\n",
        "\n",
        "Es uno de los algoritmos de stemming m√°s utilizados en tareas de procesamiento de lenguaje natural (NLP).\n",
        "\n",
        "---\n",
        "\n",
        "## üîß C√≥mo importar y usar\n",
        "\n",
        "```python\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "print(ps.stem(\"running\"))   # 'run'\n",
        "print(ps.stem(\"flies\"))     # 'fli'\n",
        "print(ps.stem(\"studies\"))   # 'studi'\n",
        "print(ps.stem(\"believable\"))# 'believ'\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Ejemplo con lista de palabras\n",
        "\n",
        "```python\n",
        "words = [\"run\", \"runner\", \"running\", \"ran\", \"runs\", \"easily\", \"fairly\"]\n",
        "\n",
        "for word in words:\n",
        "    print(f\"{word} ‚Üí {ps.stem(word)}\")\n",
        "```\n",
        "\n",
        "### Salida esperada:\n",
        "```\n",
        "run ‚Üí run\n",
        "runner ‚Üí runner\n",
        "running ‚Üí run\n",
        "ran ‚Üí ran\n",
        "runs ‚Üí run\n",
        "easily ‚Üí easili\n",
        "fairly ‚Üí fairli\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Caracter√≠sticas del algoritmo Porter\n",
        "\n",
        "- Basado en reglas morfol√≥gicas\n",
        "- No usa diccionarios\n",
        "- R√°pido y eficiente\n",
        "- Puede generar palabras inexistentes (ej. \"easily\" ‚Üí \"easili\")\n",
        "\n",
        "---\n",
        "\n",
        "## üÜö Comparaci√≥n con otros stemmers\n",
        "\n",
        "| Palabra     | Porter    | Lancaster |\n",
        "|-------------|-----------|-----------|\n",
        "| studies     | studi     | study     |\n",
        "| maximum     | maximum   | maxim     |\n",
        "| swimming    | swim      | swim      |\n",
        "| possibly    | possibl   | poss      |\n",
        "\n",
        "---\n",
        "\n",
        "## üì¶ Requisitos para usar\n",
        "\n",
        "```python\n",
        "import nltk\n",
        "nltk.download('punkt')  # Solo si vas a tokenizar\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üß≠ Cu√°ndo usar PorterStemmer\n",
        "\n",
        "- Cuando necesitas rapidez y simplicidad\n",
        "- En an√°lisis de texto donde la precisi√≥n gramatical no es cr√≠tica\n",
        "- En sistemas de recuperaci√≥n de informaci√≥n (IR), b√∫squeda o clustering\n",
        "\n",
        "---\n",
        "\n",
        "## üîö Conclusi√≥n\n",
        "\n",
        "`PorterStemmer` es una herramienta confiable y veloz para normalizar palabras en tareas de NLP. Aunque no devuelve formas reales del idioma, es muy √∫til cuando se requiere agrupar variaciones de una ra√≠z com√∫n."
      ],
      "metadata": {
        "id": "oj5NGyWKPzGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "ps = PorterStemmer()\n",
        "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n",
        "for w in example_words:\n",
        "    print(ps.stem(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiFgo3NLPTi7",
        "outputId": "93e588eb-58df-4102-be94-d96294e7b9aa"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python\n",
            "python\n",
            "python\n",
            "python\n",
            "pythonli\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "animals = [\n",
        "    \"aardwolves\",\n",
        "    \"mice\",\n",
        "    \"geese\",\n",
        "    \"deer\",\n",
        "    \"sheep\",\n",
        "    \"moose\",\n",
        "    \"oxen\",\n",
        "    \"manatees\",\n",
        "    \"fish\",\n",
        "    \"fishes\",\n",
        "    \"wolves\",\n",
        "    \"leafcutter ants\",\n",
        "    \"calves\",\n",
        "    \"hoofed animals\",\n",
        "    \"octopuses\",\n",
        "    \"octopi\",\n",
        "    \"cacti\",\n",
        "    \"fungi\",\n",
        "    \"platypuses\",\n",
        "    \"platypi\",\n",
        "    \"larvae\",\n",
        "    \"bacteria\"\n",
        "]\n",
        "\n",
        "print([ps.stem(a) for a in animals])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwgTnrpvPzwm",
        "outputId": "efa5c8d7-db56-44f5-e076-048030a16094"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['aardwolv', 'mice', 'gees', 'deer', 'sheep', 'moos', 'oxen', 'manate', 'fish', 'fish', 'wolv', 'leafcutter ', 'calv', 'hoofed anim', 'octopus', 'octopi', 'cacti', 'fungi', 'platypus', 'platypi', 'larva', 'bacteria']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# nltk.stem.PorterStemmer\n",
        "\n",
        "## Clase PorterStemmer\n",
        "\n",
        "```python\n",
        "class nltk.stem.PorterStemmer()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Descripci√≥n\n",
        "\n",
        "Esta clase implementa el algoritmo **Porter stemming**, uno de los algoritmos de stemming m√°s populares en procesamiento de lenguaje natural.\n",
        "\n",
        "El algoritmo fue creado por Martin Porter en 1980 y tiene como objetivo recortar los sufijos de las palabras para reducirlas a su ra√≠z o \"stem\".\n",
        "\n",
        "El Porter Stemmer no garantiza que el resultado sea una palabra real, pero es eficiente para agrupar palabras similares.\n",
        "\n",
        "---\n",
        "\n",
        "## M√©todos principales\n",
        "\n",
        "### `stem(word)`\n",
        "\n",
        "Reduce la palabra dada a su ra√≠z o stem.\n",
        "\n",
        "- **Par√°metros**:\n",
        "  - `word` (`str`): La palabra a la que se le aplicar√° el stemming.\n",
        "\n",
        "- **Retorna**:\n",
        "  - `str`: La ra√≠z (stem) de la palabra.\n",
        "\n",
        "- **Ejemplo**:\n",
        "\n",
        "```python\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "print(ps.stem('running'))  # Salida: run\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Uso b√°sico\n",
        "\n",
        "```python\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "words = ['caresses', 'flies', 'dies', 'mules', 'denied', 'died',\n",
        "         'agreed', 'owned', 'humbled', 'sized', 'meeting', 'stating',\n",
        "         'siezing', 'itemization', 'sensational', 'traditional',\n",
        "         'reference', 'colonizer', 'plotted']\n",
        "\n",
        "for word in words:\n",
        "    print(f\"{word} -> {stemmer.stem(word)}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Referencias\n",
        "\n",
        "- [Art√≠culo original de Martin Porter (1980)](https://tartarus.org/martin/PorterStemmer/)\n",
        "- Documentaci√≥n oficial de NLTK: [PorterStemmer](https://www.nltk.org/api/nltk.stem.PorterStemmer.html)\n",
        "\n",
        "---\n",
        "\n",
        "## Notas\n",
        "\n",
        "- El stemming puede ser muy √∫til para tareas de miner√≠a de texto y recuperaci√≥n de informaci√≥n.\n",
        "- Aunque el algoritmo es simple, es efectivo en muchos casos pr√°cticos.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "3JoHTA06Rlu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMOK4Gl3RKBd",
        "outputId": "881532e6-f8b0-4100-db49-3b8551bbb1a0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "s = PorterStemmer()\n",
        "text = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
        "words = word_tokenize(text)\n",
        "print(words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhYGwwldREof",
        "outputId": "3202899c-9edd-4ba1-c4ad-58879f348dd7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print([s.stem(w) for w in words])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9BjtBhgRGR1",
        "outputId": "f78deda8-c892-4a5c-e1aa-e0b4bb7e6e66"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
          ]
        }
      ]
    }
  ]
}