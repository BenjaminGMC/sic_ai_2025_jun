{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsG4IJUKlOU/J/sk9Qsgnm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidlealo/sic_ai_2025_jun/blob/main/04pln/clase_25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clase 25"
      ],
      "metadata": {
        "id": "Ka7Q3nZILHmp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# WordNetLemmatizer - DocumentaciÃ³n detallada\n",
        "\n",
        "ðŸ“Œ **Fuente oficial:**  \n",
        "https://www.nltk.org/api/nltk.stem.WordNetLemmatizer.html\n",
        "\n",
        "## Â¿QuÃ© es lematizaciÃ³n?\n",
        "\n",
        "La **lematizaciÃ³n** es el proceso de reducir una palabra a su forma base o \"lema\" utilizando un diccionario lingÃ¼Ã­stico. A diferencia del \"stemming\", que simplemente corta sufijos sin tener en cuenta el contexto gramatical, la lematizaciÃ³n produce formas reales de palabras.\n",
        "\n",
        "Ejemplo:\n",
        "- Stemming de *\"better\"* â†’ *\"bett\"*\n",
        "- LematizaciÃ³n de *\"better\"* â†’ *\"good\"* (basado en el contexto gramatical)\n",
        "\n",
        "---\n",
        "\n",
        "## IntroducciÃ³n a `WordNetLemmatizer`\n",
        "\n",
        "`WordNetLemmatizer` es una clase en `nltk.stem` que utiliza el corpus **WordNet**, una base de datos lÃ©xica del inglÃ©s, para hacer lematizaciÃ³n basada en reglas lingÃ¼Ã­sticas.\n",
        "\n",
        "```python\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## MÃ©todo principal: `lemmatize(word, pos='n')`\n",
        "\n",
        "Este mÃ©todo devuelve el **lema (forma base)** de una palabra dada, considerando opcionalmente su categorÃ­a gramatical.\n",
        "\n",
        "### ðŸ”§ ParÃ¡metros\n",
        "\n",
        "- `word` (`str`): la palabra a lematizar.\n",
        "- `pos` (`str`, opcional): parte del discurso (part-of-speech). Los valores vÃ¡lidos son:\n",
        "  - `'n'` â†’ sustantivo *(noun)* (por defecto)\n",
        "  - `'v'` â†’ verbo *(verb)*\n",
        "  - `'a'` â†’ adjetivo *(adjective)*\n",
        "  - `'r'` â†’ adverbio *(adverb)*\n",
        "  - `'s'` â†’ adjetivo satÃ©lite *(adjective satellite)*\n",
        "\n",
        "### ðŸ” Retorno\n",
        "\n",
        "Devuelve el **lema** de la palabra, es decir, su forma base segÃºn WordNet.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§ª Ejemplos de uso\n",
        "\n",
        "```python\n",
        "lemmatizer.lemmatize(\"cats\")          # 'cat'\n",
        "lemmatizer.lemmatize(\"cacti\")         # 'cactus'\n",
        "lemmatizer.lemmatize(\"geese\")         # 'goose'\n",
        "lemmatizer.lemmatize(\"rocks\")         # 'rock'\n",
        "lemmatizer.lemmatize(\"python\")        # 'python'\n",
        "lemmatizer.lemmatize(\"better\", pos=\"a\")  # 'good'\n",
        "lemmatizer.lemmatize(\"running\", pos=\"v\") # 'run'\n",
        "```\n",
        "\n",
        "âš ï¸ Por defecto, el mÃ©todo trata las palabras como **sustantivos**, por lo que es importante pasar la etiqueta `pos` adecuada para obtener resultados precisos.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§  Â¿CÃ³mo mejorar la precisiÃ³n?\n",
        "\n",
        "Para lematizar correctamente, especialmente verbos o adjetivos, es recomendable hacer un **etiquetado gramatical (POS tagging)** antes:\n",
        "\n",
        "```python\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# FunciÃ³n para convertir etiquetas POS de nltk a WordNet\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN  # default\n",
        "\n",
        "# Texto de entrada\n",
        "text = \"The striped bats are hanging on their feet for best\"\n",
        "tokens = word_tokenize(text)\n",
        "tagged = pos_tag(tokens)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tagged]\n",
        "print(lemmatized)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“¦ Requisitos previos\n",
        "\n",
        "Para usar WordNet, necesitas descargar los siguientes recursos:\n",
        "\n",
        "```python\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # Para traducciones y sinÃ³nimos\n",
        "nltk.download('punkt')    # Para tokenizar textos\n",
        "nltk.download('averaged_perceptron_tagger')  # Para POS tagging\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§­ Diferencias entre lematizaciÃ³n y stemming\n",
        "\n",
        "| CaracterÃ­stica       | LemmatizaciÃ³n                   | Stemming                        |\n",
        "|----------------------|----------------------------------|----------------------------------|\n",
        "| Usa diccionario real | âœ… SÃ­                           | âŒ No                            |\n",
        "| PrecisiÃ³n contextual | âœ… Alta (con POS)               | âŒ Baja                          |\n",
        "| Velocidad            | âš ï¸ MÃ¡s lenta                   | âœ… MÃ¡s rÃ¡pida                    |\n",
        "| Ejemplo              | \"better\" â†’ \"good\"               | \"better\" â†’ \"bett\"               |\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”š ConclusiÃ³n\n",
        "\n",
        "`WordNetLemmatizer` es ideal para tareas de procesamiento de lenguaje natural (NLP) donde se requiere una forma limpia y gramaticalmente vÃ¡lida de las palabras, como:\n",
        "\n",
        "- AnÃ¡lisis de sentimientos\n",
        "- ClasificaciÃ³n de texto\n",
        "- ExtracciÃ³n de entidades\n",
        "- TraducciÃ³n automÃ¡tica\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”— Referencias Ãºtiles\n",
        "\n",
        "- [DocumentaciÃ³n oficial de NLTK](https://www.nltk.org)\n",
        "- [WordNet en NLTK](https://www.nltk.org/howto/wordnet.html)\n",
        "- [WordNet API](https://wordnet.princeton.edu/)\n"
      ],
      "metadata": {
        "id": "YKK-VbnKMhSr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbq1ZN-6KunW",
        "outputId": "27bddd6a-43a6-4bea-cf4c-29768164452c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dog\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "print(wnl.lemmatize('dogs'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wnl.lemmatize('churches'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3fMY5fgMwcd",
        "outputId": "6a489c64-be1b-4c6e-b6de-fa883fe4e187"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "church\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wnl.lemmatize('aardwolves'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HF95U0JpM0Bt",
        "outputId": "87823ca9-6d73-4b16-c105-aab4b9a8bbeb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aardwolf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wnl.lemmatize('abaci'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mz4nOeMXNA4_",
        "outputId": "b7c0c6d0-78b9-4dfd-b0d4-d10abc72042d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abacus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wnl.lemmatize('hardrock'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppIeUSqyNDyA",
        "outputId": "001e0058-e5f3-4c98-8bdf-b3d9dd86be95"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hardrock\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print([wnl.lemmatize(w) for w in words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dlDBZLcMVXT",
        "outputId": "b3360f63-d9a3-4d72-85e6-c76929b79a2e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wnl.lemmatize('dies', 'v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPHUV1N1NjoW",
        "outputId": "ea62b8e5-5220-4e39-e15b-21884781582e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "die\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wnl.lemmatize('watched', 'v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EP1kEOo6NsB7",
        "outputId": "1a1841e0-beb0-4161-ca49-4f74c89ef9c4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "watch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wnl.lemmatize('has', 'v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8aVEw66NyQm",
        "outputId": "72ff13f8-af86-4591-f655-4b914301484a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "have\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "animals = [\n",
        "    \"aardwolves\",\n",
        "    \"mice\",\n",
        "    \"geese\",\n",
        "    \"deer\",\n",
        "    \"sheep\",\n",
        "    \"moose\",\n",
        "    \"oxen\",\n",
        "    \"manatees\",\n",
        "    \"fish\",\n",
        "    \"fishes\",\n",
        "    \"wolves\",\n",
        "    \"leafcutter ants\",\n",
        "    \"calves\",\n",
        "    \"hoofed animals\",\n",
        "    \"octopuses\",\n",
        "    \"octopi\",\n",
        "    \"cacti\",\n",
        "    \"fungi\",\n",
        "    \"platypuses\",\n",
        "    \"platypi\",\n",
        "    \"larvae\",\n",
        "    \"bacteria\"\n",
        "]\n",
        "\n",
        "print([wnl.lemmatize(a) for a in animals])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ym2vGQ-OKZ5",
        "outputId": "55135333-7e88-454a-edd1-fe7be0a55aa4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['aardwolf', 'mouse', 'goose', 'deer', 'sheep', 'moose', 'ox', 'manatee', 'fish', 'fish', 'wolf', 'leafcutter ants', 'calf', 'hoofed animals', 'octopus', 'octopus', 'cactus', 'fungi', 'platypus', 'platypi', 'larva', 'bacteria']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wnl.lemmatize('leafcutter ants', 'n'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiTxyg8IOn1f",
        "outputId": "92754017-08d8-4df6-89a4-0668347c9de9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leafcutter ants\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# PorterStemmer en NLTK - GuÃ­a detallada\n",
        "\n",
        "ðŸ“Œ Basado en la documentaciÃ³n de NLTK: https://www.nltk.org/howto/stem.html\n",
        "\n",
        "## ðŸŒ± Â¿QuÃ© es PorterStemmer?\n",
        "\n",
        "`PorterStemmer` es una implementaciÃ³n del algoritmo de stemming creado por **Martin Porter** en 1980.  \n",
        "Su objetivo es reducir una palabra a su **raÃ­z morfolÃ³gica** (stem) de forma eficiente, aunque sin preocuparse por la correcciÃ³n gramatical o si el resultado es una palabra real del idioma.\n",
        "\n",
        "Es uno de los algoritmos de stemming mÃ¡s utilizados en tareas de procesamiento de lenguaje natural (NLP).\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”§ CÃ³mo importar y usar\n",
        "\n",
        "```python\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "print(ps.stem(\"running\"))   # 'run'\n",
        "print(ps.stem(\"flies\"))     # 'fli'\n",
        "print(ps.stem(\"studies\"))   # 'studi'\n",
        "print(ps.stem(\"believable\"))# 'believ'\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§ª Ejemplo con lista de palabras\n",
        "\n",
        "```python\n",
        "words = [\"run\", \"runner\", \"running\", \"ran\", \"runs\", \"easily\", \"fairly\"]\n",
        "\n",
        "for word in words:\n",
        "    print(f\"{word} â†’ {ps.stem(word)}\")\n",
        "```\n",
        "\n",
        "### Salida esperada:\n",
        "```\n",
        "run â†’ run\n",
        "runner â†’ runner\n",
        "running â†’ run\n",
        "ran â†’ ran\n",
        "runs â†’ run\n",
        "easily â†’ easili\n",
        "fairly â†’ fairli\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“‹ CaracterÃ­sticas del algoritmo Porter\n",
        "\n",
        "- Basado en reglas morfolÃ³gicas\n",
        "- No usa diccionarios\n",
        "- RÃ¡pido y eficiente\n",
        "- Puede generar palabras inexistentes (ej. \"easily\" â†’ \"easili\")\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ†š ComparaciÃ³n con otros stemmers\n",
        "\n",
        "| Palabra     | Porter    | Lancaster |\n",
        "|-------------|-----------|-----------|\n",
        "| studies     | studi     | study     |\n",
        "| maximum     | maximum   | maxim     |\n",
        "| swimming    | swim      | swim      |\n",
        "| possibly    | possibl   | poss      |\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“¦ Requisitos para usar\n",
        "\n",
        "```python\n",
        "import nltk\n",
        "nltk.download('punkt')  # Solo si vas a tokenizar\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ§­ CuÃ¡ndo usar PorterStemmer\n",
        "\n",
        "- Cuando necesitas rapidez y simplicidad\n",
        "- En anÃ¡lisis de texto donde la precisiÃ³n gramatical no es crÃ­tica\n",
        "- En sistemas de recuperaciÃ³n de informaciÃ³n (IR), bÃºsqueda o clustering\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”š ConclusiÃ³n\n",
        "\n",
        "`PorterStemmer` es una herramienta confiable y veloz para normalizar palabras en tareas de NLP. Aunque no devuelve formas reales del idioma, es muy Ãºtil cuando se requiere agrupar variaciones de una raÃ­z comÃºn."
      ],
      "metadata": {
        "id": "oj5NGyWKPzGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "ps = PorterStemmer()\n",
        "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n",
        "for w in example_words:\n",
        "    print(ps.stem(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiFgo3NLPTi7",
        "outputId": "93e588eb-58df-4102-be94-d96294e7b9aa"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python\n",
            "python\n",
            "python\n",
            "python\n",
            "pythonli\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "animals = [\n",
        "    \"aardwolves\",\n",
        "    \"mice\",\n",
        "    \"geese\",\n",
        "    \"deer\",\n",
        "    \"sheep\",\n",
        "    \"moose\",\n",
        "    \"oxen\",\n",
        "    \"manatees\",\n",
        "    \"fish\",\n",
        "    \"fishes\",\n",
        "    \"wolves\",\n",
        "    \"leafcutter ants\",\n",
        "    \"calves\",\n",
        "    \"hoofed animals\",\n",
        "    \"octopuses\",\n",
        "    \"octopi\",\n",
        "    \"cacti\",\n",
        "    \"fungi\",\n",
        "    \"platypuses\",\n",
        "    \"platypi\",\n",
        "    \"larvae\",\n",
        "    \"bacteria\"\n",
        "]\n",
        "\n",
        "print([ps.stem(a) for a in animals])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwgTnrpvPzwm",
        "outputId": "efa5c8d7-db56-44f5-e076-048030a16094"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['aardwolv', 'mice', 'gees', 'deer', 'sheep', 'moos', 'oxen', 'manate', 'fish', 'fish', 'wolv', 'leafcutter ', 'calv', 'hoofed anim', 'octopus', 'octopi', 'cacti', 'fungi', 'platypus', 'platypi', 'larva', 'bacteria']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# nltk.stem.PorterStemmer\n",
        "\n",
        "## Clase PorterStemmer\n",
        "\n",
        "```python\n",
        "class nltk.stem.PorterStemmer()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## DescripciÃ³n\n",
        "\n",
        "Esta clase implementa el algoritmo **Porter stemming**, uno de los algoritmos de stemming mÃ¡s populares en procesamiento de lenguaje natural.\n",
        "\n",
        "El algoritmo fue creado por Martin Porter en 1980 y tiene como objetivo recortar los sufijos de las palabras para reducirlas a su raÃ­z o \"stem\".\n",
        "\n",
        "El Porter Stemmer no garantiza que el resultado sea una palabra real, pero es eficiente para agrupar palabras similares.\n",
        "\n",
        "---\n",
        "\n",
        "## MÃ©todos principales\n",
        "\n",
        "### `stem(word)`\n",
        "\n",
        "Reduce la palabra dada a su raÃ­z o stem.\n",
        "\n",
        "- **ParÃ¡metros**:\n",
        "  - `word` (`str`): La palabra a la que se le aplicarÃ¡ el stemming.\n",
        "\n",
        "- **Retorna**:\n",
        "  - `str`: La raÃ­z (stem) de la palabra.\n",
        "\n",
        "- **Ejemplo**:\n",
        "\n",
        "```python\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "print(ps.stem('running'))  # Salida: run\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Uso bÃ¡sico\n",
        "\n",
        "```python\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "words = ['caresses', 'flies', 'dies', 'mules', 'denied', 'died',\n",
        "         'agreed', 'owned', 'humbled', 'sized', 'meeting', 'stating',\n",
        "         'siezing', 'itemization', 'sensational', 'traditional',\n",
        "         'reference', 'colonizer', 'plotted']\n",
        "\n",
        "for word in words:\n",
        "    print(f\"{word} -> {stemmer.stem(word)}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Referencias\n",
        "\n",
        "- [ArtÃ­culo original de Martin Porter (1980)](https://tartarus.org/martin/PorterStemmer/)\n",
        "- DocumentaciÃ³n oficial de NLTK: [PorterStemmer](https://www.nltk.org/api/nltk.stem.PorterStemmer.html)\n",
        "\n",
        "---\n",
        "\n",
        "## Notas\n",
        "\n",
        "- El stemming puede ser muy Ãºtil para tareas de minerÃ­a de texto y recuperaciÃ³n de informaciÃ³n.\n",
        "- Aunque el algoritmo es simple, es efectivo en muchos casos prÃ¡cticos.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "3JoHTA06Rlu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMOK4Gl3RKBd",
        "outputId": "881532e6-f8b0-4100-db49-3b8551bbb1a0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "s = PorterStemmer()\n",
        "text = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
        "words = word_tokenize(text)\n",
        "print(words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhYGwwldREof",
        "outputId": "3202899c-9edd-4ba1-c4ad-58879f348dd7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print([s.stem(w) for w in words])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9BjtBhgRGR1",
        "outputId": "f78deda8-c892-4a5c-e1aa-e0b4bb7e6e66"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemmatization algoritmo de Porter tiene las siguientes reglas.\n",
        "#ALIZE â†’ AL\n",
        "#ANCE â†’ Borrar\n",
        "#ICAL â†’ IC\n",
        "\n",
        "# Ejemplo de stemming con reglas personalizadas (aparentemente solo ilustrativo)\n",
        "words = ['formalize', 'allowance', 'electricical']\n",
        "print([s.stem(w) for w in words])  # Esto requiere que definas un stemmer 's'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPAA6Pl3SNy5",
        "outputId": "a36ecb65-f876-48de-c557-ff1cca866345"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['formal', 'allow', 'electric']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Usando PorterStemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "s = PorterStemmer()\n",
        "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print([s.stem(w) for w in words])\n",
        "# Output esperado: ['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbRQs0d-SPmE",
        "outputId": "88dfb86f-7f29-4836-c684-d913bf0f41c3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Usando LancasterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "l = LancasterStemmer()\n",
        "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print([l.stem(w) for w in words])\n",
        "# Output esperado: ['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tLQeuOcSL8A",
        "outputId": "b3b1a24e-9d6b-4b25-89ae-492597bb1453"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# POS Tagging (Etiquetado Gramatical)\n",
        "\n",
        "## Â¿QuÃ© es el POS Tagging?\n",
        "\n",
        "El POS tagging (Part-of-Speech Tagging) es el proceso de asignar una categorÃ­a gramatical a cada palabra en una oraciÃ³n, como sustantivo, verbo, adjetivo, etc.\n",
        "\n",
        "---\n",
        "\n",
        "## Ejemplo de POS Tagging\n",
        "\n",
        "Frase: \"El gato duerme en la alfombra\"\n",
        "\n",
        "Etiquetas posibles (en inglÃ©s):\n",
        "\n",
        "| Palabra   | Etiqueta | Significado         |\n",
        "|-----------|----------|---------------------|\n",
        "| The       | DT       | Determiner          |\n",
        "| cat       | NN       | Noun, singular      |\n",
        "| sleeps    | VBZ      | Verb, 3ra pers. sing. presente |\n",
        "| on        | IN       | Preposition/Subordinating conjunction |\n",
        "| the       | DT       | Determiner          |\n",
        "| mat       | NN       | Noun, singular      |\n",
        "\n",
        "---\n",
        "\n",
        "## Tabla de etiquetas POS (Penn Treebank)\n",
        "\n",
        "| Etiqueta | Significado en inglÃ©s                 | TraducciÃ³n / DescripciÃ³n                         |\n",
        "|----------|----------------------------------------|--------------------------------------------------|\n",
        "| CC       | Coordinating conjunction              | ConjunciÃ³n coordinada (e.g., and, but, or)       |\n",
        "| CD       | Cardinal number                       | NÃºmero cardinal (e.g., one, two)                 |\n",
        "| DT       | Determiner                            | Determinante (e.g., the, a, an)                  |\n",
        "| EX       | Existential there                     | \"There\" existencial                              |\n",
        "| FW       | Foreign word                          | Palabra extranjera                               |\n",
        "| IN       | Preposition or subordinating conjunction | PreposiciÃ³n / conjunciÃ³n subordinante        |\n",
        "| JJ       | Adjective                             | Adjetivo                                          |\n",
        "| JJR      | Adjective, comparative                | Adjetivo comparativo (e.g., better)              |\n",
        "| JJS      | Adjective, superlative                | Adjetivo superlativo (e.g., best)                |\n",
        "| LS       | List item marker                      | Marcador de Ã­tem en lista                        |\n",
        "| MD       | Modal                                 | Verbo modal (e.g., can, should)                  |\n",
        "| NN       | Noun, singular or mass                | Sustantivo singular o incontable                 |\n",
        "| NNS      | Noun, plural                          | Sustantivo plural                                 |\n",
        "| NNP      | Proper noun, singular                 | Nombre propio singular (e.g., John)              |\n",
        "| NNPS     | Proper noun, plural                   | Nombre propio plural                              |\n",
        "| PDT      | Predeterminer                         | Predeterminante (e.g., all the kids)             |\n",
        "| POS      | Possessive ending                     | TerminaciÃ³n posesiva (e.g., â€™s)                  |\n",
        "| PRP      | Personal pronoun                      | Pronombre personal (e.g., he, they)              |\n",
        "| PRP$     | Possessive pronoun                    | Pronombre posesivo (e.g., his, her)              |\n",
        "| RB       | Adverb                                | Adverbio (e.g., quickly)                         |\n",
        "| RBR      | Adverb, comparative                   | Adverbio comparativo                             |\n",
        "| RBS      | Adverb, superlative                   | Adverbio superlativo                             |\n",
        "| RP       | Particle                              | PartÃ­cula (e.g., give up)                        |\n",
        "| SYM      | Symbol                                | SÃ­mbolo                                          |\n",
        "| TO       | to                                    | â€œtoâ€ como preposiciÃ³n o parte de infinitivo      |\n",
        "| UH       | Interjection                          | InterjecciÃ³n (e.g., uh, wow)                     |\n",
        "| VB       | Verb, base form                       | Verbo en infinitivo                              |\n",
        "| VBD      | Verb, past tense                      | Verbo en pasado                                  |\n",
        "| VBG      | Verb, gerund/present participle       | Verbo en gerundio                                |\n",
        "| VBN      | Verb, past participle                 | Participio pasado                                |\n",
        "| VBP      | Verb, non-3rd pers. sing. present     | Presente simple (yo, tÃº, nosotros)               |\n",
        "| VBZ      | Verb, 3rd pers. sing. present         | Presente simple (Ã©l, ella, eso)                  |\n",
        "| WDT      | Wh-determiner                         | Determinante interrogativo (which)               |\n",
        "| WP       | Wh-pronoun                            | Pronombre interrogativo (who)                    |\n",
        "| WP$      | Possessive wh-pronoun                 | Pronombre interrogativo posesivo (whose)         |\n",
        "| WRB      | Wh-adverb                             | Adverbio interrogativo (where, when)             |\n",
        "\n",
        "---\n",
        "\n",
        "## CÃ³digo en Python con NLTK\n",
        "\n",
        "```python\n",
        "import nltk\n",
        "\n",
        "# Descargar recursos necesarios\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Frase de ejemplo (en inglÃ©s)\n",
        "frase = \"The cat sleeps on the mat\"\n",
        "\n",
        "# TokenizaciÃ³n\n",
        "tokens = nltk.word_tokenize(frase)\n",
        "\n",
        "# Etiquetado gramatical\n",
        "etiquetas = nltk.pos_tag(tokens)\n",
        "\n",
        "# Mostrar resultados\n",
        "print(etiquetas)\n",
        "```\n",
        "\n",
        "### Salida esperada:\n",
        "```python\n",
        "[('The', 'DT'), ('cat', 'NN'), ('sleeps', 'VBZ'), ('on', 'IN'), ('the', 'DT'), ('mat', 'NN')]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Recursos adicionales\n",
        "\n",
        "- https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
        "- https://www.nltk.org/\n",
        "- https://spacy.io/models/es\n"
      ],
      "metadata": {
        "id": "5inFoU7UTsSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Descargar recursos necesarios\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epskTrN2TvV1",
        "outputId": "e3b37e40-3885-4ac5-a7e9-2668a98e53bd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQ0jhin0VAIm",
        "outputId": "91155c59-282b-47d1-b8f9-d4d259179d24"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Frase de ejemplo (en inglÃ©s)\n",
        "frase = \"The Colosseum was build by the emperor Vespassian\"\n",
        "\n",
        "# TokenizaciÃ³n\n",
        "tokens = nltk.word_tokenize(frase)\n",
        "\n",
        "# Etiquetado gramatical\n",
        "etiquetas = nltk.pos_tag(tokens)\n",
        "\n",
        "# Mostrar resultados\n",
        "print(etiquetas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okrNcWL_UutD",
        "outputId": "13f92944-bd23-4773-b7de-9d036a9a9444"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('Colosseum', 'NNP'), ('was', 'VBD'), ('build', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('emperor', 'NN'), ('Vespassian', 'JJ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# CodificaciÃ³n de Enteros en PLN\n",
        "\n",
        "## Â¿QuÃ© es la codificaciÃ³n de enteros?\n",
        "\n",
        "La codificaciÃ³n de enteros convierte palabras en nÃºmeros Ãºnicos para que los modelos de aprendizaje automÃ¡tico puedan trabajar con texto.\n",
        "\n",
        "Este proceso suele incluir los siguientes pasos:\n",
        "- Preprocesamiento del texto.\n",
        "- TokenizaciÃ³n.\n",
        "- Contar la frecuencia de las palabras.\n",
        "- Filtrar palabras poco frecuentes.\n",
        "- Asignar Ã­ndices Ãºnicos a las palabras mÃ¡s relevantes.\n",
        "\n",
        "---\n",
        "\n",
        "## CÃ³digo completo con explicaciones de cada librerÃ­a\n",
        "\n",
        "# CodificaciÃ³n de Enteros en Procesamiento de Lenguaje Natural\n",
        "\n",
        "## Paso 1: Carga de librerÃ­as necesarias\n",
        "\n",
        "```python\n",
        "import nltk\n",
        "nltk.download('punkt')  # Descarga el modelo de tokenizaciÃ³n preentrenado para segmentaciÃ³n\n",
        "```\n",
        "\n",
        "- `nltk`: Biblioteca para el procesamiento de lenguaje natural.\n",
        "- `nltk.download('punkt')`: Descarga los modelos de tokenizaciÃ³n por oraciones y palabras (preentrenados).\n",
        "\n",
        "---\n",
        "\n",
        "## Paso 2: ImportaciÃ³n de funciones clave\n",
        "\n",
        "```python\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "```\n",
        "\n",
        "- `sent_tokenize`: Divide un texto en oraciones.\n",
        "- `word_tokenize`: Divide una oraciÃ³n en palabras.\n",
        "- `stopwords`: Lista de palabras vacÃ­as (como â€œelâ€, â€œlaâ€, â€œdeâ€, â€œandâ€, etc.) en distintos idiomas.\n",
        "\n",
        "---\n",
        "\n",
        "## Paso 3: Definir el texto de entrada\n",
        "\n",
        "```python\n",
        "text = \"A barber is a person. a barber is a good person. a barber is a huge person. he Knew A Secret!. The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\"\n",
        "```\n",
        "\n",
        "AquÃ­ definimos un pÃ¡rrafo con repeticiones y variaciones para observar cÃ³mo se tokenizan y codifican las palabras.\n",
        "\n",
        "---\n",
        "\n",
        "## Paso 4: Tokenizar el texto por oraciones\n",
        "\n",
        "```python\n",
        "text = sent_tokenize(text)\n",
        "print(text)\n",
        "```\n",
        "\n",
        "Resultado:\n",
        "\n",
        "```python\n",
        "['A barber is a person.', 'a barber is a good person.', 'a barber is a huge person.', ...]\n",
        "```\n",
        "\n",
        "**Â¿QuÃ© hace esto?**  \n",
        "Convierte el pÃ¡rrafo completo en una lista de oraciones.\n",
        "\n",
        "---\n",
        "\n",
        "## Paso 5: Limpieza y tokenizaciÃ³n de palabras\n",
        "\n",
        "```python\n",
        "vocab = {}  # Diccionario para contar palabras\n",
        "sentences = []\n",
        "stop_words = set(stopwords.words('english'))  # Palabras vacÃ­as del inglÃ©s\n",
        "\n",
        "for i in text:\n",
        "    sentence = word_tokenize(i)  # Tokenizar oraciÃ³n a palabras\n",
        "    result = []\n",
        "    for word in sentence:\n",
        "        word = word.lower()  # Convertir a minÃºsculas\n",
        "        if word not in stop_words and len(word) > 2:  # Eliminar stopwords y palabras muy cortas\n",
        "            result.append(word)\n",
        "            if word not in vocab:\n",
        "                vocab[word] = 0\n",
        "            vocab[word] += 1\n",
        "    sentences.append(result)\n",
        "\n",
        "print(sentences)\n",
        "```\n",
        "\n",
        "Esto genera una lista de listas, donde cada sublista contiene las palabras relevantes de una oraciÃ³n, sin stopwords ni palabras menores a 3 caracteres.\n",
        "\n",
        "---\n",
        "\n",
        "## Paso 6: Conteo de frecuencia de palabras\n",
        "\n",
        "```python\n",
        "print(vocab)\n",
        "```\n",
        "\n",
        "Resultado esperado:\n",
        "\n",
        "```python\n",
        "{'barber': 8, 'person': 3, 'good': 1, 'huge': 5, ...}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Paso 7: Ordenar por frecuencia\n",
        "\n",
        "```python\n",
        "vocab_sorted = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n",
        "print(vocab_sorted)\n",
        "```\n",
        "\n",
        "Esto ordena las palabras desde la mÃ¡s frecuente a la menos frecuente.\n",
        "\n",
        "---\n",
        "\n",
        "## Paso 8: Asignar Ã­ndices enteros\n",
        "\n",
        "```python\n",
        "word_to_index = {}\n",
        "i = 0\n",
        "for (word, frequency) in vocab_sorted:\n",
        "    if frequency > 1:  # Solo incluir palabras con frecuencia mayor a 1\n",
        "        i += 1\n",
        "        word_to_index[word] = i\n",
        "\n",
        "print(word_to_index)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Paso 9: Usar solo las 5 palabras mÃ¡s frecuentes\n",
        "\n",
        "```python\n",
        "vocab_size = 5\n",
        "words_frequency = [w for w, c in word_to_index.items() if c >= vocab_size + 1]\n",
        "for w in words_frequency:\n",
        "    del word_to_index[w]\n",
        "\n",
        "print(word_to_index)\n",
        "```\n",
        "\n",
        "Esto filtra para conservar solo las 5 palabras mÃ¡s frecuentes.\n",
        "\n",
        "---\n",
        "\n",
        "## Paso 10: AÃ±adir palabra 'OOV' y codificar las oraciones\n",
        "\n",
        "```python\n",
        "word_to_index['OOV'] = len(word_to_index) + 1\n",
        "```\n",
        "\n",
        "`OOV` (Out-Of-Vocabulary) representa las palabras fuera del vocabulario definido.\n",
        "\n",
        "```python\n",
        "encoded = []\n",
        "for s in sentences:\n",
        "    temp = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            temp.append(word_to_index[w])\n",
        "        except KeyError:\n",
        "            temp.append(word_to_index['OOV'])\n",
        "    encoded.append(temp)\n",
        "\n",
        "print(encoded)\n",
        "```\n",
        "\n",
        "Este paso convierte cada oraciÃ³n a una lista de enteros. Si una palabra no estÃ¡ en `word_to_index`, se reemplaza por el Ã­ndice de `OOV`.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "uKrCOutRbGwH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AaK1D_H4c2NO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# ðŸ“˜ CodificaciÃ³n por Enteros en PLN (Integer Encoding)\n",
        "\n",
        "Este documento presenta distintas formas de representar texto como nÃºmeros enteros, lo cual es fundamental para tareas de procesamiento de lenguaje natural (PLN). Usaremos diversas bibliotecas de Python: `collections`, `nltk`, `numpy` y `keras`.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ 1. Uso de `Counter` de Python\n",
        "\n",
        "La clase `Counter` del mÃ³dulo `collections` permite contar la frecuencia de elementos en una lista de manera muy eficiente.\n",
        "\n",
        "```python\n",
        "from collections import Counter\n",
        "\n",
        "sentences = [['barber', 'person'], ['barber', 'good', 'person'],\n",
        "             ['barber', 'huge', 'person'], ['knew', 'secret'],\n",
        "             ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'],\n",
        "             ['barber', 'kept', 'word'], ['barber', 'kept', 'word'],\n",
        "             ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving'],\n",
        "             ['barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n",
        "\n",
        "words = sum(sentences, [])\n",
        "vocab = Counter(words)\n",
        "```\n",
        "\n",
        "`Counter(words)` crea un diccionario donde las claves son las palabras y los valores son sus frecuencias.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ 2. SelecciÃ³n de palabras mÃ¡s frecuentes\n",
        "\n",
        "```python\n",
        "vocab_size = 5\n",
        "vocab = vocab.most_common(vocab_size)\n",
        "```\n",
        "\n",
        "El mÃ©todo `.most_common(n)` devuelve una lista con las `n` palabras mÃ¡s frecuentes y sus respectivas frecuencias.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ 3. AsignaciÃ³n de Ã­ndices a palabras\n",
        "\n",
        "```python\n",
        "word_to_index = {}\n",
        "i = 0\n",
        "for word, frequency in vocab:\n",
        "    i += 1\n",
        "    word_to_index[word] = i\n",
        "```\n",
        "\n",
        "Se asigna un Ã­ndice entero creciente a las palabras mÃ¡s frecuentes. Las palabras menos frecuentes pueden ser descartadas o tratadas como \"desconocidas\".\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ 4. Uso de `FreqDist` de NLTK\n",
        "\n",
        "NLTK (Natural Language Toolkit) es una biblioteca especializada para PLN. `FreqDist` es una clase de NLTK que cuenta frecuencias al igual que `Counter`, pero estÃ¡ optimizada para tareas de procesamiento de texto.\n",
        "\n",
        "```python\n",
        "from nltk import FreqDist\n",
        "import numpy as np\n",
        "\n",
        "vocab = FreqDist(np.hstack(sentences))\n",
        "```\n",
        "\n",
        "`np.hstack(sentences)` convierte la lista de listas en una lista plana. `FreqDist` luego cuenta las frecuencias.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ 5. Uso de `enumerate`\n",
        "\n",
        "```python\n",
        "test = ['a', 'b', 'c']\n",
        "for index, value in enumerate(test):\n",
        "    print(index, value)\n",
        "```\n",
        "\n",
        "`enumerate` es Ãºtil para recorrer listas y al mismo tiempo obtener el Ã­ndice de cada elemento.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ 6. TokenizaciÃ³n con `Tokenizer` de Keras\n",
        "\n",
        "Keras es una biblioteca de alto nivel para redes neuronales que incluye utilidades para preprocesamiento de texto. `Tokenizer` convierte texto en secuencias de enteros.\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "```\n",
        "\n",
        "- `.word_index` devuelve un diccionario de palabras con su Ã­ndice asignado.\n",
        "- `.word_counts` devuelve las frecuencias.\n",
        "- `.texts_to_sequences(sentences)` convierte cada oraciÃ³n en una secuencia de enteros segÃºn el Ã­ndice de cada palabra.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ”¹ Resultado de Tokenizer\n",
        "\n",
        "```python\n",
        "tokenizer.word_index\n",
        "# {'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, ...}\n",
        "\n",
        "tokenizer.texts_to_sequences(sentences)\n",
        "# [[1, 5], [1, 8, 5], [1, 3, 5], ...]\n",
        "```\n",
        "\n",
        "Esto permite representar texto como vectores numÃ©ricos, requisito esencial para que los modelos de aprendizaje automÃ¡tico puedan procesar texto.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "0rgcbe-wcOnE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eADxMdE9bHaG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}