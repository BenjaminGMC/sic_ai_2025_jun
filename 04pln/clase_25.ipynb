{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsG4IJUKlOU/J/sk9Qsgnm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidlealo/sic_ai_2025_jun/blob/main/04pln/clase_25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clase 25"
      ],
      "metadata": {
        "id": "Ka7Q3nZILHmp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# WordNetLemmatizer - Documentaci√≥n detallada\n",
        "\n",
        "üìå **Fuente oficial:**  \n",
        "https://www.nltk.org/api/nltk.stem.WordNetLemmatizer.html\n",
        "\n",
        "## ¬øQu√© es lematizaci√≥n?\n",
        "\n",
        "La **lematizaci√≥n** es el proceso de reducir una palabra a su forma base o \"lema\" utilizando un diccionario ling√º√≠stico. A diferencia del \"stemming\", que simplemente corta sufijos sin tener en cuenta el contexto gramatical, la lematizaci√≥n produce formas reales de palabras.\n",
        "\n",
        "Ejemplo:\n",
        "- Stemming de *\"better\"* ‚Üí *\"bett\"*\n",
        "- Lematizaci√≥n de *\"better\"* ‚Üí *\"good\"* (basado en el contexto gramatical)\n",
        "\n",
        "---\n",
        "\n",
        "## Introducci√≥n a `WordNetLemmatizer`\n",
        "\n",
        "`WordNetLemmatizer` es una clase en `nltk.stem` que utiliza el corpus **WordNet**, una base de datos l√©xica del ingl√©s, para hacer lematizaci√≥n basada en reglas ling√º√≠sticas.\n",
        "\n",
        "```python\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## M√©todo principal: `lemmatize(word, pos='n')`\n",
        "\n",
        "Este m√©todo devuelve el **lema (forma base)** de una palabra dada, considerando opcionalmente su categor√≠a gramatical.\n",
        "\n",
        "### üîß Par√°metros\n",
        "\n",
        "- `word` (`str`): la palabra a lematizar.\n",
        "- `pos` (`str`, opcional): parte del discurso (part-of-speech). Los valores v√°lidos son:\n",
        "  - `'n'` ‚Üí sustantivo *(noun)* (por defecto)\n",
        "  - `'v'` ‚Üí verbo *(verb)*\n",
        "  - `'a'` ‚Üí adjetivo *(adjective)*\n",
        "  - `'r'` ‚Üí adverbio *(adverb)*\n",
        "  - `'s'` ‚Üí adjetivo sat√©lite *(adjective satellite)*\n",
        "\n",
        "### üîÅ Retorno\n",
        "\n",
        "Devuelve el **lema** de la palabra, es decir, su forma base seg√∫n WordNet.\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Ejemplos de uso\n",
        "\n",
        "```python\n",
        "lemmatizer.lemmatize(\"cats\")          # 'cat'\n",
        "lemmatizer.lemmatize(\"cacti\")         # 'cactus'\n",
        "lemmatizer.lemmatize(\"geese\")         # 'goose'\n",
        "lemmatizer.lemmatize(\"rocks\")         # 'rock'\n",
        "lemmatizer.lemmatize(\"python\")        # 'python'\n",
        "lemmatizer.lemmatize(\"better\", pos=\"a\")  # 'good'\n",
        "lemmatizer.lemmatize(\"running\", pos=\"v\") # 'run'\n",
        "```\n",
        "\n",
        "‚ö†Ô∏è Por defecto, el m√©todo trata las palabras como **sustantivos**, por lo que es importante pasar la etiqueta `pos` adecuada para obtener resultados precisos.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† ¬øC√≥mo mejorar la precisi√≥n?\n",
        "\n",
        "Para lematizar correctamente, especialmente verbos o adjetivos, es recomendable hacer un **etiquetado gramatical (POS tagging)** antes:\n",
        "\n",
        "```python\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Funci√≥n para convertir etiquetas POS de nltk a WordNet\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN  # default\n",
        "\n",
        "# Texto de entrada\n",
        "text = \"The striped bats are hanging on their feet for best\"\n",
        "tokens = word_tokenize(text)\n",
        "tagged = pos_tag(tokens)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) for word, pos in tagged]\n",
        "print(lemmatized)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üì¶ Requisitos previos\n",
        "\n",
        "Para usar WordNet, necesitas descargar los siguientes recursos:\n",
        "\n",
        "```python\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # Para traducciones y sin√≥nimos\n",
        "nltk.download('punkt')    # Para tokenizar textos\n",
        "nltk.download('averaged_perceptron_tagger')  # Para POS tagging\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üß≠ Diferencias entre lematizaci√≥n y stemming\n",
        "\n",
        "| Caracter√≠stica       | Lemmatizaci√≥n                   | Stemming                        |\n",
        "|----------------------|----------------------------------|----------------------------------|\n",
        "| Usa diccionario real | ‚úÖ S√≠                           | ‚ùå No                            |\n",
        "| Precisi√≥n contextual | ‚úÖ Alta (con POS)               | ‚ùå Baja                          |\n",
        "| Velocidad            | ‚ö†Ô∏è M√°s lenta                   | ‚úÖ M√°s r√°pida                    |\n",
        "| Ejemplo              | \"better\" ‚Üí \"good\"               | \"better\" ‚Üí \"bett\"               |\n",
        "\n",
        "---\n",
        "\n",
        "## üîö Conclusi√≥n\n",
        "\n",
        "`WordNetLemmatizer` es ideal para tareas de procesamiento de lenguaje natural (NLP) donde se requiere una forma limpia y gramaticalmente v√°lida de las palabras, como:\n",
        "\n",
        "- An√°lisis de sentimientos\n",
        "- Clasificaci√≥n de texto\n",
        "- Extracci√≥n de entidades\n",
        "- Traducci√≥n autom√°tica\n",
        "\n",
        "---\n",
        "\n",
        "## üîó Referencias √∫tiles\n",
        "\n",
        "- [Documentaci√≥n oficial de NLTK](https://www.nltk.org)\n",
        "- [WordNet en NLTK](https://www.nltk.org/howto/wordnet.html)\n",
        "- [WordNet API](https://wordnet.princeton.edu/)\n"
      ],
      "metadata": {
        "id": "YKK-VbnKMhSr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbq1ZN-6KunW",
        "outputId": "27bddd6a-43a6-4bea-cf4c-29768164452c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dog\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "print(wnl.lemmatize('dogs'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wnl.lemmatize('churches'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3fMY5fgMwcd",
        "outputId": "6a489c64-be1b-4c6e-b6de-fa883fe4e187"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "church\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wnl.lemmatize('aardwolves'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HF95U0JpM0Bt",
        "outputId": "87823ca9-6d73-4b16-c105-aab4b9a8bbeb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "aardwolf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wnl.lemmatize('abaci'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mz4nOeMXNA4_",
        "outputId": "b7c0c6d0-78b9-4dfd-b0d4-d10abc72042d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "abacus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wnl.lemmatize('hardrock'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppIeUSqyNDyA",
        "outputId": "001e0058-e5f3-4c98-8bdf-b3d9dd86be95"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hardrock\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print([wnl.lemmatize(w) for w in words])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dlDBZLcMVXT",
        "outputId": "b3360f63-d9a3-4d72-85e6-c76929b79a2e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wnl.lemmatize('dies', 'v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPHUV1N1NjoW",
        "outputId": "ea62b8e5-5220-4e39-e15b-21884781582e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "die\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wnl.lemmatize('watched', 'v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EP1kEOo6NsB7",
        "outputId": "1a1841e0-beb0-4161-ca49-4f74c89ef9c4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "watch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wnl.lemmatize('has', 'v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8aVEw66NyQm",
        "outputId": "72ff13f8-af86-4591-f655-4b914301484a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "have\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "animals = [\n",
        "    \"aardwolves\",\n",
        "    \"mice\",\n",
        "    \"geese\",\n",
        "    \"deer\",\n",
        "    \"sheep\",\n",
        "    \"moose\",\n",
        "    \"oxen\",\n",
        "    \"manatees\",\n",
        "    \"fish\",\n",
        "    \"fishes\",\n",
        "    \"wolves\",\n",
        "    \"leafcutter ants\",\n",
        "    \"calves\",\n",
        "    \"hoofed animals\",\n",
        "    \"octopuses\",\n",
        "    \"octopi\",\n",
        "    \"cacti\",\n",
        "    \"fungi\",\n",
        "    \"platypuses\",\n",
        "    \"platypi\",\n",
        "    \"larvae\",\n",
        "    \"bacteria\"\n",
        "]\n",
        "\n",
        "print([wnl.lemmatize(a) for a in animals])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ym2vGQ-OKZ5",
        "outputId": "55135333-7e88-454a-edd1-fe7be0a55aa4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['aardwolf', 'mouse', 'goose', 'deer', 'sheep', 'moose', 'ox', 'manatee', 'fish', 'fish', 'wolf', 'leafcutter ants', 'calf', 'hoofed animals', 'octopus', 'octopus', 'cactus', 'fungi', 'platypus', 'platypi', 'larva', 'bacteria']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(wnl.lemmatize('leafcutter ants', 'n'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiTxyg8IOn1f",
        "outputId": "92754017-08d8-4df6-89a4-0668347c9de9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "leafcutter ants\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# PorterStemmer en NLTK - Gu√≠a detallada\n",
        "\n",
        "üìå Basado en la documentaci√≥n de NLTK: https://www.nltk.org/howto/stem.html\n",
        "\n",
        "## üå± ¬øQu√© es PorterStemmer?\n",
        "\n",
        "`PorterStemmer` es una implementaci√≥n del algoritmo de stemming creado por **Martin Porter** en 1980.  \n",
        "Su objetivo es reducir una palabra a su **ra√≠z morfol√≥gica** (stem) de forma eficiente, aunque sin preocuparse por la correcci√≥n gramatical o si el resultado es una palabra real del idioma.\n",
        "\n",
        "Es uno de los algoritmos de stemming m√°s utilizados en tareas de procesamiento de lenguaje natural (NLP).\n",
        "\n",
        "---\n",
        "\n",
        "## üîß C√≥mo importar y usar\n",
        "\n",
        "```python\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "print(ps.stem(\"running\"))   # 'run'\n",
        "print(ps.stem(\"flies\"))     # 'fli'\n",
        "print(ps.stem(\"studies\"))   # 'studi'\n",
        "print(ps.stem(\"believable\"))# 'believ'\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ Ejemplo con lista de palabras\n",
        "\n",
        "```python\n",
        "words = [\"run\", \"runner\", \"running\", \"ran\", \"runs\", \"easily\", \"fairly\"]\n",
        "\n",
        "for word in words:\n",
        "    print(f\"{word} ‚Üí {ps.stem(word)}\")\n",
        "```\n",
        "\n",
        "### Salida esperada:\n",
        "```\n",
        "run ‚Üí run\n",
        "runner ‚Üí runner\n",
        "running ‚Üí run\n",
        "ran ‚Üí ran\n",
        "runs ‚Üí run\n",
        "easily ‚Üí easili\n",
        "fairly ‚Üí fairli\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Caracter√≠sticas del algoritmo Porter\n",
        "\n",
        "- Basado en reglas morfol√≥gicas\n",
        "- No usa diccionarios\n",
        "- R√°pido y eficiente\n",
        "- Puede generar palabras inexistentes (ej. \"easily\" ‚Üí \"easili\")\n",
        "\n",
        "---\n",
        "\n",
        "## üÜö Comparaci√≥n con otros stemmers\n",
        "\n",
        "| Palabra     | Porter    | Lancaster |\n",
        "|-------------|-----------|-----------|\n",
        "| studies     | studi     | study     |\n",
        "| maximum     | maximum   | maxim     |\n",
        "| swimming    | swim      | swim      |\n",
        "| possibly    | possibl   | poss      |\n",
        "\n",
        "---\n",
        "\n",
        "## üì¶ Requisitos para usar\n",
        "\n",
        "```python\n",
        "import nltk\n",
        "nltk.download('punkt')  # Solo si vas a tokenizar\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üß≠ Cu√°ndo usar PorterStemmer\n",
        "\n",
        "- Cuando necesitas rapidez y simplicidad\n",
        "- En an√°lisis de texto donde la precisi√≥n gramatical no es cr√≠tica\n",
        "- En sistemas de recuperaci√≥n de informaci√≥n (IR), b√∫squeda o clustering\n",
        "\n",
        "---\n",
        "\n",
        "## üîö Conclusi√≥n\n",
        "\n",
        "`PorterStemmer` es una herramienta confiable y veloz para normalizar palabras en tareas de NLP. Aunque no devuelve formas reales del idioma, es muy √∫til cuando se requiere agrupar variaciones de una ra√≠z com√∫n."
      ],
      "metadata": {
        "id": "oj5NGyWKPzGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "ps = PorterStemmer()\n",
        "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n",
        "for w in example_words:\n",
        "    print(ps.stem(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiFgo3NLPTi7",
        "outputId": "93e588eb-58df-4102-be94-d96294e7b9aa"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python\n",
            "python\n",
            "python\n",
            "python\n",
            "pythonli\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "animals = [\n",
        "    \"aardwolves\",\n",
        "    \"mice\",\n",
        "    \"geese\",\n",
        "    \"deer\",\n",
        "    \"sheep\",\n",
        "    \"moose\",\n",
        "    \"oxen\",\n",
        "    \"manatees\",\n",
        "    \"fish\",\n",
        "    \"fishes\",\n",
        "    \"wolves\",\n",
        "    \"leafcutter ants\",\n",
        "    \"calves\",\n",
        "    \"hoofed animals\",\n",
        "    \"octopuses\",\n",
        "    \"octopi\",\n",
        "    \"cacti\",\n",
        "    \"fungi\",\n",
        "    \"platypuses\",\n",
        "    \"platypi\",\n",
        "    \"larvae\",\n",
        "    \"bacteria\"\n",
        "]\n",
        "\n",
        "print([ps.stem(a) for a in animals])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwgTnrpvPzwm",
        "outputId": "efa5c8d7-db56-44f5-e076-048030a16094"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['aardwolv', 'mice', 'gees', 'deer', 'sheep', 'moos', 'oxen', 'manate', 'fish', 'fish', 'wolv', 'leafcutter ', 'calv', 'hoofed anim', 'octopus', 'octopi', 'cacti', 'fungi', 'platypus', 'platypi', 'larva', 'bacteria']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# nltk.stem.PorterStemmer\n",
        "\n",
        "## Clase PorterStemmer\n",
        "\n",
        "```python\n",
        "class nltk.stem.PorterStemmer()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Descripci√≥n\n",
        "\n",
        "Esta clase implementa el algoritmo **Porter stemming**, uno de los algoritmos de stemming m√°s populares en procesamiento de lenguaje natural.\n",
        "\n",
        "El algoritmo fue creado por Martin Porter en 1980 y tiene como objetivo recortar los sufijos de las palabras para reducirlas a su ra√≠z o \"stem\".\n",
        "\n",
        "El Porter Stemmer no garantiza que el resultado sea una palabra real, pero es eficiente para agrupar palabras similares.\n",
        "\n",
        "---\n",
        "\n",
        "## M√©todos principales\n",
        "\n",
        "### `stem(word)`\n",
        "\n",
        "Reduce la palabra dada a su ra√≠z o stem.\n",
        "\n",
        "- **Par√°metros**:\n",
        "  - `word` (`str`): La palabra a la que se le aplicar√° el stemming.\n",
        "\n",
        "- **Retorna**:\n",
        "  - `str`: La ra√≠z (stem) de la palabra.\n",
        "\n",
        "- **Ejemplo**:\n",
        "\n",
        "```python\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "print(ps.stem('running'))  # Salida: run\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Uso b√°sico\n",
        "\n",
        "```python\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "words = ['caresses', 'flies', 'dies', 'mules', 'denied', 'died',\n",
        "         'agreed', 'owned', 'humbled', 'sized', 'meeting', 'stating',\n",
        "         'siezing', 'itemization', 'sensational', 'traditional',\n",
        "         'reference', 'colonizer', 'plotted']\n",
        "\n",
        "for word in words:\n",
        "    print(f\"{word} -> {stemmer.stem(word)}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Referencias\n",
        "\n",
        "- [Art√≠culo original de Martin Porter (1980)](https://tartarus.org/martin/PorterStemmer/)\n",
        "- Documentaci√≥n oficial de NLTK: [PorterStemmer](https://www.nltk.org/api/nltk.stem.PorterStemmer.html)\n",
        "\n",
        "---\n",
        "\n",
        "## Notas\n",
        "\n",
        "- El stemming puede ser muy √∫til para tareas de miner√≠a de texto y recuperaci√≥n de informaci√≥n.\n",
        "- Aunque el algoritmo es simple, es efectivo en muchos casos pr√°cticos.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "3JoHTA06Rlu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMOK4Gl3RKBd",
        "outputId": "881532e6-f8b0-4100-db49-3b8551bbb1a0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "s = PorterStemmer()\n",
        "text = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
        "words = word_tokenize(text)\n",
        "print(words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhYGwwldREof",
        "outputId": "3202899c-9edd-4ba1-c4ad-58879f348dd7"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print([s.stem(w) for w in words])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9BjtBhgRGR1",
        "outputId": "f78deda8-c892-4a5c-e1aa-e0b4bb7e6e66"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemmatization algoritmo de Porter tiene las siguientes reglas.\n",
        "#ALIZE ‚Üí AL\n",
        "#ANCE ‚Üí Borrar\n",
        "#ICAL ‚Üí IC\n",
        "\n",
        "# Ejemplo de stemming con reglas personalizadas (aparentemente solo ilustrativo)\n",
        "words = ['formalize', 'allowance', 'electricical']\n",
        "print([s.stem(w) for w in words])  # Esto requiere que definas un stemmer 's'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPAA6Pl3SNy5",
        "outputId": "a36ecb65-f876-48de-c557-ff1cca866345"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['formal', 'allow', 'electric']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Usando PorterStemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "s = PorterStemmer()\n",
        "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print([s.stem(w) for w in words])\n",
        "# Output esperado: ['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbRQs0d-SPmE",
        "outputId": "88dfb86f-7f29-4836-c684-d913bf0f41c3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Usando LancasterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "l = LancasterStemmer()\n",
        "words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print([l.stem(w) for w in words])\n",
        "# Output esperado: ['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tLQeuOcSL8A",
        "outputId": "b3b1a24e-9d6b-4b25-89ae-492597bb1453"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# POS Tagging (Etiquetado Gramatical)\n",
        "\n",
        "## ¬øQu√© es el POS Tagging?\n",
        "\n",
        "El POS tagging (Part-of-Speech Tagging) es el proceso de asignar una categor√≠a gramatical a cada palabra en una oraci√≥n, como sustantivo, verbo, adjetivo, etc.\n",
        "\n",
        "---\n",
        "\n",
        "## Ejemplo de POS Tagging\n",
        "\n",
        "Frase: \"El gato duerme en la alfombra\"\n",
        "\n",
        "Etiquetas posibles (en ingl√©s):\n",
        "\n",
        "| Palabra   | Etiqueta | Significado         |\n",
        "|-----------|----------|---------------------|\n",
        "| The       | DT       | Determiner          |\n",
        "| cat       | NN       | Noun, singular      |\n",
        "| sleeps    | VBZ      | Verb, 3ra pers. sing. presente |\n",
        "| on        | IN       | Preposition/Subordinating conjunction |\n",
        "| the       | DT       | Determiner          |\n",
        "| mat       | NN       | Noun, singular      |\n",
        "\n",
        "---\n",
        "\n",
        "## Tabla de etiquetas POS (Penn Treebank)\n",
        "\n",
        "| Etiqueta | Significado en ingl√©s                 | Traducci√≥n / Descripci√≥n                         |\n",
        "|----------|----------------------------------------|--------------------------------------------------|\n",
        "| CC       | Coordinating conjunction              | Conjunci√≥n coordinada (e.g., and, but, or)       |\n",
        "| CD       | Cardinal number                       | N√∫mero cardinal (e.g., one, two)                 |\n",
        "| DT       | Determiner                            | Determinante (e.g., the, a, an)                  |\n",
        "| EX       | Existential there                     | \"There\" existencial                              |\n",
        "| FW       | Foreign word                          | Palabra extranjera                               |\n",
        "| IN       | Preposition or subordinating conjunction | Preposici√≥n / conjunci√≥n subordinante        |\n",
        "| JJ       | Adjective                             | Adjetivo                                          |\n",
        "| JJR      | Adjective, comparative                | Adjetivo comparativo (e.g., better)              |\n",
        "| JJS      | Adjective, superlative                | Adjetivo superlativo (e.g., best)                |\n",
        "| LS       | List item marker                      | Marcador de √≠tem en lista                        |\n",
        "| MD       | Modal                                 | Verbo modal (e.g., can, should)                  |\n",
        "| NN       | Noun, singular or mass                | Sustantivo singular o incontable                 |\n",
        "| NNS      | Noun, plural                          | Sustantivo plural                                 |\n",
        "| NNP      | Proper noun, singular                 | Nombre propio singular (e.g., John)              |\n",
        "| NNPS     | Proper noun, plural                   | Nombre propio plural                              |\n",
        "| PDT      | Predeterminer                         | Predeterminante (e.g., all the kids)             |\n",
        "| POS      | Possessive ending                     | Terminaci√≥n posesiva (e.g., ‚Äôs)                  |\n",
        "| PRP      | Personal pronoun                      | Pronombre personal (e.g., he, they)              |\n",
        "| PRP$     | Possessive pronoun                    | Pronombre posesivo (e.g., his, her)              |\n",
        "| RB       | Adverb                                | Adverbio (e.g., quickly)                         |\n",
        "| RBR      | Adverb, comparative                   | Adverbio comparativo                             |\n",
        "| RBS      | Adverb, superlative                   | Adverbio superlativo                             |\n",
        "| RP       | Particle                              | Part√≠cula (e.g., give up)                        |\n",
        "| SYM      | Symbol                                | S√≠mbolo                                          |\n",
        "| TO       | to                                    | ‚Äúto‚Äù como preposici√≥n o parte de infinitivo      |\n",
        "| UH       | Interjection                          | Interjecci√≥n (e.g., uh, wow)                     |\n",
        "| VB       | Verb, base form                       | Verbo en infinitivo                              |\n",
        "| VBD      | Verb, past tense                      | Verbo en pasado                                  |\n",
        "| VBG      | Verb, gerund/present participle       | Verbo en gerundio                                |\n",
        "| VBN      | Verb, past participle                 | Participio pasado                                |\n",
        "| VBP      | Verb, non-3rd pers. sing. present     | Presente simple (yo, t√∫, nosotros)               |\n",
        "| VBZ      | Verb, 3rd pers. sing. present         | Presente simple (√©l, ella, eso)                  |\n",
        "| WDT      | Wh-determiner                         | Determinante interrogativo (which)               |\n",
        "| WP       | Wh-pronoun                            | Pronombre interrogativo (who)                    |\n",
        "| WP$      | Possessive wh-pronoun                 | Pronombre interrogativo posesivo (whose)         |\n",
        "| WRB      | Wh-adverb                             | Adverbio interrogativo (where, when)             |\n",
        "\n",
        "---\n",
        "\n",
        "## C√≥digo en Python con NLTK\n",
        "\n",
        "```python\n",
        "import nltk\n",
        "\n",
        "# Descargar recursos necesarios\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Frase de ejemplo (en ingl√©s)\n",
        "frase = \"The cat sleeps on the mat\"\n",
        "\n",
        "# Tokenizaci√≥n\n",
        "tokens = nltk.word_tokenize(frase)\n",
        "\n",
        "# Etiquetado gramatical\n",
        "etiquetas = nltk.pos_tag(tokens)\n",
        "\n",
        "# Mostrar resultados\n",
        "print(etiquetas)\n",
        "```\n",
        "\n",
        "### Salida esperada:\n",
        "```python\n",
        "[('The', 'DT'), ('cat', 'NN'), ('sleeps', 'VBZ'), ('on', 'IN'), ('the', 'DT'), ('mat', 'NN')]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Recursos adicionales\n",
        "\n",
        "- https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
        "- https://www.nltk.org/\n",
        "- https://spacy.io/models/es\n"
      ],
      "metadata": {
        "id": "5inFoU7UTsSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Descargar recursos necesarios\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epskTrN2TvV1",
        "outputId": "e3b37e40-3885-4ac5-a7e9-2668a98e53bd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQ0jhin0VAIm",
        "outputId": "91155c59-282b-47d1-b8f9-d4d259179d24"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Frase de ejemplo (en ingl√©s)\n",
        "frase = \"The Colosseum was build by the emperor Vespassian\"\n",
        "\n",
        "# Tokenizaci√≥n\n",
        "tokens = nltk.word_tokenize(frase)\n",
        "\n",
        "# Etiquetado gramatical\n",
        "etiquetas = nltk.pos_tag(tokens)\n",
        "\n",
        "# Mostrar resultados\n",
        "print(etiquetas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okrNcWL_UutD",
        "outputId": "13f92944-bd23-4773-b7de-9d036a9a9444"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('Colosseum', 'NNP'), ('was', 'VBD'), ('build', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('emperor', 'NN'), ('Vespassian', 'JJ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Codificaci√≥n de Enteros en PLN\n",
        "\n",
        "## ¬øQu√© es la codificaci√≥n de enteros?\n",
        "\n",
        "La codificaci√≥n de enteros convierte palabras en n√∫meros √∫nicos para que los modelos de aprendizaje autom√°tico puedan trabajar con texto.\n",
        "\n",
        "Este proceso suele incluir los siguientes pasos:\n",
        "- Preprocesamiento del texto.\n",
        "- Tokenizaci√≥n.\n",
        "- Contar la frecuencia de las palabras.\n",
        "- Filtrar palabras poco frecuentes.\n",
        "- Asignar √≠ndices √∫nicos a las palabras m√°s relevantes.\n",
        "\n",
        "---\n",
        "\n",
        "## C√≥digo completo con explicaciones de cada librer√≠a\n",
        "\n",
        "# Codificaci√≥n de Enteros en Procesamiento de Lenguaje Natural\n",
        "\n",
        "## Paso 1: Carga de librer√≠as necesarias\n",
        "\n",
        "```python\n",
        "import nltk\n",
        "nltk.download('punkt')  # Descarga el modelo de tokenizaci√≥n preentrenado para segmentaci√≥n\n",
        "```\n",
        "\n",
        "- `nltk`: Biblioteca para el procesamiento de lenguaje natural.\n",
        "- `nltk.download('punkt')`: Descarga los modelos de tokenizaci√≥n por oraciones y palabras (preentrenados).\n",
        "\n",
        "---\n",
        "\n",
        "## Paso 2: Importaci√≥n de funciones clave\n",
        "\n",
        "```python\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "```\n",
        "\n",
        "- `sent_tokenize`: Divide un texto en oraciones.\n",
        "- `word_tokenize`: Divide una oraci√≥n en palabras.\n",
        "- `stopwords`: Lista de palabras vac√≠as (como ‚Äúel‚Äù, ‚Äúla‚Äù, ‚Äúde‚Äù, ‚Äúand‚Äù, etc.) en distintos idiomas.\n",
        "\n",
        "---\n",
        "\n",
        "## Paso 3: Definir el texto de entrada\n",
        "\n",
        "```python\n",
        "text = \"A barber is a person. a barber is a good person. a barber is a huge person. he Knew A Secret!. The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain.\"\n",
        "```\n",
        "\n",
        "Aqu√≠ definimos un p√°rrafo con repeticiones y variaciones para observar c√≥mo se tokenizan y codifican las palabras.\n",
        "\n",
        "---\n",
        "\n",
        "## Paso 4: Tokenizar el texto por oraciones\n",
        "\n",
        "```python\n",
        "text = sent_tokenize(text)\n",
        "print(text)\n",
        "```\n",
        "\n",
        "Resultado:\n",
        "\n",
        "```python\n",
        "['A barber is a person.', 'a barber is a good person.', 'a barber is a huge person.', ...]\n",
        "```\n",
        "\n",
        "**¬øQu√© hace esto?**  \n",
        "Convierte el p√°rrafo completo en una lista de oraciones.\n",
        "\n",
        "---\n",
        "\n",
        "## Paso 5: Limpieza y tokenizaci√≥n de palabras\n",
        "\n",
        "```python\n",
        "vocab = {}  # Diccionario para contar palabras\n",
        "sentences = []\n",
        "stop_words = set(stopwords.words('english'))  # Palabras vac√≠as del ingl√©s\n",
        "\n",
        "for i in text:\n",
        "    sentence = word_tokenize(i)  # Tokenizar oraci√≥n a palabras\n",
        "    result = []\n",
        "    for word in sentence:\n",
        "        word = word.lower()  # Convertir a min√∫sculas\n",
        "        if word not in stop_words and len(word) > 2:  # Eliminar stopwords y palabras muy cortas\n",
        "            result.append(word)\n",
        "            if word not in vocab:\n",
        "                vocab[word] = 0\n",
        "            vocab[word] += 1\n",
        "    sentences.append(result)\n",
        "\n",
        "print(sentences)\n",
        "```\n",
        "\n",
        "Esto genera una lista de listas, donde cada sublista contiene las palabras relevantes de una oraci√≥n, sin stopwords ni palabras menores a 3 caracteres.\n",
        "\n",
        "---\n",
        "\n",
        "## Paso 6: Conteo de frecuencia de palabras\n",
        "\n",
        "```python\n",
        "print(vocab)\n",
        "```\n",
        "\n",
        "Resultado esperado:\n",
        "\n",
        "```python\n",
        "{'barber': 8, 'person': 3, 'good': 1, 'huge': 5, ...}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Paso 7: Ordenar por frecuencia\n",
        "\n",
        "```python\n",
        "vocab_sorted = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n",
        "print(vocab_sorted)\n",
        "```\n",
        "\n",
        "Esto ordena las palabras desde la m√°s frecuente a la menos frecuente.\n",
        "\n",
        "---\n",
        "\n",
        "## Paso 8: Asignar √≠ndices enteros\n",
        "\n",
        "```python\n",
        "word_to_index = {}\n",
        "i = 0\n",
        "for (word, frequency) in vocab_sorted:\n",
        "    if frequency > 1:  # Solo incluir palabras con frecuencia mayor a 1\n",
        "        i += 1\n",
        "        word_to_index[word] = i\n",
        "\n",
        "print(word_to_index)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Paso 9: Usar solo las 5 palabras m√°s frecuentes\n",
        "\n",
        "```python\n",
        "vocab_size = 5\n",
        "words_frequency = [w for w, c in word_to_index.items() if c >= vocab_size + 1]\n",
        "for w in words_frequency:\n",
        "    del word_to_index[w]\n",
        "\n",
        "print(word_to_index)\n",
        "```\n",
        "\n",
        "Esto filtra para conservar solo las 5 palabras m√°s frecuentes.\n",
        "\n",
        "---\n",
        "\n",
        "## Paso 10: A√±adir palabra 'OOV' y codificar las oraciones\n",
        "\n",
        "```python\n",
        "word_to_index['OOV'] = len(word_to_index) + 1\n",
        "```\n",
        "\n",
        "`OOV` (Out-Of-Vocabulary) representa las palabras fuera del vocabulario definido.\n",
        "\n",
        "```python\n",
        "encoded = []\n",
        "for s in sentences:\n",
        "    temp = []\n",
        "    for w in s:\n",
        "        try:\n",
        "            temp.append(word_to_index[w])\n",
        "        except KeyError:\n",
        "            temp.append(word_to_index['OOV'])\n",
        "    encoded.append(temp)\n",
        "\n",
        "print(encoded)\n",
        "```\n",
        "\n",
        "Este paso convierte cada oraci√≥n a una lista de enteros. Si una palabra no est√° en `word_to_index`, se reemplaza por el √≠ndice de `OOV`.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "uKrCOutRbGwH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AaK1D_H4c2NO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# üìò Codificaci√≥n por Enteros en PLN (Integer Encoding)\n",
        "\n",
        "Este documento presenta distintas formas de representar texto como n√∫meros enteros, lo cual es fundamental para tareas de procesamiento de lenguaje natural (PLN). Usaremos diversas bibliotecas de Python: `collections`, `nltk`, `numpy` y `keras`.\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ 1. Uso de `Counter` de Python\n",
        "\n",
        "La clase `Counter` del m√≥dulo `collections` permite contar la frecuencia de elementos en una lista de manera muy eficiente.\n",
        "\n",
        "```python\n",
        "from collections import Counter\n",
        "\n",
        "sentences = [['barber', 'person'], ['barber', 'good', 'person'],\n",
        "             ['barber', 'huge', 'person'], ['knew', 'secret'],\n",
        "             ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'],\n",
        "             ['barber', 'kept', 'word'], ['barber', 'kept', 'word'],\n",
        "             ['barber', 'kept', 'secret'], ['keeping', 'keeping', 'huge', 'secret', 'driving'],\n",
        "             ['barber', 'crazy'], ['barber', 'went', 'huge', 'mountain']]\n",
        "\n",
        "words = sum(sentences, [])\n",
        "vocab = Counter(words)\n",
        "```\n",
        "\n",
        "`Counter(words)` crea un diccionario donde las claves son las palabras y los valores son sus frecuencias.\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ 2. Selecci√≥n de palabras m√°s frecuentes\n",
        "\n",
        "```python\n",
        "vocab_size = 5\n",
        "vocab = vocab.most_common(vocab_size)\n",
        "```\n",
        "\n",
        "El m√©todo `.most_common(n)` devuelve una lista con las `n` palabras m√°s frecuentes y sus respectivas frecuencias.\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ 3. Asignaci√≥n de √≠ndices a palabras\n",
        "\n",
        "```python\n",
        "word_to_index = {}\n",
        "i = 0\n",
        "for word, frequency in vocab:\n",
        "    i += 1\n",
        "    word_to_index[word] = i\n",
        "```\n",
        "\n",
        "Se asigna un √≠ndice entero creciente a las palabras m√°s frecuentes. Las palabras menos frecuentes pueden ser descartadas o tratadas como \"desconocidas\".\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ 4. Uso de `FreqDist` de NLTK\n",
        "\n",
        "NLTK (Natural Language Toolkit) es una biblioteca especializada para PLN. `FreqDist` es una clase de NLTK que cuenta frecuencias al igual que `Counter`, pero est√° optimizada para tareas de procesamiento de texto.\n",
        "\n",
        "```python\n",
        "from nltk import FreqDist\n",
        "import numpy as np\n",
        "\n",
        "vocab = FreqDist(np.hstack(sentences))\n",
        "```\n",
        "\n",
        "`np.hstack(sentences)` convierte la lista de listas en una lista plana. `FreqDist` luego cuenta las frecuencias.\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ 5. Uso de `enumerate`\n",
        "\n",
        "```python\n",
        "test = ['a', 'b', 'c']\n",
        "for index, value in enumerate(test):\n",
        "    print(index, value)\n",
        "```\n",
        "\n",
        "`enumerate` es √∫til para recorrer listas y al mismo tiempo obtener el √≠ndice de cada elemento.\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ 6. Tokenizaci√≥n con `Tokenizer` de Keras\n",
        "\n",
        "Keras es una biblioteca de alto nivel para redes neuronales que incluye utilidades para preprocesamiento de texto. `Tokenizer` convierte texto en secuencias de enteros.\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "```\n",
        "\n",
        "- `.word_index` devuelve un diccionario de palabras con su √≠ndice asignado.\n",
        "- `.word_counts` devuelve las frecuencias.\n",
        "- `.texts_to_sequences(sentences)` convierte cada oraci√≥n en una secuencia de enteros seg√∫n el √≠ndice de cada palabra.\n",
        "\n",
        "---\n",
        "\n",
        "## üîπ Resultado de Tokenizer\n",
        "\n",
        "```python\n",
        "tokenizer.word_index\n",
        "# {'barber': 1, 'secret': 2, 'huge': 3, 'kept': 4, 'person': 5, 'word': 6, ...}\n",
        "\n",
        "tokenizer.texts_to_sequences(sentences)\n",
        "# [[1, 5], [1, 8, 5], [1, 3, 5], ...]\n",
        "```\n",
        "\n",
        "Esto permite representar texto como vectores num√©ricos, requisito esencial para que los modelos de aprendizaje autom√°tico puedan procesar texto.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "0rgcbe-wcOnE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eADxMdE9bHaG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}