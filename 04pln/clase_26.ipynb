{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOzGkhwGn61TqBaPfdDx9ap",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidlealo/sic_ai_2025_jun/blob/main/04pln/clase_26.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Padding en Procesamiento de Lenguaje Natural (PLN)\n",
        "\n",
        "El *padding* es una técnica usada para igualar la longitud de las secuencias de texto. Es especialmente útil cuando usamos modelos que requieren entradas de tamaño fijo, como redes neuronales.\n",
        "\n",
        "En este ejemplo veremos cómo se realiza el padding utilizando:\n",
        "\n",
        "1. NumPy (manualmente)\n",
        "2. La función `pad_sequences` de Keras, más simple y automática\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Tokenización y codificación de frases\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "```\n",
        "\n",
        "Importamos las librerías necesarias:\n",
        "- `numpy`: para manipulación numérica y de arrays.\n",
        "- `Tokenizer` de `keras.preprocessing.text`: para transformar texto a números.\n",
        "\n",
        "```python\n",
        "sentences = [\n",
        "    ['barber', 'person'],\n",
        "    ['barber', 'good', 'person'],\n",
        "    ['barber', 'huge', 'person'],\n",
        "    ['knew', 'secret'],\n",
        "    ['secret', 'kept', 'huge', 'secret'],\n",
        "    ['huge', 'secret'],\n",
        "    ['barber', 'kept', 'word'],\n",
        "    ['barber', 'kept', 'word'],\n",
        "    ['barber', 'kept', 'secret'],\n",
        "    ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
        "    ['barber', 'went', 'huge', 'mountain']\n",
        "]\n",
        "```\n",
        "\n",
        "Creamos una lista de frases (listas de palabras). Estas frases se van a codificar posteriormente.\n",
        "\n",
        "```python\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "```\n",
        "\n",
        "- Creamos un objeto `Tokenizer`.\n",
        "- Usamos `fit_on_texts(sentences)` para crear un vocabulario basado en la frecuencia de aparición de las palabras.\n",
        "\n",
        "```python\n",
        "encoded = tokenizer.texts_to_sequences(sentences)\n",
        "print(encoded)\n",
        "```\n",
        "\n",
        "Transformamos las palabras en enteros. Cada palabra del vocabulario se asigna a un número único. Por ejemplo:\n",
        "```python\n",
        "[['barber', 'person']] -> [1, 5]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Encontrar la longitud máxima\n",
        "\n",
        "```python\n",
        "max_len = max(len(item) for item in encoded)\n",
        "print(max_len)\n",
        "```\n",
        "\n",
        "Calculamos la longitud máxima de las frases codificadas. Esto es necesario para saber cuántos ceros agregar a las secuencias más cortas.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Padding manual con NumPy\n",
        "\n",
        "```python\n",
        "for item in encoded:             # Para cada frase\n",
        "    while len(item) < max_len:  # Si es menor que la longitud máxima\n",
        "        item.append(0)          # Agrega ceros al final (post-padding)\n",
        "\n",
        "padded_np = np.array(encoded)\n",
        "padded_np\n",
        "```\n",
        "\n",
        "- Recorremos cada lista de enteros (`item`) y le agregamos ceros al final hasta igualar la longitud máxima.\n",
        "- Convertimos la lista a un `array` de NumPy.\n",
        "\n",
        "Ejemplo de salida:\n",
        "```python\n",
        "array([[ 1,  5,  0,  0,  0,  0,  0],\n",
        "       [ 1,  8,  5,  0,  0,  0,  0],\n",
        "       ...\n",
        "       [ 1, 12,  3, 13,  0,  0,  0]])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Padding con la herramienta de Keras\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "```\n",
        "\n",
        "Importamos la función `pad_sequences`, que permite aplicar padding fácilmente a listas de enteros.\n",
        "\n",
        "```python\n",
        "encoded = tokenizer.texts_to_sequences(sentences)\n",
        "print(encoded)\n",
        "```\n",
        "\n",
        "Codificamos nuevamente las frases, igual que antes.\n",
        "\n",
        "```python\n",
        "padded = pad_sequences(encoded)\n",
        "padded\n",
        "```\n",
        "\n",
        "- `pad_sequences` aplica padding automático por defecto al inicio de cada secuencia (pre-padding).\n",
        "- Agrega ceros al comienzo de las secuencias más cortas para igualarlas con la más larga.\n",
        "\n",
        "Ejemplo de salida:\n",
        "```python\n",
        "array([[ 0,  0,  0,  0,  0,  1,  5],\n",
        "       [ 0,  0,  0,  0,  1,  8,  5],\n",
        "       ...\n",
        "       [ 0,  0,  1, 12,  3, 13]])\n",
        "```\n",
        "\n",
        "Si quisiéramos usar **post-padding** (agregar ceros al final), podemos usar:\n",
        "```python\n",
        "pad_sequences(encoded, padding='post')\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusión\n",
        "\n",
        "El padding es fundamental para trabajar con modelos de aprendizaje automático en NLP, ya que la mayoría requiere que las entradas tengan la misma forma. Podemos hacerlo de forma manual con NumPy, o automatizarlo usando `pad_sequences` de Keras, lo cual es más limpio y eficiente.\n"
      ],
      "metadata": {
        "id": "8i6kjbbFU3hz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKVJpCaySllU"
      },
      "outputs": [],
      "source": [
        "# Visita la documentación https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'],\n",
        "             ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'],\n",
        "             ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'],\n",
        "             ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
        "             ['barber', 'went', 'huge', 'mountain']]"
      ],
      "metadata": {
        "id": "32w1mTiOXQu_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences) #The group of words are created based on frequency when putting corpus fit_ontexts().\n",
        "\n",
        "encoded = tokenizer.texts_to_sequences(sentences)\n",
        "print(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bivSDAIxXUmP",
        "outputId": "cd3ed5b6-bf5f-4b7f-a151-82c8d56c65a2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = max(len(item) for item in encoded)\n",
        "print(max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6G-MsBOrXXrL",
        "outputId": "c949d9d0-cc0e-4859-b35c-a6ca4ca87214"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1Zdc0ySVyjX",
        "outputId": "b948a7ad-147e-44ca-a374-d08d4d66f802"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  5,  0,  0,  0,  0,  0],\n",
              "       [ 1,  8,  5,  0,  0,  0,  0],\n",
              "       [ 1,  3,  5,  0,  0,  0,  0],\n",
              "       [ 9,  2,  0,  0,  0,  0,  0],\n",
              "       [ 2,  4,  3,  2,  0,  0,  0],\n",
              "       [ 3,  2,  0,  0,  0,  0,  0],\n",
              "       [ 1,  4,  6,  0,  0,  0,  0],\n",
              "       [ 1,  4,  6,  0,  0,  0,  0],\n",
              "       [ 1,  4,  2,  0,  0,  0,  0],\n",
              "       [ 7,  7,  3,  2, 10,  1, 11],\n",
              "       [ 1, 12,  3, 13,  0,  0,  0]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "for item in encoded: # For each item\n",
        "    while len(item) < max_len:   # If less than max_len\n",
        "        item.append(0)\n",
        "\n",
        "padded_np = np.array(encoded)\n",
        "padded_np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdqaHjykVyjY"
      },
      "source": [
        "Padding with Keras preprocessing tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oX72vx4eVyja"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3u7MXFTVyja",
        "outputId": "9399aa9a-6cfd-4f1e-bc66-aca4e0c6151b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
          ]
        }
      ],
      "source": [
        "encoded = tokenizer.texts_to_sequences(sentences)\n",
        "print(encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZsNGeoGVyjb",
        "outputId": "826bc743-6ecb-400b-ab4f-b2eabf002e36"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0,  0,  0,  1,  5],\n",
              "       [ 0,  0,  0,  0,  1,  8,  5],\n",
              "       [ 0,  0,  0,  0,  1,  3,  5],\n",
              "       [ 0,  0,  0,  0,  0,  9,  2],\n",
              "       [ 0,  0,  0,  2,  4,  3,  2],\n",
              "       [ 0,  0,  0,  0,  0,  3,  2],\n",
              "       [ 0,  0,  0,  0,  1,  4,  6],\n",
              "       [ 0,  0,  0,  0,  1,  4,  6],\n",
              "       [ 0,  0,  0,  0,  1,  4,  2],\n",
              "       [ 7,  7,  3,  2, 10,  1, 11],\n",
              "       [ 0,  0,  0,  1, 12,  3, 13]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "padded = pad_sequences(encoded)\n",
        "padded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3zWnq6ZVyjb",
        "outputId": "cd3886be-8725-4aa0-e30e-e5bb2a856c84"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  5,  0,  0,  0,  0,  0],\n",
              "       [ 1,  8,  5,  0,  0,  0,  0],\n",
              "       [ 1,  3,  5,  0,  0,  0,  0],\n",
              "       [ 9,  2,  0,  0,  0,  0,  0],\n",
              "       [ 2,  4,  3,  2,  0,  0,  0],\n",
              "       [ 3,  2,  0,  0,  0,  0,  0],\n",
              "       [ 1,  4,  6,  0,  0,  0,  0],\n",
              "       [ 1,  4,  6,  0,  0,  0,  0],\n",
              "       [ 1,  4,  2,  0,  0,  0,  0],\n",
              "       [ 7,  7,  3,  2, 10,  1, 11],\n",
              "       [ 1, 12,  3, 13,  0,  0,  0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "padded = pad_sequences(encoded, padding='post')\n",
        "padded"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Modelos que requieren entradas de tamaño fijo en Deep Learning\n",
        "\n",
        "Muchos modelos de aprendizaje profundo requieren entradas de tamaño fijo para funcionar correctamente, especialmente durante el entrenamiento en lotes (*batch training*). Esto se debe a que las capas internas (densas, convolucionales, etc.) necesitan dimensiones predecibles.\n",
        "\n",
        "A continuación se explican los principales modelos que requieren entradas de tamaño fijo, junto con ejemplos en Python.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Redes Neuronales Densas (Dense / Fully Connected)\n",
        "\n",
        "### ¿Por qué requieren tamaño fijo?\n",
        "Cada neurona espera una cantidad fija de entradas. Por lo tanto, el vector de entrada debe tener una dimensión específica.\n",
        "\n",
        "### Ejemplo en Python\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([[1, 2, 3], [4, 5, 6]])  # entrada de tamaño fijo: 3 features\n",
        "model = Sequential([\n",
        "    Dense(10, input_shape=(3,), activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.summary()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Redes Convolucionales (CNNs)\n",
        "\n",
        "### ¿Por qué requieren tamaño fijo?\n",
        "Las operaciones de convolución requieren dimensiones definidas (ancho, alto, canales).\n",
        "\n",
        "### Ejemplo en Python\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, Flatten\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, kernel_size=(3, 3), input_shape=(28, 28, 1), activation='relu'),\n",
        "    Flatten(),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "model.summary()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Redes Recurrentes (RNN, LSTM, GRU)\n",
        "\n",
        "### ¿Por qué requieren tamaño fijo?\n",
        "Aunque las RNN pueden manejar secuencias de longitud variable, en entrenamiento por lotes es necesario paddear las secuencias para que todas tengan la misma longitud.\n",
        "\n",
        "### Ejemplo en Python\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM\n",
        "\n",
        "# Ejemplo de secuencias con longitudes distintas\n",
        "sequences = [[1, 2, 3], [4, 5], [6]]\n",
        "padded = pad_sequences(sequences)\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=10, output_dim=4, input_length=padded.shape[1]),\n",
        "    LSTM(8)\n",
        "])\n",
        "model.summary()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Transformers (BERT, GPT)\n",
        "\n",
        "### ¿Por qué requieren tamaño fijo?\n",
        "Las entradas a modelos transformers se paddean para igualar la longitud, y se usan máscaras para ignorar los ceros.\n",
        "\n",
        "### Ejemplo en Python con Hugging Face\n",
        "\n",
        "```python\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "inputs = tokenizer([\"Hello world\", \"Hi\"], padding=True, return_tensors=\"np\")\n",
        "print(inputs['input_ids'])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Modelos Tradicionales (SVM, KNN, etc.)\n",
        "\n",
        "### ¿Por qué requieren tamaño fijo?\n",
        "Estos modelos trabajan con vectores de características de longitud fija, como BoW o TF-IDF.\n",
        "\n",
        "### Ejemplo en Python\n",
        "\n",
        "```python\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "texts = [\"hello world\", \"hello\"]\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(texts).toarray()\n",
        "\n",
        "clf = SVC()\n",
        "clf.fit(X, [0, 1])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ¿Qué modelos NO necesitan entradas de tamaño fijo?\n",
        "\n",
        "- Modelos que usan **generadores** o procesan secuencias individualmente.\n",
        "- Algunos modelos autoregresivos en inferencia (*token por token*).\n",
        "- Árboles de decisión (como XGBoost) pueden usarse con padding si se maneja adecuadamente.\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusión\n",
        "\n",
        "El padding es una práctica común en PLN y visión por computadora. En la mayoría de los modelos, es indispensable garantizar entradas de tamaño uniforme para que la arquitectura funcione correctamente durante el entrenamiento y la inferencia.\n"
      ],
      "metadata": {
        "id": "7xHVx5FjWNF4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Eptzp6QWN1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Codificación One-Hot con Keras\n",
        "\n",
        "La codificación *one-hot* es una técnica común en Procesamiento de Lenguaje Natural (PLN) para representar palabras como vectores binarios. Cada palabra se representa con un vector donde solo una posición (correspondiente a esa palabra en el vocabulario) tiene el valor 1 y el resto son ceros.\n",
        "\n",
        "---\n",
        "\n",
        "## Paso 1: Texto de entrada\n",
        "\n",
        "```python\n",
        "text = 'I want to go to lunch with me. The lunch menu is hamburgers. Hamburgers are the best'\n",
        "```\n",
        "\n",
        "Este es el texto de entrada que se usará para generar el vocabulario y codificar.\n",
        "\n",
        "---\n",
        "\n",
        "## Paso 2: Importar librerías necesarias\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "```\n",
        "\n",
        "- `Tokenizer`: convierte texto a secuencias numéricas.\n",
        "- `to_categorical`: convierte números enteros a vectores one-hot.\n",
        "\n",
        "---\n",
        "\n",
        "## Paso 3: Crear y entrenar el Tokenizer\n",
        "\n",
        "```python\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts([text])\n",
        "print(t.word_index)\n",
        "```\n",
        "\n",
        "Esto genera un diccionario que asigna un índice único a cada palabra, basado en su frecuencia. Por ejemplo:\n",
        "\n",
        "```python\n",
        "{\n",
        "    'to': 1, 'lunch': 2, 'the': 3, 'hamburgers': 4, 'i': 5, 'want': 6,\n",
        "    'go': 7, 'with': 8, 'me': 9, 'menu': 10, 'is': 11, 'are': 12, 'best': 13\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Paso 4: Convertir texto a secuencias numéricas\n",
        "\n",
        "```python\n",
        "encoded = t.texts_to_sequences([text])\n",
        "print(encoded)\n",
        "```\n",
        "\n",
        "Esto convierte el texto a una lista de índices:\n",
        "\n",
        "```python\n",
        "[[5, 6, 1, 7, 1, 2, 8, 9, 3, 2, 10, 11, 4, 4, 12, 3, 13]]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Paso 5: Codificación One-Hot\n",
        "\n",
        "```python\n",
        "one_hot = to_categorical(encoded)\n",
        "print(one_hot)\n",
        "```\n",
        "\n",
        "Cada número entero es transformado en un vector donde el índice correspondiente es 1 y los demás 0.\n",
        "\n",
        "Por ejemplo, si `i = 5`, el vector resultante tiene un 1 en la posición 5 y ceros en el resto:\n",
        "\n",
        "```python\n",
        "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
        "```\n",
        "\n",
        "La forma final de la matriz `one_hot` es `(1, N, vocab_size + 1)`, donde `N` es el número de tokens y `vocab_size` es el número de palabras únicas.\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusión\n",
        "\n",
        "La codificación one-hot transforma texto en un formato numérico entendible para modelos de redes neuronales. Aunque es simple, puede ser ineficiente para vocabularios grandes. En esos casos, se prefieren métodos como **word embeddings** (Word2Vec, GloVe, etc.).\n"
      ],
      "metadata": {
        "id": "UEUukeIcYEN9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Modelos de Lenguaje y N-Gramas\n",
        "\n",
        "Un **modelo de lenguaje** es un sistema que predice o genera el siguiente elemento en una secuencia lingüística (letra, palabra, oración, párrafo, etc.). Se utilizan ampliamente en tareas de procesamiento de lenguaje natural (NLP) como traducción automática, resumen de textos, corrección ortográfica, entre otros.\n",
        "\n",
        "---\n",
        "\n",
        "## ¿Cómo funciona un modelo de lenguaje?\n",
        "\n",
        "Un modelo de lenguaje estima la probabilidad de una secuencia de palabras:\n",
        "\n",
        "\\[\n",
        "P(w_1, w_2, w_3, ..., w_m) = P(w_1) P(w_2|w_1) P(w_3|w_1, w_2) ... P(w_m|w_1, ..., w_{m-1})\n",
        "\\]\n",
        "\n",
        "Esto significa que la probabilidad de una palabra depende del contexto anterior.\n",
        "\n",
        "---\n",
        "\n",
        "## Escasez de datos\n",
        "\n",
        "A medida que la longitud de la secuencia crece, la cantidad de datos necesarios para estimar estas probabilidades crece exponencialmente. Por eso, se utilizan aproximaciones conocidas como **n-gramas**.\n",
        "\n",
        "---\n",
        "\n",
        "## N-Gramas\n",
        "\n",
        "Los **n-gramas** son secuencias de n palabras. Ejemplos:\n",
        "\n",
        "- **Unigrama** (n = 1): \"Tres\", \"cerditos\", \"vivieron\", \"felices\"\n",
        "- **Bigramas** (n = 2): \"Tres cerditos\", \"cerditos vivieron\", \"vivieron felices\"\n",
        "- **Trigramas** (n = 3): \"Tres cerditos vivieron\", \"cerditos vivieron felices\"\n",
        "\n",
        "### Ejemplo en Python\n",
        "\n",
        "```python\n",
        "from nltk import ngrams\n",
        "\n",
        "sentence = \"tres cerditos vivieron felices\".split()\n",
        "print(\"Unigramas:\", list(ngrams(sentence, 1)))\n",
        "print(\"Bigramas:\", list(ngrams(sentence, 2)))\n",
        "print(\"Trigramas:\", list(ngrams(sentence, 3)))\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Probabilidades con N-Gramas\n",
        "\n",
        "Para evitar depender del contexto completo, se usa una aproximación con n-gramas. Por ejemplo:\n",
        "\n",
        "\\[\n",
        "P(w_i|w_1, ..., w_{i-1}) \\approx P(w_i | w_{i-n+1}, ..., w_{i-1})\n",
        "\\]\n",
        "\n",
        "Esto reduce la complejidad y permite usar conteos para estimar:\n",
        "\n",
        "\\[\n",
        "P(w_i | w_{i-1}) = \\frac{\\text{Count}(w_{i-1}, w_i)}{\\text{Count}(w_{i-1})}\n",
        "\\]\n",
        "\n",
        "---\n",
        "\n",
        "## Ejemplo de cálculo de probabilidad de una secuencia (bigramas)\n",
        "\n",
        "```python\n",
        "from collections import Counter\n",
        "\n",
        "sentence = \"three little pigs lived happily\".split()\n",
        "bigrams = list(ngrams(sentence, 2))\n",
        "unigrams = sentence\n",
        "\n",
        "bigram_counts = Counter(bigrams)\n",
        "unigram_counts = Counter(unigrams)\n",
        "\n",
        "# Aproximación de probabilidad P(w_i | w_{i-1})\n",
        "for bigram in bigram_counts:\n",
        "    w1, w2 = bigram\n",
        "    prob = bigram_counts[bigram] / unigram_counts[w1]\n",
        "    print(f\"P({w2} | {w1}) = {prob:.2f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Aplicaciones de los modelos de lenguaje\n",
        "\n",
        "- Autocompletado de texto (como en Google)\n",
        "- Traducción automática\n",
        "- Reconocimiento de voz\n",
        "- Generación de texto\n",
        "- Corrección ortográfica\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusión\n",
        "\n",
        "Los modelos de lenguaje permiten a las máquinas comprender, predecir y generar lenguaje humano. Los n-gramas son una herramienta esencial para modelar secuencias con bajo costo computacional, aunque modelos modernos como transformers han superado estas limitaciones usando atención y entrenamiento en grandes corpus.\n"
      ],
      "metadata": {
        "id": "7X4XSxiegos0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelo de lenguaje: Gabriela Mistral\n",
        "\n",
        "\n",
        "Información desde: https://chilecultura.gob.cl/cultural-sections/396/"
      ],
      "metadata": {
        "id": "JDDajkDxjHLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Crear la carpeta 'corpus' si no existe\n",
        "os.makedirs(\"corpus\", exist_ok=True)\n",
        "\n",
        "# Mover archivos PDF y EPUB cargados al entorno a la carpeta 'corpus'\n",
        "for file in os.listdir():\n",
        "    if file.endswith(\".pdf\") or file.endswith(\".epub\"):\n",
        "        print(f\"Moviendo {file} a carpeta corpus\")\n",
        "        shutil.move(file, os.path.join(\"corpus\", file))\n",
        "\n",
        "# Confirmar contenido de la carpeta\n",
        "print(\"\\nArchivos en la carpeta 'corpus':\")\n",
        "print(os.listdir(\"corpus\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fasssiD3YFB1",
        "outputId": "ca4f9f0f-f469-425a-dd4c-db5c9c379761"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Archivos en la carpeta 'corpus':\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# Subir archivos .pdf y .epub desde tu equipo\n",
        "uploaded = files.upload()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "id": "ZOmCgzBajLDC",
        "outputId": "b402c495-7dce-4d32-8c23-079ddc56fa9b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b8094e9b-a0f9-4d14-a479-3acf6f578969\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b8094e9b-a0f9-4d14-a479-3acf6f578969\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving tomo8_m.pdf to tomo8_m.pdf\n",
            "Saving tomo7_m.pdf to tomo7_m.pdf\n",
            "Saving tomo6_m-1.pdf to tomo6_m-1.pdf\n",
            "Saving MC0003261.pdf to MC0003261.pdf\n",
            "Saving CH0000129_0001.pdf to CH0000129_0001.pdf\n",
            "Saving tomo5_gm(2).epub to tomo5_gm(2).epub\n",
            "Saving tomo4_gm.epub to tomo4_gm.epub\n",
            "Saving tomo3_e.epub to tomo3_e.epub\n",
            "Saving gm_tomo2.epub to gm_tomo2.epub\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ebooklib PyMuPDF\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zO4lkFd4koon",
        "outputId": "60ed7bb6-8386-49c2-94f6-bb5819515626"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ebooklib\n",
            "  Downloading ebooklib-0.19-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from ebooklib) (5.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from ebooklib) (1.17.0)\n",
            "Downloading ebooklib-0.19-py3-none-any.whl (39 kB)\n",
            "Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF, ebooklib\n",
            "Successfully installed PyMuPDF-1.26.3 ebooklib-0.19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import fitz  # PyMuPDF\n",
        "from ebooklib import epub\n",
        "from bs4 import BeautifulSoup\n"
      ],
      "metadata": {
        "id": "ZZQd2XYUkqUS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_epub_text(path):\n",
        "    book = epub.read_epub(path)\n",
        "    text = ''\n",
        "    for item in book.get_items():\n",
        "        if item.get_type() == epub.ITEM_DOCUMENT:\n",
        "            soup = BeautifulSoup(item.get_content(), 'html.parser')\n",
        "            text += soup.get_text()\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "p-trf2k0ksKP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_pdf_text(path):\n",
        "    text = ''\n",
        "    with fitz.open(path) as doc:\n",
        "        for page in doc:\n",
        "            text += page.get_text()\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "WQiJ3g6nkuej"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Crear carpeta corpus si no existe\n",
        "os.makedirs(\"corpus\", exist_ok=True)\n",
        "\n",
        "# Mover todos los archivos .pdf y .epub que estén en la raíz a la carpeta corpus\n",
        "for archivo in os.listdir():\n",
        "    if archivo.endswith(\".pdf\") or archivo.endswith(\".epub\"):\n",
        "        if not archivo.startswith(\"corpus\"):  # Evitar mover de nuevo si ya están ahí\n",
        "            print(f\"Moviendo {archivo} a carpeta corpus/\")\n",
        "            shutil.move(archivo, os.path.join(\"corpus\", archivo))\n",
        "\n",
        "# Verificación\n",
        "print(\"\\nContenido de la carpeta corpus:\")\n",
        "print(os.listdir(\"corpus\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMZ9RVyMk92J",
        "outputId": "473b195e-3592-4240-af14-e525c8ed669e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviendo tomo4_gm.epub a carpeta corpus/\n",
            "Moviendo tomo6_m-1.pdf a carpeta corpus/\n",
            "Moviendo tomo7_m.pdf a carpeta corpus/\n",
            "Moviendo tomo8_m.pdf a carpeta corpus/\n",
            "Moviendo gm_tomo2.epub a carpeta corpus/\n",
            "Moviendo tomo3_e.epub a carpeta corpus/\n",
            "Moviendo CH0000129_0001.pdf a carpeta corpus/\n",
            "Moviendo MC0003261.pdf a carpeta corpus/\n",
            "Moviendo tomo5_gm(2).epub a carpeta corpus/\n",
            "\n",
            "Contenido de la carpeta corpus:\n",
            "['tomo4_gm.epub', 'tomo6_m-1.pdf', 'tomo7_m.pdf', 'tomo8_m.pdf', 'gm_tomo2.epub', 'tomo3_e.epub', 'CH0000129_0001.pdf', 'MC0003261.pdf', 'tomo5_gm(2).epub']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Librerías necesarias\n",
        "import os\n",
        "import fitz  # PyMuPDF\n",
        "from ebooklib import epub, ITEM_DOCUMENT\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def extract_epub_text(path):\n",
        "    book = epub.read_epub(path)\n",
        "    text = ''\n",
        "    for item in book.get_items():\n",
        "        if item.get_type() == ITEM_DOCUMENT:\n",
        "            soup = BeautifulSoup(item.get_content(), 'html.parser')\n",
        "            text += soup.get_text()\n",
        "    return text\n",
        "\n",
        "\n",
        "# 3. Función para PDF\n",
        "def extract_pdf_text(path):\n",
        "    text = ''\n",
        "    with fitz.open(path) as doc:\n",
        "        for page in doc:\n",
        "            text += page.get_text()\n",
        "    return text\n",
        "\n",
        "# 4. Extraer textos\n",
        "corpus_path = \"corpus\"\n",
        "textos_completos = \"\"\n",
        "\n",
        "for archivo in os.listdir(corpus_path):\n",
        "    ruta = os.path.join(corpus_path, archivo)\n",
        "    if archivo.endswith(\".epub\"):\n",
        "        print(f\"Extrayendo EPUB: {archivo}\")\n",
        "        textos_completos += extract_epub_text(ruta) + \"\\n\"\n",
        "    elif archivo.endswith(\".pdf\"):\n",
        "        print(f\"Extrayendo PDF: {archivo}\")\n",
        "        textos_completos += extract_pdf_text(ruta) + \"\\n\"\n",
        "\n",
        "print(\"\\nLongitud total del corpus:\", len(textos_completos), \"caracteres\")\n",
        "\n",
        "# 5. Guardar en archivo\n",
        "with open(\"gabriela_mistral_corpus.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(textos_completos)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZxipgjclOBC",
        "outputId": "457a412e-5cb2-432b-994c-69196a61ad6c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extrayendo EPUB: tomo4_gm.epub\n",
            "Extrayendo PDF: tomo6_m-1.pdf\n",
            "Extrayendo PDF: tomo7_m.pdf\n",
            "Extrayendo PDF: tomo8_m.pdf\n",
            "Extrayendo EPUB: gm_tomo2.epub\n",
            "Extrayendo EPUB: tomo3_e.epub\n",
            "Extrayendo PDF: CH0000129_0001.pdf\n",
            "Extrayendo PDF: MC0003261.pdf\n",
            "Extrayendo EPUB: tomo5_gm(2).epub\n",
            "\n",
            "Longitud total del corpus: 4718633 caracteres\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar corpus\n",
        "with open(\"gabriela_mistral_corpus.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    corpus = f.read()\n",
        "\n",
        "# Limpiar y tokenizar\n",
        "import re\n",
        "tokens = re.findall(r'\\b\\w+\\b', corpus.lower())  # palabras minúsculas sin signos\n",
        "print(f\"Número de tokens: {len(tokens)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BwZdEjNJlsUG",
        "outputId": "dce58246-65a7-49b2-819a-d5f59c851dc8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de tokens: 860429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import random\n",
        "\n",
        "n = 3  # trigrama\n",
        "ngrams = defaultdict(list)\n",
        "\n",
        "# Construir el modelo\n",
        "for i in range(len(tokens) - n):\n",
        "    key = tuple(tokens[i:i+n-1])  # (palabra1, palabra2)\n",
        "    next_word = tokens[i+n-1]     # palabra3\n",
        "    ngrams[key].append(next_word)\n",
        "\n",
        "print(f\"Número de pares de contexto en el modelo: {len(ngrams)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDoXJ1Ueluro",
        "outputId": "8943c857-107b-4035-e030-4ee20d09bd2e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de pares de contexto en el modelo: 376942\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generar_texto(ngrams, largo=50, semilla=None):\n",
        "    if not semilla:\n",
        "        semilla = random.choice(list(ngrams.keys()))\n",
        "    salida = list(semilla)\n",
        "\n",
        "    for _ in range(largo):\n",
        "        context = tuple(salida[-2:])\n",
        "        posibles = ngrams.get(context, [])\n",
        "        if posibles:\n",
        "            salida.append(random.choice(posibles))\n",
        "        else:\n",
        "            break\n",
        "    return ' '.join(salida)\n"
      ],
      "metadata": {
        "id": "Wre8EcXFlxRK"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generar_texto(ngrams, largo=100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edRIJXH1l1V6",
        "outputId": "4b30894c-3292-4eab-b91c-1a0f1d3ebc2d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sin halagarme estas sensualidades del oído a los puteros españoles de ayer que la ha divisado todavía llegó doña constanza traía su cortejo a las instituciones no debieron descuajar en él si quiere que haya cogido mi mano en la noche ah yo sabía de golpe según haya sido la de hoy seguiré siéndolo tanto o más mujeres no chata ni insignificante como cualquiera otra parte no será su regalo de un padecimiento especial de los hábitos y a mi padre que jamás ha entendido baroja la insulta cada vez que llevamos y yo paso de dos bandas de silencio o de\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ✍️ Escribe una frase para continuar en estilo Gabriela Mistral\n",
        "frase_inicial = \"flor amarillenta\"  #@param {type:\"string\"}\n",
        "largo_generado = 170  #@param {type:\"slider\", min:10, max:200, step:10}\n",
        "\n",
        "# Tokenizar la frase\n",
        "semilla_tokens = re.findall(r'\\b\\w+\\b', frase_inicial.lower())\n",
        "\n",
        "# Verificamos que tenga al menos n-1 tokens\n",
        "if len(semilla_tokens) < n - 1:\n",
        "    print(f\"Debes escribir al menos {n-1} palabras.\")\n",
        "else:\n",
        "    semilla = tuple(semilla_tokens[-(n-1):])\n",
        "    texto_generado = generar_texto(ngrams, largo=largo_generado, semilla=semilla)\n",
        "    print(\"🔮 Texto generado:\\n\")\n",
        "    print(texto_generado)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jxhzctyel6zp",
        "outputId": "916c5242-3a45-45a3-9ee6-612b92d742d9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔮 Texto generado:\n",
            "\n",
            "flor amarillenta sus perfectos sembra díos los más nobles que la dulzura cuando vimos la muerte la prisa es pura mi almohada alborotaban como un objeto de caucho vivo gracias a él los deja pasar al peatón en el final de mi hermana ya me hastío de oírmela en las córdobas blancas cantan sus muros santa catalina y algunas rimas de mi paladar las recuerda en sus paisajes plantas y los abro y los injertos los que me descu bren casi todas sabe usted qué profunda huella me dejó una especie de acción de gracias rondas ronda del arco iris a fryda schultz de mantovani la mitad del año en protesta contra los trastámara y que hizo de las casas alzada de la costumbre de dos mil veinte colofón impreso en chile supongo que las gentes la ironía constante del santo tan bella y que se le ha escuchado mucho y sabía mi mano gabriela mistral 186 no me dieron mucha ternura y lectura para mujeres pero en verdad de esas trampas\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar sugerencias para una palabra inicial\n",
        "def sugerencias_para(palabra_inicial):\n",
        "    palabra_inicial = palabra_inicial.lower()\n",
        "    candidatos = set()\n",
        "\n",
        "    for contexto in ngrams:\n",
        "        if contexto[0] == palabra_inicial:\n",
        "            candidatos.add(contexto[1])\n",
        "\n",
        "    if candidatos:\n",
        "        print(f\"Sugerencias para continuar después de '{palabra_inicial}':\")\n",
        "        print(\", \".join(sorted(candidatos)))\n",
        "    else:\n",
        "        print(f\"No se encontraron continuaciones para '{palabra_inicial}'.\")\n",
        "\n",
        "# Ejemplo:\n",
        "sugerencias_para(\"amiga\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AO1p-wZ-mnk5",
        "outputId": "5d8feec9-fe20-46a9-f26f-4d2e6b961189"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sugerencias para continuar después de 'amiga':\n",
            "a, al, c, cara, casi, con, consu, crear, cree, de, dirigir, dolores, entienda, española, esther, estoy, francesa, gabriela, gracias, habría, haciendo, hay, inglesa, la, las, le, los, lucila, me, mejor, mexicana, muy, más, mía, no, norteamericana, para, permítame, pero, pídale, que, querida, respetuosa, se, secretaria, si, sin, suele, sí, tan, tiene, todo, usted, vamos, varias, vieja, y, yanqui, yo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear lista de opciones desde los primeros tokens\n",
        "opciones_inicio = sorted(set([k[0] for k in ngrams]))\n",
        "\n",
        "#@title ✨ Elige una palabra para comenzar\n",
        "palabra_inicial = \"flor\"  #@param [\"madre\", \"niña\", \"luz\", \"tierra\", \"sol\", \"vida\", \"tala\", \"flor\", \"día\", \"viento\"]\n",
        "print(f\"Elegiste comenzar con: {palabra_inicial}\")\n",
        "sugerencias_para(palabra_inicial)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "wLISVO1amiqI",
        "outputId": "92f488ef-69b3-43a3-f4a6-ca93477f9864"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elegiste comenzar con: flor\n",
            "Sugerencias para continuar después de 'flor':\n",
            "a, al, amarillenta, americana, aristócrata, azul, azulada, blanca, callada, casi, caída, como, con, cortan, cristiana, cómo, de, deja, del, e, empavesada, en, entonces, es, escasa, espiga, esta, estaba, esto, esté, eterna, flor, frenética, fue, g, gabriela, guarda, ha, hiende, juguete, junto, la, le, llameando, lleva, local, los, manso, mareando, mira, misma, mojados, más, mío, nace, natural, ni, no, o, oh, otra, parecía, pasta, patricia, poca, por, preciosa, preferida, primeros, que, queda, quiere, quiero, quién, recién, roja, salte, se, sean, seremos, si, sin, sino, sobre, solo, suma, tan, te, tenía, tiesa, tremolada, tuviera, un, una, vendrá, verdadera, vuelve, y, yo, él\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Explicación Detallada: Generación de Texto con N-Gramas\n",
        "\n",
        "Este documento explica línea por línea cómo funciona el proceso de generación de texto utilizando un modelo de n-gramas basado en trigramas (n=3), aplicado con Python, `CountVectorizer` y `NLTK`.\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 Objetivo\n",
        "\n",
        "El objetivo del ejercicio es predecir y generar palabras utilizando un modelo basado en frecuencias de n-gramas extraídos de un texto. El modelo trabaja como un sistema de autocompletado simple.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧩 Parte Principal: Generación de Texto\n",
        "\n",
        "```python\n",
        "# Inicializa la semilla aleatoria\n",
        "seed(123)\n",
        "\n",
        "# Semilla de entrada proporcionada por el usuario\n",
        "my_seed_str = 'machine learning'  # Debe ser un (n-1)-grama válido\n",
        "\n",
        "a_nm1_gram = my_seed_str\n",
        "output_string = my_seed_str  # Inicializa el texto generado\n",
        "\n",
        "# Bucle de generación de texto\n",
        "while a_nm1_gram in my_dict:\n",
        "    output_string += \" \" + predict_next(a_nm1_gram)\n",
        "    words = nltk.word_tokenize(output_string)\n",
        "    a_nm1_gram = ' '.join(words[-n+1:])  # Actualiza el contexto (últimas n-1 palabras)\n",
        "\n",
        "# Muestra el texto generado\n",
        "output_string\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 🔍 Explicación Línea por Línea\n",
        "\n",
        "### `seed(123)`\n",
        "- Fija la semilla aleatoria para asegurar reproducibilidad.\n",
        "- La función `predict_next()` depende de aleatoriedad, así que esto permite resultados consistentes en cada ejecución.\n",
        "\n",
        "---\n",
        "\n",
        "### `my_seed_str = 'machine learning'`\n",
        "- Define el punto de partida del texto generado.\n",
        "- Tiene que ser un (n-1)-grama válido (con n=3, debe ser un bigrama).\n",
        "\n",
        "---\n",
        "\n",
        "### `a_nm1_gram = my_seed_str`\n",
        "- Inicializa la variable que contiene el contexto actual (el (n-1)-grama desde el cual se predecirá la siguiente palabra).\n",
        "\n",
        "---\n",
        "\n",
        "### `output_string = my_seed_str`\n",
        "- Guarda la cadena de texto generada.\n",
        "- Se irá completando conforme se predigan nuevas palabras.\n",
        "\n",
        "---\n",
        "\n",
        "### `while a_nm1_gram in my_dict:`\n",
        "- Mientras el contexto actual exista en el diccionario, continúa la generación.\n",
        "- Si no hay registros de palabras que sigan al contexto, la generación termina.\n",
        "\n",
        "---\n",
        "\n",
        "### `output_string += \" \" + predict_next(a_nm1_gram)`\n",
        "- Llama a la función `predict_next()` para predecir la siguiente palabra.\n",
        "- Esa palabra se agrega al `output_string` con un espacio.\n",
        "\n",
        "---\n",
        "\n",
        "### `words = nltk.word_tokenize(output_string)`\n",
        "- Tokeniza el texto generado en palabras individuales.\n",
        "- Necesario para extraer las últimas `n-1` palabras correctamente.\n",
        "\n",
        "---\n",
        "\n",
        "### `a_nm1_gram = ' '.join(words[-n+1:])`\n",
        "- Toma las últimas `n-1` palabras generadas.\n",
        "- Esto forma el nuevo contexto para la siguiente predicción.\n",
        "\n",
        "---\n",
        "\n",
        "### Resultado Final\n",
        "\n",
        "- El resultado es una cadena de texto que comienza con la semilla y se expande automáticamente hasta que no se pueden hacer más predicciones.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔁 Esquema General\n",
        "\n",
        "```\n",
        "Semilla → predecir 1 palabra → agregar → actualizar contexto → repetir\n",
        "```\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "MA-MZ46o5ksY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Explicación Detallada: Autocompletado de Texto con N-Gramas\n",
        "\n",
        "Este documento explica **línea por línea** todo el proceso de construcción y uso de un modelo de autocompletado de texto basado en **n-gramas**, específicamente **trigramas** (`n=3`). El código está dividido en cuatro partes fundamentales:\n",
        "\n",
        "1. Preparación del texto y extracción de n-gramas\n",
        "2. Construcción del diccionario de predicción\n",
        "3. Función para predecir la siguiente palabra\n",
        "4. Generación de una secuencia de texto\n",
        "\n",
        "---\n",
        "\n",
        "## 📌 Librerías utilizadas\n",
        "\n",
        "```python\n",
        "import nltk\n",
        "from numpy.random import randint, seed\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "```\n",
        "\n",
        "- `nltk`: Biblioteca para procesamiento de lenguaje natural. Aquí se usa para tokenizar.\n",
        "- `randint` y `seed`: De `numpy.random`, permiten elegir elementos al azar y fijar la semilla para reproducibilidad.\n",
        "- `CountVectorizer`: De `sklearn`, para generar n-gramas del texto.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Preparación del Texto y Extracción de n-Gramas\n",
        "\n",
        "```python\n",
        "my_text = [my_text.lower()]\n",
        "```\n",
        "\n",
        "- Convierte todo el texto a minúsculas.\n",
        "- Lo convierte en una lista, como lo requiere `CountVectorizer`.\n",
        "\n",
        "```python\n",
        "n = 3\n",
        "n_min = n\n",
        "n_max = n\n",
        "n_gram_type = 'word'\n",
        "vectorizer = CountVectorizer(ngram_range=(n_min,n_max), analyzer=n_gram_type)\n",
        "```\n",
        "\n",
        "- Define el tamaño de los n-gramas (`n = 3` para trigramas).\n",
        "- `analyzer='word'` indica que se trata de n-gramas de palabras (no de caracteres).\n",
        "\n",
        "```python\n",
        "n_grams = vectorizer.fit(my_text).get_feature_names_out()\n",
        "n_gram_cts = vectorizer.transform(my_text).toarray()[0]\n",
        "list(zip(n_grams,n_gram_cts))\n",
        "```\n",
        "\n",
        "- `fit()` entrena el vectorizador con el texto.\n",
        "- `get_feature_names_out()` obtiene los trigramas encontrados.\n",
        "- `transform(...).toarray()` devuelve las frecuencias de cada n-grama como un array.\n",
        "- Finalmente, se muestra una lista de pares (trigrama, frecuencia).\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Construcción del Diccionario de Predicción\n",
        "\n",
        "```python\n",
        "my_dict = {}\n",
        "for a_gram in n_grams:\n",
        "    words = nltk.word_tokenize(a_gram)\n",
        "    a_nm1_gram = ' '.join(words[0:n-1])\n",
        "    next_word = words[-1]\n",
        "    if a_nm1_gram not in my_dict:\n",
        "        my_dict[a_nm1_gram] = [next_word]\n",
        "    else:\n",
        "        my_dict[a_nm1_gram].append(next_word)\n",
        "```\n",
        "\n",
        "- Inicializa un diccionario vacío.\n",
        "- Itera sobre cada trigrama (`a_gram`) generado previamente.\n",
        "- Lo divide en palabras usando `nltk.word_tokenize()`.\n",
        "- Extrae el (n-1)-grama (`a_nm1_gram`) como clave (las primeras dos palabras).\n",
        "- La tercera palabra es la que sigue y se guarda como valor.\n",
        "- Si la clave ya existe, se agrega la palabra al final de la lista.\n",
        "\n",
        "✅ El diccionario final tiene la forma:\n",
        "\n",
        "```python\n",
        "{\n",
        "  'machine learning': ['is', 'algorithms', 'algorithms'],\n",
        "  'learning is': ['the'],\n",
        "  ...\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Función de Predicción de la Siguiente Palabra\n",
        "\n",
        "```python\n",
        "def predict_next(a_nm1_gram):\n",
        "    value_list_size = len(my_dict[a_nm1_gram])\n",
        "    i_pick = randint(0, value_list_size)\n",
        "    return my_dict[a_nm1_gram][i_pick]\n",
        "```\n",
        "\n",
        "- Toma como entrada un bigrama válido (`a_nm1_gram`).\n",
        "- Calcula cuántas palabras posibles lo continúan.\n",
        "- Elige una posición aleatoria usando `randint(...)`.\n",
        "- Devuelve la palabra correspondiente de la lista de valores.\n",
        "\n",
        "📌 Esta función simula una predicción aleatoria basada en la frecuencia de ocurrencia (sin calcular probabilidades explícitas).\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Generación de Secuencia de Texto\n",
        "\n",
        "```python\n",
        "seed(123)\n",
        "my_seed_str = 'machine learning'\n",
        "a_nm1_gram = my_seed_str\n",
        "output_string = my_seed_str\n",
        "```\n",
        "\n",
        "- Se fija la semilla para reproducibilidad.\n",
        "- Se define la frase semilla (`my_seed_str`) con la que comenzará la generación.\n",
        "- Se inicializa la variable `output_string` con esa semilla.\n",
        "\n",
        "```python\n",
        "while a_nm1_gram in my_dict:\n",
        "    output_string += \" \" + predict_next(a_nm1_gram)\n",
        "    words = nltk.word_tokenize(output_string)\n",
        "    a_nm1_gram = ' '.join(words[-n+1:])\n",
        "```\n",
        "\n",
        "- Mientras existan continuaciones posibles:\n",
        "  - Se predice la siguiente palabra.\n",
        "  - Se agrega al texto generado.\n",
        "  - Se tokeniza el texto y se extraen las últimas `n-1` palabras para formar el nuevo contexto.\n",
        "\n",
        "🔁 Este bucle se repite hasta que el último bigrama generado **no tiene más continuaciones registradas**.\n",
        "\n",
        "```python\n",
        "output_string\n",
        "```\n",
        "\n",
        "- Finalmente, se muestra el texto generado completo.\n",
        "\n",
        "---\n",
        "\n",
        "## 🔁 Flujo General del Modelo\n",
        "\n",
        "```\n",
        "1. Texto original → n-gramas → diccionario de predicción\n",
        "2. Semilla → predecir siguiente palabra → actualizar contexto → repetir\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ✅ Requisitos para que funcione\n",
        "\n",
        "- El bigrama de entrada (`my_seed_str`) **debe existir** en `my_dict`.\n",
        "- El texto debe tener suficiente longitud para generar ejemplos variados.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 Posibles Extensiones\n",
        "\n",
        "- Calcular probabilidades en lugar de elegir aleatoriamente.\n",
        "- Probar con otros valores de `n`.\n",
        "- Usar múltiples documentos o textos más extensos.\n",
        "- Limitar la cantidad de palabras generadas para evitar bucles infinitos.\n",
        "\n",
        "---\n",
        "\n",
        "Este modelo es simple pero ilustrativo, y es una excelente introducción a los conceptos básicos de modelado de lenguaje mediante n-gramas."
      ],
      "metadata": {
        "id": "E1cLelnS6NZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iBhwp9t86WKr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# llegamos a p 110"
      ],
      "metadata": {
        "id": "jdUEzM8MznhS"
      }
    }
  ]
}