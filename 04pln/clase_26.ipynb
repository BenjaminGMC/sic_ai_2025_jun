{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjcWkzlXKC0WyNq4bl9a3W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidlealo/sic_ai_2025_jun/blob/main/04pln/clase_26.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Padding en Procesamiento de Lenguaje Natural (PLN)\n",
        "\n",
        "El *padding* es una técnica usada para igualar la longitud de las secuencias de texto. Es especialmente útil cuando usamos modelos que requieren entradas de tamaño fijo, como redes neuronales.\n",
        "\n",
        "En este ejemplo veremos cómo se realiza el padding utilizando:\n",
        "\n",
        "1. NumPy (manualmente)\n",
        "2. La función `pad_sequences` de Keras, más simple y automática\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Tokenización y codificación de frases\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "```\n",
        "\n",
        "Importamos las librerías necesarias:\n",
        "- `numpy`: para manipulación numérica y de arrays.\n",
        "- `Tokenizer` de `keras.preprocessing.text`: para transformar texto a números.\n",
        "\n",
        "```python\n",
        "sentences = [\n",
        "    ['barber', 'person'],\n",
        "    ['barber', 'good', 'person'],\n",
        "    ['barber', 'huge', 'person'],\n",
        "    ['knew', 'secret'],\n",
        "    ['secret', 'kept', 'huge', 'secret'],\n",
        "    ['huge', 'secret'],\n",
        "    ['barber', 'kept', 'word'],\n",
        "    ['barber', 'kept', 'word'],\n",
        "    ['barber', 'kept', 'secret'],\n",
        "    ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
        "    ['barber', 'went', 'huge', 'mountain']\n",
        "]\n",
        "```\n",
        "\n",
        "Creamos una lista de frases (listas de palabras). Estas frases se van a codificar posteriormente.\n",
        "\n",
        "```python\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "```\n",
        "\n",
        "- Creamos un objeto `Tokenizer`.\n",
        "- Usamos `fit_on_texts(sentences)` para crear un vocabulario basado en la frecuencia de aparición de las palabras.\n",
        "\n",
        "```python\n",
        "encoded = tokenizer.texts_to_sequences(sentences)\n",
        "print(encoded)\n",
        "```\n",
        "\n",
        "Transformamos las palabras en enteros. Cada palabra del vocabulario se asigna a un número único. Por ejemplo:\n",
        "```python\n",
        "[['barber', 'person']] -> [1, 5]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Encontrar la longitud máxima\n",
        "\n",
        "```python\n",
        "max_len = max(len(item) for item in encoded)\n",
        "print(max_len)\n",
        "```\n",
        "\n",
        "Calculamos la longitud máxima de las frases codificadas. Esto es necesario para saber cuántos ceros agregar a las secuencias más cortas.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Padding manual con NumPy\n",
        "\n",
        "```python\n",
        "for item in encoded:             # Para cada frase\n",
        "    while len(item) < max_len:  # Si es menor que la longitud máxima\n",
        "        item.append(0)          # Agrega ceros al final (post-padding)\n",
        "\n",
        "padded_np = np.array(encoded)\n",
        "padded_np\n",
        "```\n",
        "\n",
        "- Recorremos cada lista de enteros (`item`) y le agregamos ceros al final hasta igualar la longitud máxima.\n",
        "- Convertimos la lista a un `array` de NumPy.\n",
        "\n",
        "Ejemplo de salida:\n",
        "```python\n",
        "array([[ 1,  5,  0,  0,  0,  0,  0],\n",
        "       [ 1,  8,  5,  0,  0,  0,  0],\n",
        "       ...\n",
        "       [ 1, 12,  3, 13,  0,  0,  0]])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Padding con la herramienta de Keras\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "```\n",
        "\n",
        "Importamos la función `pad_sequences`, que permite aplicar padding fácilmente a listas de enteros.\n",
        "\n",
        "```python\n",
        "encoded = tokenizer.texts_to_sequences(sentences)\n",
        "print(encoded)\n",
        "```\n",
        "\n",
        "Codificamos nuevamente las frases, igual que antes.\n",
        "\n",
        "```python\n",
        "padded = pad_sequences(encoded)\n",
        "padded\n",
        "```\n",
        "\n",
        "- `pad_sequences` aplica padding automático por defecto al inicio de cada secuencia (pre-padding).\n",
        "- Agrega ceros al comienzo de las secuencias más cortas para igualarlas con la más larga.\n",
        "\n",
        "Ejemplo de salida:\n",
        "```python\n",
        "array([[ 0,  0,  0,  0,  0,  1,  5],\n",
        "       [ 0,  0,  0,  0,  1,  8,  5],\n",
        "       ...\n",
        "       [ 0,  0,  1, 12,  3, 13]])\n",
        "```\n",
        "\n",
        "Si quisiéramos usar **post-padding** (agregar ceros al final), podemos usar:\n",
        "```python\n",
        "pad_sequences(encoded, padding='post')\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusión\n",
        "\n",
        "El padding es fundamental para trabajar con modelos de aprendizaje automático en NLP, ya que la mayoría requiere que las entradas tengan la misma forma. Podemos hacerlo de forma manual con NumPy, o automatizarlo usando `pad_sequences` de Keras, lo cual es más limpio y eficiente.\n"
      ],
      "metadata": {
        "id": "8i6kjbbFU3hz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKVJpCaySllU"
      },
      "outputs": [],
      "source": [
        "# Visita la documentación https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = [['barber', 'person'], ['barber', 'good', 'person'], ['barber', 'huge', 'person'],\n",
        "             ['knew', 'secret'], ['secret', 'kept', 'huge', 'secret'], ['huge', 'secret'],\n",
        "             ['barber', 'kept', 'word'], ['barber', 'kept', 'word'], ['barber', 'kept', 'secret'],\n",
        "             ['keeping', 'keeping', 'huge', 'secret', 'driving', 'barber', 'crazy'],\n",
        "             ['barber', 'went', 'huge', 'mountain']]"
      ],
      "metadata": {
        "id": "32w1mTiOXQu_"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sentences) #The group of words are created based on frequency when putting corpus fit_ontexts().\n",
        "\n",
        "encoded = tokenizer.texts_to_sequences(sentences)\n",
        "print(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bivSDAIxXUmP",
        "outputId": "cd3ed5b6-bf5f-4b7f-a151-82c8d56c65a2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = max(len(item) for item in encoded)\n",
        "print(max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6G-MsBOrXXrL",
        "outputId": "c949d9d0-cc0e-4859-b35c-a6ca4ca87214"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1Zdc0ySVyjX",
        "outputId": "b948a7ad-147e-44ca-a374-d08d4d66f802"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  5,  0,  0,  0,  0,  0],\n",
              "       [ 1,  8,  5,  0,  0,  0,  0],\n",
              "       [ 1,  3,  5,  0,  0,  0,  0],\n",
              "       [ 9,  2,  0,  0,  0,  0,  0],\n",
              "       [ 2,  4,  3,  2,  0,  0,  0],\n",
              "       [ 3,  2,  0,  0,  0,  0,  0],\n",
              "       [ 1,  4,  6,  0,  0,  0,  0],\n",
              "       [ 1,  4,  6,  0,  0,  0,  0],\n",
              "       [ 1,  4,  2,  0,  0,  0,  0],\n",
              "       [ 7,  7,  3,  2, 10,  1, 11],\n",
              "       [ 1, 12,  3, 13,  0,  0,  0]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "for item in encoded: # For each item\n",
        "    while len(item) < max_len:   # If less than max_len\n",
        "        item.append(0)\n",
        "\n",
        "padded_np = np.array(encoded)\n",
        "padded_np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdqaHjykVyjY"
      },
      "source": [
        "Padding with Keras preprocessing tool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oX72vx4eVyja"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3u7MXFTVyja",
        "outputId": "9399aa9a-6cfd-4f1e-bc66-aca4e0c6151b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 5], [1, 8, 5], [1, 3, 5], [9, 2], [2, 4, 3, 2], [3, 2], [1, 4, 6], [1, 4, 6], [1, 4, 2], [7, 7, 3, 2, 10, 1, 11], [1, 12, 3, 13]]\n"
          ]
        }
      ],
      "source": [
        "encoded = tokenizer.texts_to_sequences(sentences)\n",
        "print(encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZsNGeoGVyjb",
        "outputId": "826bc743-6ecb-400b-ab4f-b2eabf002e36"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0,  0,  0,  1,  5],\n",
              "       [ 0,  0,  0,  0,  1,  8,  5],\n",
              "       [ 0,  0,  0,  0,  1,  3,  5],\n",
              "       [ 0,  0,  0,  0,  0,  9,  2],\n",
              "       [ 0,  0,  0,  2,  4,  3,  2],\n",
              "       [ 0,  0,  0,  0,  0,  3,  2],\n",
              "       [ 0,  0,  0,  0,  1,  4,  6],\n",
              "       [ 0,  0,  0,  0,  1,  4,  6],\n",
              "       [ 0,  0,  0,  0,  1,  4,  2],\n",
              "       [ 7,  7,  3,  2, 10,  1, 11],\n",
              "       [ 0,  0,  0,  1, 12,  3, 13]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "padded = pad_sequences(encoded)\n",
        "padded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3zWnq6ZVyjb",
        "outputId": "cd3886be-8725-4aa0-e30e-e5bb2a856c84"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  5,  0,  0,  0,  0,  0],\n",
              "       [ 1,  8,  5,  0,  0,  0,  0],\n",
              "       [ 1,  3,  5,  0,  0,  0,  0],\n",
              "       [ 9,  2,  0,  0,  0,  0,  0],\n",
              "       [ 2,  4,  3,  2,  0,  0,  0],\n",
              "       [ 3,  2,  0,  0,  0,  0,  0],\n",
              "       [ 1,  4,  6,  0,  0,  0,  0],\n",
              "       [ 1,  4,  6,  0,  0,  0,  0],\n",
              "       [ 1,  4,  2,  0,  0,  0,  0],\n",
              "       [ 7,  7,  3,  2, 10,  1, 11],\n",
              "       [ 1, 12,  3, 13,  0,  0,  0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "padded = pad_sequences(encoded, padding='post')\n",
        "padded"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Modelos que requieren entradas de tamaño fijo en Deep Learning\n",
        "\n",
        "Muchos modelos de aprendizaje profundo requieren entradas de tamaño fijo para funcionar correctamente, especialmente durante el entrenamiento en lotes (*batch training*). Esto se debe a que las capas internas (densas, convolucionales, etc.) necesitan dimensiones predecibles.\n",
        "\n",
        "A continuación se explican los principales modelos que requieren entradas de tamaño fijo, junto con ejemplos en Python.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Redes Neuronales Densas (Dense / Fully Connected)\n",
        "\n",
        "### ¿Por qué requieren tamaño fijo?\n",
        "Cada neurona espera una cantidad fija de entradas. Por lo tanto, el vector de entrada debe tener una dimensión específica.\n",
        "\n",
        "### Ejemplo en Python\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([[1, 2, 3], [4, 5, 6]])  # entrada de tamaño fijo: 3 features\n",
        "model = Sequential([\n",
        "    Dense(10, input_shape=(3,), activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.summary()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Redes Convolucionales (CNNs)\n",
        "\n",
        "### ¿Por qué requieren tamaño fijo?\n",
        "Las operaciones de convolución requieren dimensiones definidas (ancho, alto, canales).\n",
        "\n",
        "### Ejemplo en Python\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, Flatten\n",
        "\n",
        "model = Sequential([\n",
        "    Conv2D(32, kernel_size=(3, 3), input_shape=(28, 28, 1), activation='relu'),\n",
        "    Flatten(),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "model.summary()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Redes Recurrentes (RNN, LSTM, GRU)\n",
        "\n",
        "### ¿Por qué requieren tamaño fijo?\n",
        "Aunque las RNN pueden manejar secuencias de longitud variable, en entrenamiento por lotes es necesario paddear las secuencias para que todas tengan la misma longitud.\n",
        "\n",
        "### Ejemplo en Python\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM\n",
        "\n",
        "# Ejemplo de secuencias con longitudes distintas\n",
        "sequences = [[1, 2, 3], [4, 5], [6]]\n",
        "padded = pad_sequences(sequences)\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=10, output_dim=4, input_length=padded.shape[1]),\n",
        "    LSTM(8)\n",
        "])\n",
        "model.summary()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Transformers (BERT, GPT)\n",
        "\n",
        "### ¿Por qué requieren tamaño fijo?\n",
        "Las entradas a modelos transformers se paddean para igualar la longitud, y se usan máscaras para ignorar los ceros.\n",
        "\n",
        "### Ejemplo en Python con Hugging Face\n",
        "\n",
        "```python\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "inputs = tokenizer([\"Hello world\", \"Hi\"], padding=True, return_tensors=\"np\")\n",
        "print(inputs['input_ids'])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Modelos Tradicionales (SVM, KNN, etc.)\n",
        "\n",
        "### ¿Por qué requieren tamaño fijo?\n",
        "Estos modelos trabajan con vectores de características de longitud fija, como BoW o TF-IDF.\n",
        "\n",
        "### Ejemplo en Python\n",
        "\n",
        "```python\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "texts = [\"hello world\", \"hello\"]\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(texts).toarray()\n",
        "\n",
        "clf = SVC()\n",
        "clf.fit(X, [0, 1])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ¿Qué modelos NO necesitan entradas de tamaño fijo?\n",
        "\n",
        "- Modelos que usan **generadores** o procesan secuencias individualmente.\n",
        "- Algunos modelos autoregresivos en inferencia (*token por token*).\n",
        "- Árboles de decisión (como XGBoost) pueden usarse con padding si se maneja adecuadamente.\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusión\n",
        "\n",
        "El padding es una práctica común en PLN y visión por computadora. En la mayoría de los modelos, es indispensable garantizar entradas de tamaño uniforme para que la arquitectura funcione correctamente durante el entrenamiento y la inferencia.\n"
      ],
      "metadata": {
        "id": "7xHVx5FjWNF4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Eptzp6QWN1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Codificación One-Hot con Keras\n",
        "\n",
        "La codificación *one-hot* es una técnica común en Procesamiento de Lenguaje Natural (PLN) para representar palabras como vectores binarios. Cada palabra se representa con un vector donde solo una posición (correspondiente a esa palabra en el vocabulario) tiene el valor 1 y el resto son ceros.\n",
        "\n",
        "---\n",
        "\n",
        "## Paso 1: Texto de entrada\n",
        "\n",
        "```python\n",
        "text = 'I want to go to lunch with me. The lunch menu is hamburgers. Hamburgers are the best'\n",
        "```\n",
        "\n",
        "Este es el texto de entrada que se usará para generar el vocabulario y codificar.\n",
        "\n",
        "---\n",
        "\n",
        "## Paso 2: Importar librerías necesarias\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "```\n",
        "\n",
        "- `Tokenizer`: convierte texto a secuencias numéricas.\n",
        "- `to_categorical`: convierte números enteros a vectores one-hot.\n",
        "\n",
        "---\n",
        "\n",
        "## Paso 3: Crear y entrenar el Tokenizer\n",
        "\n",
        "```python\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts([text])\n",
        "print(t.word_index)\n",
        "```\n",
        "\n",
        "Esto genera un diccionario que asigna un índice único a cada palabra, basado en su frecuencia. Por ejemplo:\n",
        "\n",
        "```python\n",
        "{\n",
        "    'to': 1, 'lunch': 2, 'the': 3, 'hamburgers': 4, 'i': 5, 'want': 6,\n",
        "    'go': 7, 'with': 8, 'me': 9, 'menu': 10, 'is': 11, 'are': 12, 'best': 13\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Paso 4: Convertir texto a secuencias numéricas\n",
        "\n",
        "```python\n",
        "encoded = t.texts_to_sequences([text])\n",
        "print(encoded)\n",
        "```\n",
        "\n",
        "Esto convierte el texto a una lista de índices:\n",
        "\n",
        "```python\n",
        "[[5, 6, 1, 7, 1, 2, 8, 9, 3, 2, 10, 11, 4, 4, 12, 3, 13]]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Paso 5: Codificación One-Hot\n",
        "\n",
        "```python\n",
        "one_hot = to_categorical(encoded)\n",
        "print(one_hot)\n",
        "```\n",
        "\n",
        "Cada número entero es transformado en un vector donde el índice correspondiente es 1 y los demás 0.\n",
        "\n",
        "Por ejemplo, si `i = 5`, el vector resultante tiene un 1 en la posición 5 y ceros en el resto:\n",
        "\n",
        "```python\n",
        "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
        "```\n",
        "\n",
        "La forma final de la matriz `one_hot` es `(1, N, vocab_size + 1)`, donde `N` es el número de tokens y `vocab_size` es el número de palabras únicas.\n",
        "\n",
        "---\n",
        "\n",
        "## Conclusión\n",
        "\n",
        "La codificación one-hot transforma texto en un formato numérico entendible para modelos de redes neuronales. Aunque es simple, puede ser ineficiente para vocabularios grandes. En esos casos, se prefieren métodos como **word embeddings** (Word2Vec, GloVe, etc.).\n"
      ],
      "metadata": {
        "id": "UEUukeIcYEN9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fasssiD3YFB1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}